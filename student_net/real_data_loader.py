"""
çœŸå®Truckåœºæ™¯æ•°æ®åŠ è½½å™¨ï¼šåŠ è½½çœŸå®çš„ç‚¹äº‘ã€å›¾ç‰‡ã€é«˜æ–¯çƒæ•°æ®å¹¶ç”Ÿæˆpatch dataset
"""

import os
import sys
import numpy as np
import torch
from PIL import Image
import json
import pickle
import struct
from typing import List, Dict, Tuple, Optional
import shutil

# æ·»åŠ çˆ¶ç›®å½•åˆ°path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from multimodal_patch_sampler import MultiModalPatchSampler, MultiModalPatch

class PLYReader:
    """PLYæ–‡ä»¶è¯»å–å™¨"""
    
    @staticmethod
    def read_ply_points(ply_path: str) -> np.ndarray:
        """è¯»å–PLYæ ¼å¼çš„ç‚¹äº‘æ•°æ®"""
        print(f"ğŸ“‚ è¯»å–PLYç‚¹äº‘: {ply_path}")
        
        with open(ply_path, 'rb') as f:
            # è¯»å–header
            header_lines = []
            while True:
                line = f.readline().decode('ascii').strip()
                header_lines.append(line)
                if line == 'end_header':
                    break
            
            # è§£æheaderä¿¡æ¯
            vertex_count = 0
            properties = []
            
            for line in header_lines:
                if line.startswith('element vertex'):
                    vertex_count = int(line.split()[-1])
                elif line.startswith('property'):
                    properties.append(line.split()[1:])  # [type, name]
            
            print(f"  é¡¶ç‚¹æ•°é‡: {vertex_count:,}")
            print(f"  å±æ€§æ•°é‡: {len(properties)}")
            
            # è¯»å–äºŒè¿›åˆ¶æ•°æ®
            points = []
            for i in range(vertex_count):
                point_data = []
                for prop_type, prop_name in properties:
                    if prop_type == 'float':
                        value = struct.unpack('f', f.read(4))[0]
                    elif prop_type == 'double': 
                        value = struct.unpack('d', f.read(8))[0]
                    elif prop_type == 'uchar':
                        value = struct.unpack('B', f.read(1))[0]
                    elif prop_type == 'int':
                        value = struct.unpack('i', f.read(4))[0]
                    else:
                        raise ValueError(f"æœªæ”¯æŒçš„æ•°æ®ç±»å‹: {prop_type}")
                    
                    point_data.append(value)
                
                # åªå–å‰3ä¸ªåæ ‡ (x, y, z)
                points.append(point_data[:3])
        
        points = np.array(points)
        print(f"  âœ… æˆåŠŸè¯»å– {len(points):,} ä¸ª3Dç‚¹")
        return points

    @staticmethod 
    def read_gaussian_positions(ply_path: str, max_points: Optional[int] = None) -> np.ndarray:
        """è¯»å–é«˜æ–¯çƒPLYæ–‡ä»¶ä¸­çš„ä½ç½®ä¿¡æ¯"""
        print(f"ğŸ“‚ è¯»å–é«˜æ–¯çƒä½ç½®: {ply_path}")
        
        # ä½¿ç”¨ä¸points3D.plyç›¸åŒçš„è¯»å–é€»è¾‘
        points = PLYReader.read_ply_points(ply_path)
        
        if max_points and len(points) > max_points:
            # éšæœºé‡‡æ ·å‡å°‘å†…å­˜ä½¿ç”¨
            indices = np.random.choice(len(points), max_points, replace=False)
            points = points[indices]
            print(f"  âš ï¸ éšæœºé‡‡æ ·åˆ° {max_points:,} ä¸ªé«˜æ–¯çƒ")
        
        return points

class RealTruckDataLoader:
    """çœŸå®Truckåœºæ™¯æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self):
        # çœŸå®æ•°æ®è·¯å¾„
        self.points_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/truck/sparse/0/points3D.ply"
        self.images_dir = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/truck/images"
        self.colmap_dir = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/truck/sparse/0"
        self.gaussian_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/output/truck-150w/gaussian_ball/iteration_994230_best_psnr/gaussian_ball.ply"
        
        print(f"ğŸš› çœŸå®Truckåœºæ™¯æ•°æ®åŠ è½½å™¨")
        print(f"  ç‚¹äº‘è·¯å¾„: {self.points_path}")
        print(f"  å›¾ç‰‡ç›®å½•: {self.images_dir}")
        print(f"  é«˜æ–¯çƒè·¯å¾„: {self.gaussian_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        self._check_files()
    
    def _check_files(self):
        """æ£€æŸ¥å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
        files_to_check = [
            self.points_path,
            self.images_dir,
            self.gaussian_path
        ]
        
        for file_path in files_to_check:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        print(f"âœ… æ‰€æœ‰å¿…è¦æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    def load_points_3d(self) -> np.ndarray:
        """åŠ è½½çœŸå®çš„3Dç‚¹äº‘"""
        points = PLYReader.read_ply_points(self.points_path)
        return points
    
    def load_gaussian_positions(self, max_gaussians: Optional[int] = 100000) -> np.ndarray:
        """åŠ è½½é«˜æ–¯çƒä½ç½® (ä¸ºäº†å†…å­˜è€ƒè™‘ï¼Œå¯ä»¥é™åˆ¶æ•°é‡)"""
        positions = PLYReader.read_gaussian_positions(self.gaussian_path, max_gaussians)
        return positions
    
    def load_images(self, max_images: Optional[int] = 16, resolution_scale: float = 4.0) -> List[np.ndarray]:
        """åŠ è½½çœŸå®å›¾ç‰‡"""
        print(f"ğŸ–¼ï¸ åŠ è½½å›¾ç‰‡æ•°æ®...")
        print(f"  æœ€å¤§å›¾ç‰‡æ•°: {max_images}")
        print(f"  åˆ†è¾¨ç‡ç¼©æ”¾: {resolution_scale}x")
        
        # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        image_files = sorted([f for f in os.listdir(self.images_dir) if f.endswith('.jpg')])
        
        if max_images:
            # å‡åŒ€é‡‡æ ·å›¾ç‰‡
            step = len(image_files) // max_images
            selected_files = image_files[::step][:max_images]
        else:
            selected_files = image_files
        
        images = []
        for img_file in selected_files:
            img_path = os.path.join(self.images_dir, img_file)
            
            # åŠ è½½å¹¶ç¼©æ”¾å›¾ç‰‡
            pil_image = Image.open(img_path)
            
            if resolution_scale != 1.0:
                new_width = int(pil_image.width / resolution_scale)
                new_height = int(pil_image.height / resolution_scale)
                pil_image = pil_image.resize((new_width, new_height), Image.LANCZOS)
            
            image_array = np.array(pil_image)
            images.append(image_array)
        
        print(f"  âœ… åŠ è½½äº† {len(images)} å¼ å›¾ç‰‡")
        print(f"  å›¾ç‰‡å°ºå¯¸: {images[0].shape}")
        return images
    
    def create_mock_cameras(self, num_cameras: int) -> List:
        """åˆ›å»ºæ¨¡æ‹Ÿç›¸æœºå‚æ•° (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥ä»COLMAPè¯»å–)"""
        print(f"ğŸ“· åˆ›å»ºæ¨¡æ‹Ÿç›¸æœºå‚æ•°...")
        
        cameras = []
        
        # å›´ç»•truckåˆ›å»ºç›¸æœº
        angles = np.linspace(0, 2*np.pi, num_cameras, endpoint=False)
        radius = 10
        height = 2
        
        for i, angle in enumerate(angles):
            cam_x = radius * np.cos(angle)
            cam_y = radius * np.sin(angle)
            cam_z = height
            
            # ç®€åŒ–çš„ç›¸æœºå‚æ•°
            camera = type('Camera', (), {
                'R': np.eye(3),
                'T': np.array([cam_x, cam_y, cam_z]),
                'fx': 500,  # ç¼©æ”¾åçš„ç„¦è·
                'fy': 500,
                'cx': 250,  # ç¼©æ”¾åçš„ä¸»ç‚¹
                'cy': 250,
                'image_width': 500,  # ç¼©æ”¾åçš„åˆ†è¾¨ç‡
                'image_height': 500,
                'uid': i
            })()
            
            cameras.append(camera)
        
        print(f"  âœ… åˆ›å»ºäº† {len(cameras)} ä¸ªç›¸æœº")
        return cameras

class PatchDatasetGenerator:
    """Patchæ•°æ®é›†ç”Ÿæˆå™¨"""
    
    def __init__(self, output_dir: str = "truck_patch_dataset"):
        self.output_dir = output_dir
        self.patches_dir = os.path.join(output_dir, "patches")
        self.metadata_dir = os.path.join(output_dir, "metadata")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.patches_dir, exist_ok=True)
        os.makedirs(self.metadata_dir, exist_ok=True)
        
        print(f"ğŸ“ Patchæ•°æ®é›†ç”Ÿæˆå™¨")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
    
    def save_patch_data(self, patches: List[MultiModalPatch], 
                       gaussians_positions: np.ndarray,
                       points_3d: np.ndarray,
                       images: List[np.ndarray]) -> str:
        """ä¿å­˜patchæ•°æ®ä¸ºè®­ç»ƒå°±ç»ªçš„dataset"""
        print(f"ğŸ’¾ ä¿å­˜patchæ•°æ®é›†...")
        
        dataset_info = {
            'total_patches': len(patches),
            'total_gaussians': len(gaussians_positions),
            'total_points': len(points_3d),
            'total_images': len(images),
            'patch_files': [],
            'creation_time': str(np.datetime64('now'))
        }
        
        # ä¿å­˜æ¯ä¸ªpatch
        for i, patch in enumerate(patches):
            patch_file = f"patch_{patch.patch_id:03d}.pkl"
            patch_path = os.path.join(self.patches_dir, patch_file)
            
            # æå–patchå¯¹åº”çš„å®é™…æ•°æ®
            patch_gaussians = gaussians_positions[patch.gaussian_indices]
            patch_points = points_3d[patch.point_indices] if len(patch.point_indices) > 0 else np.array([])
            
            # æå–patchå¯¹åº”çš„å›¾åƒæ•°æ®
            patch_images = {}
            for cam_id, img_patch_info in patch.image_patches.items():
                if cam_id < len(images):
                    bbox_2d = img_patch_info['bbox_2d']
                    x, y, w, h = bbox_2d
                    patch_image = images[cam_id][y:y+h, x:x+w]
                    patch_images[cam_id] = {
                        'image': patch_image,
                        'bbox_2d': bbox_2d
                    }
            
            # ç»„è£…patchæ•°æ®
            patch_data = {
                'patch_id': patch.patch_id,
                'gaussian_positions': patch_gaussians,  # (N, 3)
                'point_cloud': patch_points,            # (M, 3)
                'images': patch_images,                 # {cam_id: {'image': array, 'bbox_2d': tuple}}
                'bbox_3d': patch.bbox_3d,
                'statistics': {
                    'gaussian_count': patch.gaussian_count,
                    'point_count': patch.point_count,
                    'image_patch_count': len(patch.image_patches),
                    'spatial_volume': patch.spatial_volume
                }
            }
            
            # ä¿å­˜patchæ–‡ä»¶
            with open(patch_path, 'wb') as f:
                pickle.dump(patch_data, f)
            
            dataset_info['patch_files'].append({
                'file': patch_file,
                'patch_id': patch.patch_id,
                'gaussian_count': patch.gaussian_count,
                'point_count': patch.point_count,
                'image_count': len(patch_images)
            })
            
            print(f"  ğŸ’¾ ä¿å­˜ Patch {patch.patch_id}: {patch.gaussian_count:,} é«˜æ–¯çƒ, {patch.point_count:,} ç‚¹äº‘, {len(patch_images)} å›¾åƒ")
        
        # ä¿å­˜datasetå…ƒæ•°æ®
        metadata_path = os.path.join(self.metadata_dir, 'dataset_info.json')
        with open(metadata_path, 'w') as f:
            json.dump(dataset_info, f, indent=2)
        
        # ä¿å­˜åŸå§‹æ•°æ®ä¿¡æ¯
        raw_data_info = {
            'gaussians_shape': gaussians_positions.shape,
            'points_shape': points_3d.shape,
            'images_info': [img.shape for img in images],
            'data_ranges': {
                'gaussians_min': gaussians_positions.min(axis=0).tolist(),
                'gaussians_max': gaussians_positions.max(axis=0).tolist(),
                'points_min': points_3d.min(axis=0).tolist() if len(points_3d) > 0 else None,
                'points_max': points_3d.max(axis=0).tolist() if len(points_3d) > 0 else None
            }
        }
        
        raw_info_path = os.path.join(self.metadata_dir, 'raw_data_info.json')
        with open(raw_info_path, 'w') as f:
            json.dump(raw_data_info, f, indent=2)
        
        print(f"âœ… æ•°æ®é›†ä¿å­˜å®Œæˆ!")
        print(f"  Patchæ–‡ä»¶: {len(patches)} ä¸ª")
        print(f"  å…ƒæ•°æ®: {metadata_path}")
        print(f"  åŸå§‹æ•°æ®ä¿¡æ¯: {raw_info_path}")
        
        return self.output_dir
    
    def create_dataloader_script(self) -> str:
        """åˆ›å»ºPyTorch DataLoaderè„šæœ¬"""
        dataloader_script = '''"""
PyTorch DataLoader for Truck Patch Dataset
"""

import torch
from torch.utils.data import Dataset, DataLoader
import pickle
import json
import os
import numpy as np

class TruckPatchDataset(Dataset):
    """Truckåœºæ™¯Patchæ•°æ®é›†"""
    
    def __init__(self, dataset_dir, transform=None):
        self.dataset_dir = dataset_dir
        self.patches_dir = os.path.join(dataset_dir, "patches")
        self.metadata_dir = os.path.join(dataset_dir, "metadata")
        self.transform = transform
        
        # åŠ è½½æ•°æ®é›†ä¿¡æ¯
        with open(os.path.join(self.metadata_dir, 'dataset_info.json'), 'r') as f:
            self.dataset_info = json.load(f)
        
        self.patch_files = self.dataset_info['patch_files']
        
    def __len__(self):
        return len(self.patch_files)
    
    def __getitem__(self, idx):
        patch_file = self.patch_files[idx]['file']
        patch_path = os.path.join(self.patches_dir, patch_file)
        
        # åŠ è½½patchæ•°æ®
        with open(patch_path, 'rb') as f:
            patch_data = pickle.load(f)
        
        # è½¬æ¢ä¸ºtensor
        sample = {
            'patch_id': patch_data['patch_id'],
            'gaussian_positions': torch.from_numpy(patch_data['gaussian_positions']).float(),
            'point_cloud': torch.from_numpy(patch_data['point_cloud']).float() if len(patch_data['point_cloud']) > 0 else torch.empty(0, 3),
            'images': {cam_id: torch.from_numpy(img_data['image']).float() / 255.0 
                      for cam_id, img_data in patch_data['images'].items()},
            'bbox_3d': patch_data['bbox_3d'],
            'statistics': patch_data['statistics']
        }
        
        if self.transform:
            sample = self.transform(sample)
        
        return sample

def create_dataloader(dataset_dir, batch_size=4, shuffle=True, num_workers=2):
    """åˆ›å»ºDataLoader"""
    dataset = TruckPatchDataset(dataset_dir)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, 
                           num_workers=num_workers, collate_fn=patch_collate_fn)
    return dataloader

def patch_collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†å˜é•¿æ•°æ®"""
    # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è®­ç»ƒéœ€æ±‚å®šåˆ¶
    return batch  # ç®€åŒ–ç‰ˆæœ¬ç›´æ¥è¿”å›list

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åŠ è½½
    dataset = TruckPatchDataset("truck_patch_dataset")
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    sample = dataset[0]
    print(f"æ ·ä¾‹patch:")
    print(f"  é«˜æ–¯çƒ: {sample['gaussian_positions'].shape}")
    print(f"  ç‚¹äº‘: {sample['point_cloud'].shape}")
    print(f"  å›¾åƒ: {len(sample['images'])} ä¸ª")
'''
        
        script_path = os.path.join(self.output_dir, "patch_dataloader.py")
        with open(script_path, 'w') as f:
            f.write(dataloader_script)
        
        print(f"ğŸ“ åˆ›å»ºäº†DataLoaderè„šæœ¬: {script_path}")
        return script_path

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„æ•°æ®åŠ è½½å’Œdatasetç”Ÿæˆæµç¨‹"""
    print("ğŸš› çœŸå®Truckåœºæ™¯Patchæ•°æ®é›†ç”Ÿæˆ")
    print("=" * 60)
    
    # 1. åŠ è½½çœŸå®æ•°æ®
    loader = RealTruckDataLoader()
    
    # åŠ è½½å„ç§æ•°æ® (æ§åˆ¶è§„æ¨¡é¿å…å†…å­˜çˆ†ç‚¸)
    points_3d = loader.load_points_3d()
    gaussian_positions = loader.load_gaussian_positions(max_gaussians=50000)  # é™åˆ¶5ä¸‡ä¸ª
    images = loader.load_images(max_images=8, resolution_scale=4.0)  # 8å¼ å›¾ï¼Œ4xç¼©æ”¾
    cameras = loader.create_mock_cameras(len(images))
    
    # 2. åˆ›å»ºmocké«˜æ–¯çƒå¯¹è±¡
    mock_gaussians = type('MockGaussians', (), {
        'get_xyz': torch.from_numpy(gaussian_positions).float()
    })()
    
    # 3. åˆ›å»ºpatchåˆ†å‰²å™¨
    sampler = MultiModalPatchSampler(
        gaussians=mock_gaussians,
        points_3d=points_3d,
        images=images,
        cameras=cameras,
        target_gaussian_count=8000,  # æ¯ä¸ªpatch 8ké«˜æ–¯çƒ
        grid_resolution=4,           # 4x4x4ç½‘æ ¼
        overlap_ratio=0.1
    )
    
    # 4. ç”Ÿæˆpatch
    patches = sampler.create_multimodal_patches()
    
    # 5. ç”Ÿæˆæ•°æ®é›†
    dataset_generator = PatchDatasetGenerator("truck_patch_dataset")
    dataset_dir = dataset_generator.save_patch_data(
        patches, gaussian_positions, points_3d, images
    )
    
    # 6. åˆ›å»ºDataLoaderè„šæœ¬
    dataloader_script = dataset_generator.create_dataloader_script()
    
    print(f"\nğŸ¯ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"  æ•°æ®é›†ç›®å½•: {dataset_dir}")
    print(f"  Patchæ•°é‡: {len(patches)}")
    print(f"  DataLoader: {dataloader_script}")
    
    return dataset_dir, patches

if __name__ == "__main__":
    dataset_dir, patches = main()
