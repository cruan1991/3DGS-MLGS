#!/usr/bin/env python3
"""
å¿«é€Ÿé‚»å±…é¢„è®¡ç®—è„šæœ¬ - é‡‡æ ·ç‰ˆæœ¬
=====================================

ä¸“ä¸ºå¤§è§„æ¨¡æ•°æ®ä¼˜åŒ–ï¼Œé€šè¿‡æ™ºèƒ½é‡‡æ ·å¤§å¹…å‡å°‘è®¡ç®—é‡
"""

import argparse
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Union
import numpy as np
import torch

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_arrays(path: Union[str, Path]) -> torch.Tensor:
    """åŠ è½½ .pt æˆ– .npz æ ¼å¼çš„æ•°ç»„æ•°æ®"""
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    if path.suffix == '.pt':
        data = torch.load(path, map_location='cpu')
        if isinstance(data, dict):
            if 'xyz' in data:
                return data['xyz']
            elif len(data) == 1:
                return list(data.values())[0]
            else:
                raise ValueError(f"å­—å…¸æ ¼å¼çš„ .pt æ–‡ä»¶éœ€è¦åŒ…å« 'xyz' é”®: {list(data.keys())}")
        else:
            return data
    elif path.suffix == '.npz':
        data = np.load(path)
        if 'xyz' in data:
            return torch.from_numpy(data['xyz'])
        elif len(data.files) == 1:
            return torch.from_numpy(data[data.files[0]])
        else:
            raise ValueError(f"npz æ–‡ä»¶éœ€è¦åŒ…å« 'xyz' é”®: {list(data.keys())}")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")

def load_gaussian_data(path: Union[str, Path]) -> Tuple[torch.Tensor, torch.Tensor]:
    """åŠ è½½é«˜æ–¯æ•°æ®ï¼Œè¿”å› (xyz, scale)"""
    path = Path(path)
    
    if path.suffix == '.pt':
        data = torch.load(path, map_location='cpu')
        if isinstance(data, dict):
            xyz = data['xyz']
            scale = data['scale']
        else:
            raise ValueError("é«˜æ–¯æ•°æ®éœ€è¦å­—å…¸æ ¼å¼ï¼ŒåŒ…å« 'xyz' å’Œ 'scale' é”®")
    elif path.suffix == '.npz':
        data = np.load(path)
        xyz = torch.from_numpy(data['xyz'])
        scale = torch.from_numpy(data['scale'])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
    
    return xyz, scale

def smart_sample(data: torch.Tensor, target_size: int, seed: int = 42) -> Tuple[torch.Tensor, torch.Tensor]:
    """æ™ºèƒ½é‡‡æ ·ï¼Œä¿æŒæ•°æ®åˆ†å¸ƒ"""
    if len(data) <= target_size:
        return data, torch.arange(len(data))
    
    torch.manual_seed(seed)
    indices = torch.randperm(len(data))[:target_size]
    return data[indices], indices

def fast_ball_query_cpu(query_points: torch.Tensor, ref_points: torch.Tensor, 
                       radius: float, k: int) -> torch.Tensor:
    """CPUä¼˜åŒ–çš„çƒæŸ¥è¯¢å®ç°"""
    N = len(query_points)
    M = len(ref_points)
    
    # ç»“æœå­˜å‚¨
    neighbor_indices = torch.full((N, k), -1, dtype=torch.long)
    
    # åˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜
    batch_size = min(5000, N)
    
    for start_idx in range(0, N, batch_size):
        end_idx = min(start_idx + batch_size, N)
        batch_queries = query_points[start_idx:end_idx]  # [batch_size, 3]
        
        # è®¡ç®—è·ç¦» [batch_size, M]
        distances = torch.cdist(batch_queries, ref_points, p=2)
        
        # æ‰¾åˆ°åŠå¾„å†…çš„é‚»å±…
        for local_i, global_i in enumerate(range(start_idx, end_idx)):
            dist_row = distances[local_i]
            valid_mask = dist_row <= radius
            valid_indices = torch.where(valid_mask)[0]
            
            if len(valid_indices) > 0:
                valid_distances = dist_row[valid_indices]
                sorted_indices = torch.argsort(valid_distances)
                num_neighbors = min(k, len(valid_indices))
                selected = valid_indices[sorted_indices[:num_neighbors]]
                neighbor_indices[global_i, :num_neighbors] = selected
    
    return neighbor_indices

def to_csr(neighbor_indices: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """è½¬æ¢ä¸ºCSRæ ¼å¼"""
    N, K = neighbor_indices.shape
    
    # æ‰¾åˆ°æœ‰æ•ˆé‚»å±…
    valid_mask = neighbor_indices != -1
    neighbor_counts = valid_mask.sum(dim=1).int()
    
    # ç”Ÿæˆrow_splits
    row_splits = torch.zeros(N + 1, dtype=torch.int32)
    row_splits[1:] = torch.cumsum(neighbor_counts, dim=0)
    
    # æå–æœ‰æ•ˆç´¢å¼•
    indices = neighbor_indices[valid_mask].int()
    
    return indices, row_splits, neighbor_counts

def compute_stats(indices: torch.Tensor, row_splits: torch.Tensor, 
                 gauss_scale: torch.Tensor) -> Dict[str, torch.Tensor]:
    """è®¡ç®—é‚»åŸŸç»Ÿè®¡"""
    N = len(row_splits) - 1
    
    # è®¡ç®—å°ºåº¦å¤§å°
    scale_magnitudes = torch.norm(gauss_scale, dim=1)
    scale_p995 = torch.quantile(scale_magnitudes, 0.995)
    scale_magnitudes = torch.clamp(scale_magnitudes, max=scale_p995)
    log_scale = torch.log(scale_magnitudes + 1e-8)
    
    mean_log_scale = torch.zeros(N, dtype=torch.float32)
    scale_sum = torch.zeros(N, dtype=torch.float32)
    
    for i in range(N):
        start = row_splits[i].item()
        end = row_splits[i + 1].item()
        
        if start < end:
            neighbor_idx = indices[start:end]
            neighbor_log_scales = log_scale[neighbor_idx]
            neighbor_scales = scale_magnitudes[neighbor_idx]
            
            mean_log_scale[i] = neighbor_log_scales.mean()
            scale_sum[i] = neighbor_scales.sum()
    
    return {
        'mean_log_scale': mean_log_scale,
        'scale_sum': scale_sum
    }

def main():
    parser = argparse.ArgumentParser(description='å¿«é€Ÿé‚»å±…é¢„è®¡ç®—ï¼ˆé‡‡æ ·ç‰ˆæœ¬ï¼‰')
    parser.add_argument('--colmap', type=str, required=True,
                       help='COLMAP ç‚¹åæ ‡æ–‡ä»¶')
    parser.add_argument('--gauss', type=str, required=True,
                       help='é«˜æ–¯æ•°æ®æ–‡ä»¶')
    parser.add_argument('--radii', type=float, nargs='+',
                       default=[0.012, 0.039, 0.107, 0.273],
                       help='æŸ¥è¯¢åŠå¾„åˆ—è¡¨')
    parser.add_argument('--kmax', type=int, nargs='+',
                       default=[32, 128, 256, 512],
                       help='å„åŠå¾„çš„æœ€å¤§é‚»å±…æ•°')
    parser.add_argument('--colmap_sample', type=int, default=50000,
                       help='COLMAPé‡‡æ ·æ•°é‡')
    parser.add_argument('--gauss_sample', type=int, default=500000,
                       help='é«˜æ–¯çƒé‡‡æ ·æ•°é‡')
    parser.add_argument('--out', type=str, default='batches',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    if len(args.radii) != len(args.kmax):
        raise ValueError("radii å’Œ kmax çš„é•¿åº¦å¿…é¡»ç›¸åŒ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("ğŸš€ å¼€å§‹å¿«é€Ÿé‚»å±…é¢„è®¡ç®—...")
    start_time = time.time()
    
    # åŠ è½½æ•°æ®
    logger.info("ğŸ“‚ åŠ è½½æ•°æ®...")
    colmap_xyz = load_arrays(args.colmap).float()
    gauss_xyz, gauss_scale = load_gaussian_data(args.gauss)
    gauss_xyz = gauss_xyz.float()
    gauss_scale = gauss_scale.float()
    
    logger.info(f"åŸå§‹æ•°æ®: COLMAP={len(colmap_xyz):,}, é«˜æ–¯={len(gauss_xyz):,}")
    
    # æ¸…ç†æ— æ•ˆæ•°æ®
    valid_colmap = torch.isfinite(colmap_xyz).all(dim=1)
    valid_gauss = torch.isfinite(gauss_xyz).all(dim=1) & torch.isfinite(gauss_scale).all(dim=1)
    
    colmap_xyz = colmap_xyz[valid_colmap]
    gauss_xyz = gauss_xyz[valid_gauss]
    gauss_scale = gauss_scale[valid_gauss]
    
    logger.info(f"æ¸…ç†åæ•°æ®: COLMAP={len(colmap_xyz):,}, é«˜æ–¯={len(gauss_xyz):,}")
    
    # æ™ºèƒ½é‡‡æ ·
    logger.info("ğŸ¯ æ™ºèƒ½é‡‡æ ·...")
    colmap_sampled, colmap_indices = smart_sample(colmap_xyz, args.colmap_sample)
    gauss_sampled, gauss_indices = smart_sample(gauss_xyz, args.gauss_sample)
    gauss_scale_sampled = gauss_scale[gauss_indices]
    
    logger.info(f"é‡‡æ ·åæ•°æ®: COLMAP={len(colmap_sampled):,}, é«˜æ–¯={len(gauss_sampled):,}")
    
    # å¤šåŠå¾„è®¡ç®—
    results = {}
    
    for radius, kmax in zip(args.radii, args.kmax):
        logger.info(f"ğŸ” å¤„ç†åŠå¾„ {radius:.6f}, K={kmax}...")
        radius_start = time.time()
        
        # çƒæŸ¥è¯¢
        neighbor_idx = fast_ball_query_cpu(colmap_sampled, gauss_sampled, radius, kmax)
        
        # è½¬æ¢ä¸ºCSR
        indices, row_splits, count = to_csr(neighbor_idx)
        
        # è®¡ç®—ç»Ÿè®¡
        stats = compute_stats(indices, row_splits, gauss_scale_sampled)
        
        # è®°å½•ç»“æœ
        mean_neighbors = count.float().mean().item()
        median_neighbors = count.float().median().item()
        max_neighbors = count.max().item()
        
        results[f"{radius:.6f}"] = {
            'indices': indices,
            'row_splits': row_splits,
            'count': count,
            'mean_log_scale': stats['mean_log_scale'],
            'scale_sum': stats['scale_sum'],
            'radius': radius,
            'kmax': kmax,
            'colmap_sample_indices': colmap_indices,
            'gauss_sample_indices': gauss_indices,
            'stats': {
                'mean_neighbors': mean_neighbors,
                'median_neighbors': median_neighbors,
                'max_neighbors': max_neighbors,
                'total_edges': len(indices),
                'computation_time': time.time() - radius_start
            }
        }
        
        logger.info(f"  âœ… å®Œæˆ: å¹³å‡é‚»å±…æ•°={mean_neighbors:.2f}, è¾¹æ•°={len(indices):,}, è€—æ—¶={time.time()-radius_start:.1f}s")
    
    # ä¿å­˜ç»“æœ
    logger.info("ğŸ’¾ ä¿å­˜ç»“æœ...")
    for radius_str, data in results.items():
        output_path = out_dir / f"neigh_r{radius_str}_fast.pt"
        torch.save(data, output_path)
        logger.info(f"  ä¿å­˜: {output_path}")
    
    total_time = time.time() - start_time
    logger.info(f"ğŸ‰ å¿«é€Ÿé¢„è®¡ç®—å®Œæˆï¼æ€»è€—æ—¶: {total_time:.1f}s")
    
    # æ‰“å°æ±‡æ€»
    logger.info("\nğŸ“Š ç»“æœæ±‡æ€»:")
    for radius_str, data in results.items():
        stats = data['stats']
        logger.info(f"  åŠå¾„ {radius_str}: {stats['mean_neighbors']:.2f} å¹³å‡é‚»å±…, {stats['total_edges']:,} è¾¹")

if __name__ == "__main__":
    main() 