#!/usr/bin/env python3
"""
COLMAPç‚¹äº‘ vs 3DGSé«˜æ–¯çƒå¯¹åº”å…³ç³»åˆ†æå·¥å…·

ç›®æ ‡ï¼š
1. åˆ†æåŸå§‹COLMAPç‚¹ä¸æœ€ç»ˆé«˜æ–¯çƒçš„ç©ºé—´åˆ†å¸ƒå…³ç³»
2. å‘ç°densificationçš„æ¨¡å¼å’Œè§„å¾‹
3. æå–å¯ç”¨äºè®­ç»ƒçš„å…³è”ç‰¹å¾

ä½¿ç”¨æ–¹æ³•ï¼š
python correspondence_analysis.py --colmap-points points3D.ply --teacher-gaussians gaussian_ball.ply
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.neighbors import NearestNeighbors, KDTree
from sklearn.cluster import DBSCAN
import argparse
import sys
import warnings
from pathlib import Path
from tqdm import tqdm
import pandas as pd
from scipy.spatial.distance import cdist
from scipy.stats import gaussian_kde
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

warnings.filterwarnings("ignore")

# Add path for 3DGS imports
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene.gaussian_model import GaussianModel
from scene import dataset_readers
from plyfile import PlyData, PlyElement

class CorrespondenceAnalyzer:
    """åˆ†æCOLMAPç‚¹äº‘ä¸3DGSé«˜æ–¯çƒçš„å¯¹åº”å…³ç³»"""
    
    def __init__(self, colmap_points_path, teacher_gaussians_path):
        self.colmap_points_path = colmap_points_path
        self.teacher_gaussians_path = teacher_gaussians_path
        
        # æ•°æ®å­˜å‚¨
        self.colmap_points = None
        self.gaussian_positions = None
        self.gaussian_scales = None
        self.gaussian_opacities = None
        
        # åˆ†æç»“æœ
        self.correspondence_data = {}
        self.patterns = {}
        
    def load_data(self):
        """åŠ è½½COLMAPç‚¹äº‘å’ŒTeacheré«˜æ–¯çƒæ•°æ®"""
        print("=== æ•°æ®åŠ è½½ ===")
        
        # 1. åŠ è½½COLMAPç‚¹äº‘
        print(f"åŠ è½½COLMAPç‚¹äº‘: {self.colmap_points_path}")
        try:
            if self.colmap_points_path.endswith('.ply'):
                pcd = dataset_readers.fetchPly(self.colmap_points_path)
                self.colmap_points = np.stack([pcd.points['x'], pcd.points['y'], pcd.points['z']], axis=1)
            else:
                # å°è¯•å…¶ä»–æ ¼å¼
                raise ValueError(f"Unsupported format: {self.colmap_points_path}")
                
            print(f"âœ… COLMAPç‚¹äº‘åŠ è½½æˆåŠŸ: {len(self.colmap_points)} ä¸ªç‚¹")
            
        except Exception as e:
            print(f"âŒ COLMAPç‚¹äº‘åŠ è½½å¤±è´¥: {e}")
            return False
        
        # 2. åŠ è½½Teacheré«˜æ–¯çƒ
        print(f"åŠ è½½Teacheré«˜æ–¯çƒ: {self.teacher_gaussians_path}")
        try:
            # æ£€æµ‹SH degree
            plydata = PlyData.read(self.teacher_gaussians_path)
            vertex = plydata['vertex']
            f_rest_props = [prop for prop in vertex.properties if prop.name.startswith('f_rest_')]
            if f_rest_props:
                max_f_rest = max([int(prop.name.split('_')[-1]) for prop in f_rest_props])
                sh_degree = int(np.sqrt((max_f_rest + 4) / 3)) - 1
            else:
                sh_degree = 0
            
            # åŠ è½½é«˜æ–¯çƒ
            gaussians = GaussianModel(sh_degree)
            gaussians.load_ply(self.teacher_gaussians_path, use_train_test_exp=False)
            
            self.gaussian_positions = gaussians.get_xyz.detach().cpu().numpy()
            self.gaussian_scales = gaussians.get_scaling.detach().cpu().numpy()
            self.gaussian_opacities = gaussians.get_opacity.detach().cpu().numpy()
            
            # è¿‡æ»¤NaNå€¼
            valid_mask = ~(np.isnan(self.gaussian_positions).any(axis=1) | 
                          np.isnan(self.gaussian_scales).any(axis=1))
            
            if not valid_mask.all():
                n_invalid = (~valid_mask).sum()
                print(f"âš ï¸  è¿‡æ»¤æ‰ {n_invalid} ä¸ªåŒ…å«NaNçš„é«˜æ–¯çƒ")
                self.gaussian_positions = self.gaussian_positions[valid_mask]
                self.gaussian_scales = self.gaussian_scales[valid_mask]
                self.gaussian_opacities = self.gaussian_opacities[valid_mask]
            
            print(f"âœ… Teacheré«˜æ–¯çƒåŠ è½½æˆåŠŸ: {len(self.gaussian_positions)} ä¸ªé«˜æ–¯çƒ")
            
        except Exception as e:
            print(f"âŒ Teacheré«˜æ–¯çƒåŠ è½½å¤±è´¥: {e}")
            return False
        
        return True
    
    def analyze_spatial_distribution(self):
        """åˆ†æç©ºé—´åˆ†å¸ƒç‰¹å¾"""
        print("\n=== ç©ºé—´åˆ†å¸ƒåˆ†æ ===")
        
        # 1. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        colmap_stats = {
            'count': len(self.colmap_points),
            'bounds': {
                'min': self.colmap_points.min(axis=0),
                'max': self.colmap_points.max(axis=0),
                'range': self.colmap_points.max(axis=0) - self.colmap_points.min(axis=0)
            },
            'center': self.colmap_points.mean(axis=0),
            'std': self.colmap_points.std(axis=0)
        }
        
        gaussian_stats = {
            'count': len(self.gaussian_positions),
            'bounds': {
                'min': self.gaussian_positions.min(axis=0),
                'max': self.gaussian_positions.max(axis=0),
                'range': self.gaussian_positions.max(axis=0) - self.gaussian_positions.min(axis=0)
            },
            'center': self.gaussian_positions.mean(axis=0),
            'std': self.gaussian_positions.std(axis=0)
        }
        
        # 2. å¯†åº¦æ¯”è¾ƒ
        density_ratio = gaussian_stats['count'] / colmap_stats['count']
        
        print(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
        print(f"  COLMAPç‚¹æ•°: {colmap_stats['count']:,}")
        print(f"  é«˜æ–¯çƒæ•°: {gaussian_stats['count']:,}")
        print(f"  å¯†åº¦æ¯”ä¾‹: {density_ratio:.2f}x (å¹³å‡æ¯ä¸ªCOLMAPç‚¹å¯¹åº”{density_ratio:.1f}ä¸ªé«˜æ–¯çƒ)")
        
        print(f"ğŸ“Š ç©ºé—´èŒƒå›´:")
        print(f"  COLMAP: {colmap_stats['bounds']['range']}")
        print(f"  Gaussian: {gaussian_stats['bounds']['range']}")
        
        # 3. è®¡ç®—å±€éƒ¨å¯†åº¦
        colmap_density = self.compute_local_density(self.colmap_points, radius=0.1)
        gaussian_density = self.compute_local_density(self.gaussian_positions, radius=0.1)
        
        self.correspondence_data.update({
            'colmap_stats': colmap_stats,
            'gaussian_stats': gaussian_stats,
            'density_ratio': density_ratio,
            'colmap_local_density': colmap_density,
            'gaussian_local_density': gaussian_density
        })
        
        return colmap_stats, gaussian_stats
    
    def compute_local_density(self, points, radius=0.1, n_samples=5000):
        """è®¡ç®—å±€éƒ¨å¯†åº¦"""
        if len(points) > n_samples:
            # éšæœºé‡‡æ ·ä»¥æé«˜è®¡ç®—æ•ˆç‡
            indices = np.random.choice(len(points), n_samples, replace=False)
            sample_points = points[indices]
        else:
            sample_points = points
            indices = np.arange(len(points))
        
        # ä½¿ç”¨KDTreeè®¡ç®—å±€éƒ¨å¯†åº¦
        tree = KDTree(points)
        neighbor_counts = tree.query_radius(sample_points, r=radius, count_only=True)
        
        # å¯†åº¦ = é‚»å±…æ•°é‡ / çƒä½“ç§¯
        sphere_volume = (4/3) * np.pi * (radius ** 3)
        densities = neighbor_counts / sphere_volume
        
        return densities, indices
    
    def analyze_correspondence_patterns(self):
        """åˆ†æå¯¹åº”å…³ç³»æ¨¡å¼"""
        print("\n=== å¯¹åº”å…³ç³»æ¨¡å¼åˆ†æ ===")
        
        # 1. ä¸ºæ¯ä¸ªCOLMAPç‚¹æ‰¾åˆ°æœ€è¿‘çš„é«˜æ–¯çƒ
        print("è®¡ç®—æœ€è¿‘é‚»å¯¹åº”å…³ç³»...")
        tree = KDTree(self.gaussian_positions)
        
        # å¯¹æ¯ä¸ªCOLMAPç‚¹ï¼Œæ‰¾åˆ°æœ€è¿‘çš„kä¸ªé«˜æ–¯çƒ
        k_values = [1, 5, 10, 20, 30, 50]
        correspondence_results = {}
        
        for k in k_values:
            distances, indices = tree.query(self.colmap_points, k=k)
            correspondence_results[k] = {
                'distances': distances,
                'indices': indices,
                'mean_distance': distances.mean(),
                'std_distance': distances.std()
            }
            
        print(f"ğŸ“Š æœ€è¿‘é‚»è·ç¦»ç»Ÿè®¡ (k=1):")
        print(f"  å¹³å‡è·ç¦»: {correspondence_results[1]['mean_distance']:.4f}")
        print(f"  æ ‡å‡†å·®: {correspondence_results[1]['std_distance']:.4f}")
        print(f"  æœ€å¤§è·ç¦»: {correspondence_results[1]['distances'].max():.4f}")
        print(f"  æœ€å°è·ç¦»: {correspondence_results[1]['distances'].min():.4f}")
        
        # 2. åˆ†ædensificationæ¨¡å¼
        densification_analysis = self.analyze_densification_patterns(correspondence_results)
        
        # 3. åˆ†æç©ºé—´æ¢¯åº¦
        gradient_analysis = self.analyze_spatial_gradients()
        
        self.correspondence_data.update({
            'correspondence_results': correspondence_results,
            'densification_analysis': densification_analysis,
            'gradient_analysis': gradient_analysis
        })
        
        return correspondence_results
    
    def analyze_densification_patterns(self, correspondence_results):
        """åˆ†ædensificationæ¨¡å¼"""
        print("åˆ†ædensificationæ¨¡å¼...")
        
        # 1. è®¡ç®—æ¯ä¸ªCOLMAPç‚¹å‘¨å›´çš„é«˜æ–¯çƒå¯†åº¦
        radius_values = [0.05, 0.1, 0.2, 0.5]
        tree = KDTree(self.gaussian_positions)
        
        densification_patterns = {}
        
        for radius in radius_values:
            # è®¡ç®—æ¯ä¸ªCOLMAPç‚¹å‘¨å›´åŠå¾„å†…çš„é«˜æ–¯çƒæ•°é‡
            neighbor_counts = tree.query_radius(self.colmap_points, r=radius, count_only=True)
            
            densification_patterns[radius] = {
                'neighbor_counts': neighbor_counts,
                'mean_count': neighbor_counts.mean(),
                'std_count': neighbor_counts.std(),
                'max_count': neighbor_counts.max(),
                'min_count': neighbor_counts.min()
            }
            
            print(f"  åŠå¾„ {radius}: å¹³å‡{neighbor_counts.mean():.1f}ä¸ªé«˜æ–¯çƒ (Â±{neighbor_counts.std():.1f})")
        
        # 2. è¯†åˆ«é«˜å¯†åº¦å’Œä½å¯†åº¦åŒºåŸŸ
        radius = 0.1  # ä½¿ç”¨0.1ä½œä¸ºåˆ†æåŠå¾„
        neighbor_counts = densification_patterns[radius]['neighbor_counts']
        
        # ä½¿ç”¨å››åˆ†ä½æ•°åˆ’åˆ†å¯†åº¦ç­‰çº§
        q25, q50, q75 = np.percentile(neighbor_counts, [25, 50, 75])
        
        density_categories = {
            'low': neighbor_counts <= q25,
            'medium': (neighbor_counts > q25) & (neighbor_counts <= q75),
            'high': neighbor_counts > q75
        }
        
        print(f"ğŸ“Š å¯†åº¦åˆ†å¸ƒ (åŠå¾„{radius}):")
        for category, mask in density_categories.items():
            count = mask.sum()
            percentage = count / len(neighbor_counts) * 100
            print(f"  {category.capitalize()}: {count} ç‚¹ ({percentage:.1f}%)")
        
        # 3. åˆ†æä¸åŒå¯†åº¦åŒºåŸŸçš„ç‰¹å¾
        density_analysis = {}
        for category, mask in density_categories.items():
            if mask.sum() > 0:
                category_points = self.colmap_points[mask]
                category_counts = neighbor_counts[mask]
                
                density_analysis[category] = {
                    'points': category_points,
                    'counts': category_counts,
                    'mean_count': category_counts.mean(),
                    'std_count': category_counts.std(),
                    'spatial_variance': np.var(category_points, axis=0).mean()
                }
        
        return {
            'patterns': densification_patterns,
            'density_categories': density_categories,
            'density_analysis': density_analysis
        }
    
    def analyze_spatial_gradients(self):
        """åˆ†æç©ºé—´æ¢¯åº¦ç‰¹å¾"""
        print("åˆ†æç©ºé—´æ¢¯åº¦...")
        
        # 1. è®¡ç®—COLMAPç‚¹çš„å±€éƒ¨æ¢¯åº¦
        colmap_gradients = self.compute_point_gradients(self.colmap_points)
        
        # 2. è®¡ç®—é«˜æ–¯çƒçš„å±€éƒ¨æ¢¯åº¦
        gaussian_gradients = self.compute_point_gradients(self.gaussian_positions)
        
        # 3. åˆ†ææ¢¯åº¦ä¸densificationçš„å…³ç³»
        tree = KDTree(self.gaussian_positions)
        neighbor_counts = tree.query_radius(self.colmap_points, r=0.1, count_only=True)
        
        # è®¡ç®—æ¢¯åº¦å¼ºåº¦ä¸å¯†åº¦çš„ç›¸å…³æ€§
        gradient_magnitudes = np.linalg.norm(colmap_gradients, axis=1)
        correlation = np.corrcoef(gradient_magnitudes, neighbor_counts)[0, 1]
        
        print(f"ğŸ“Š æ¢¯åº¦-å¯†åº¦ç›¸å…³æ€§: {correlation:.3f}")
        
        return {
            'colmap_gradients': colmap_gradients,
            'gaussian_gradients': gaussian_gradients,
            'gradient_density_correlation': correlation,
            'gradient_magnitudes': gradient_magnitudes
        }
    
    def compute_point_gradients(self, points, k=10):
        """è®¡ç®—ç‚¹äº‘çš„å±€éƒ¨æ¢¯åº¦"""
        tree = KDTree(points)
        gradients = np.zeros_like(points)
        
        for i, point in enumerate(points):
            # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
            distances, indices = tree.query([point], k=k+1)  # +1 because it includes the point itself
            neighbors = points[indices[0][1:]]  # æ’é™¤è‡ªå·±
            
            # è®¡ç®—åˆ°é‚»å±…çš„å‘é‡
            vectors = neighbors - point
            
            # è®¡ç®—ä¸»æ–¹å‘ (ç¬¬ä¸€ä¸»æˆåˆ†)
            if len(vectors) > 0:
                cov_matrix = np.cov(vectors.T)
                eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
                # ä¸»æ–¹å‘æ˜¯æœ€å¤§ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
                principal_direction = eigenvecs[:, -1]
                gradients[i] = principal_direction
        
        return gradients
    
    def visualize_correspondence_analysis(self, save_dir='correspondence_analysis'):
        """å¯è§†åŒ–å¯¹åº”å…³ç³»åˆ†æç»“æœ"""
        print(f"\n=== ç”Ÿæˆå¯è§†åŒ–ç»“æœ ===")
        Path(save_dir).mkdir(exist_ok=True)
        
        # 1. 3Dæ•£ç‚¹å›¾å¯¹æ¯”
        self.plot_3d_comparison(save_dir)
        
        # 2. å¯†åº¦åˆ†å¸ƒå›¾
        self.plot_density_distribution(save_dir)
        
        # 3. å¯¹åº”å…³ç³»è·ç¦»åˆ†å¸ƒ
        self.plot_correspondence_distances(save_dir)
        
        # 4. Densificationæ¨¡å¼å¯è§†åŒ–
        self.plot_densification_patterns(save_dir)
        
        # 5. æ¢¯åº¦åˆ†æå›¾
        self.plot_gradient_analysis(save_dir)
        
        # 6. äº¤äº’å¼3Då¯è§†åŒ–
        self.create_interactive_3d_plot(save_dir)
        
        print(f"âœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° {save_dir}/")
    
    def plot_3d_comparison(self, save_dir):
        """3Dç©ºé—´åˆ†å¸ƒå¯¹æ¯”å›¾"""
        fig = plt.figure(figsize=(20, 8))
        
        # å­å›¾1: COLMAPç‚¹äº‘
        ax1 = fig.add_subplot(131, projection='3d')
        sample_colmap = self.colmap_points[::max(1, len(self.colmap_points)//5000)]
        ax1.scatter(sample_colmap[:, 0], sample_colmap[:, 1], sample_colmap[:, 2], 
                   c='blue', alpha=0.6, s=1)
        ax1.set_title('COLMAP Points')
        ax1.set_xlabel('X')
        ax1.set_ylabel('Y')
        ax1.set_zlabel('Z')
        
        # å­å›¾2: é«˜æ–¯çƒä½ç½®
        ax2 = fig.add_subplot(132, projection='3d')
        sample_gaussian = self.gaussian_positions[::max(1, len(self.gaussian_positions)//5000)]
        ax2.scatter(sample_gaussian[:, 0], sample_gaussian[:, 1], sample_gaussian[:, 2], 
                   c='red', alpha=0.6, s=1)
        ax2.set_title('Gaussian Positions')
        ax2.set_xlabel('X')
        ax2.set_ylabel('Y')
        ax2.set_zlabel('Z')
        
        # å­å›¾3: å åŠ å¯¹æ¯”
        ax3 = fig.add_subplot(133, projection='3d')
        ax3.scatter(sample_colmap[:, 0], sample_colmap[:, 1], sample_colmap[:, 2], 
                   c='blue', alpha=0.4, s=1, label='COLMAP')
        ax3.scatter(sample_gaussian[:, 0], sample_gaussian[:, 1], sample_gaussian[:, 2], 
                   c='red', alpha=0.4, s=1, label='Gaussians')
        ax3.set_title('Overlay Comparison')
        ax3.set_xlabel('X')
        ax3.set_ylabel('Y')
        ax3.set_zlabel('Z')
        ax3.legend()
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/3d_spatial_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_density_distribution(self, save_dir):
        """å¯†åº¦åˆ†å¸ƒåˆ†æå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. å±€éƒ¨å¯†åº¦ç›´æ–¹å›¾
        colmap_densities = self.correspondence_data['colmap_local_density'][0]
        gaussian_densities = self.correspondence_data['gaussian_local_density'][0]
        
        axes[0, 0].hist(colmap_densities, bins=50, alpha=0.7, label='COLMAP', color='blue')
        axes[0, 0].hist(gaussian_densities, bins=50, alpha=0.7, label='Gaussians', color='red')
        axes[0, 0].set_xlabel('Local Density')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Local Density Distribution')
        axes[0, 0].legend()
        axes[0, 0].set_yscale('log')
        
        # 2. å¯†åº¦æ¯”ä¾‹åˆ†æ
        densification_analysis = self.correspondence_data['densification_analysis']
        radius = 0.1
        neighbor_counts = densification_analysis['patterns'][radius]['neighbor_counts']
        
        axes[0, 1].hist(neighbor_counts, bins=50, alpha=0.7, color='green')
        axes[0, 1].axvline(neighbor_counts.mean(), color='red', linestyle='--', 
                          label=f'Mean: {neighbor_counts.mean():.1f}')
        axes[0, 1].set_xlabel('Gaussians per COLMAP Point')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title(f'Densification Distribution (r={radius})')
        axes[0, 1].legend()
        
        # 3. å¯†åº¦-è·ç¦»å…³ç³»
        correspondence_results = self.correspondence_data['correspondence_results']
        distances = correspondence_results[1]['distances'].flatten()
        
        axes[1, 0].scatter(distances, neighbor_counts, alpha=0.5, s=1)
        axes[1, 0].set_xlabel('Distance to Nearest Gaussian')
        axes[1, 0].set_ylabel('Local Gaussian Count')
        axes[1, 0].set_title('Distance vs Local Density')
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(distances, neighbor_counts)[0, 1]
        axes[1, 0].text(0.05, 0.95, f'Correlation: {correlation:.3f}', 
                       transform=axes[1, 0].transAxes, verticalalignment='top')
        
        # 4. å¯†åº¦åˆ†ç±»é¥¼å›¾
        density_categories = densification_analysis['density_categories']
        category_counts = [mask.sum() for mask in density_categories.values()]
        category_labels = list(density_categories.keys())
        
        axes[1, 1].pie(category_counts, labels=category_labels, autopct='%1.1f%%')
        axes[1, 1].set_title('Density Category Distribution')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/density_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_correspondence_distances(self, save_dir):
        """å¯¹åº”å…³ç³»è·ç¦»åˆ†æ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        correspondence_results = self.correspondence_data['correspondence_results']
        
        # 1. ä¸åŒkå€¼çš„è·ç¦»åˆ†å¸ƒ
        k_values = [1, 5, 10, 20]
        colors = ['blue', 'green', 'orange', 'red']
        
        for i, (k, color) in enumerate(zip(k_values, colors)):
            distances = correspondence_results[k]['distances']
            if k == 1:
                dist_to_plot = distances.flatten()
            else:
                dist_to_plot = distances.mean(axis=1)  # å¹³å‡è·ç¦»
            
            axes[0, 0].hist(dist_to_plot, bins=50, alpha=0.7, 
                           label=f'k={k}', color=color)
        
        axes[0, 0].set_xlabel('Distance')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Distance Distribution for Different k')
        axes[0, 0].legend()
        axes[0, 0].set_yscale('log')
        
        # 2. è·ç¦»ç»Ÿè®¡å¯¹æ¯”
        k_values_all = list(correspondence_results.keys())
        mean_distances = [correspondence_results[k]['mean_distance'] for k in k_values_all]
        std_distances = [correspondence_results[k]['std_distance'] for k in k_values_all]
        
        axes[0, 1].errorbar(k_values_all, mean_distances, yerr=std_distances, 
                           marker='o', capsize=5)
        axes[0, 1].set_xlabel('k (Number of Neighbors)')
        axes[0, 1].set_ylabel('Mean Distance')
        axes[0, 1].set_title('Distance Statistics vs k')
        axes[0, 1].grid(True)
        
        # 3. æœ€è¿‘é‚»è·ç¦»çš„ç©ºé—´åˆ†å¸ƒ
        distances_k1 = correspondence_results[1]['distances'].flatten()
        
        # åˆ›å»ºè·ç¦»çš„ç©ºé—´çƒ­å›¾ (æŠ•å½±åˆ°2D)
        x_coords = self.colmap_points[:, 0]
        y_coords = self.colmap_points[:, 1]
        
        scatter = axes[1, 0].scatter(x_coords, y_coords, c=distances_k1, 
                                    cmap='viridis', s=1, alpha=0.6)
        axes[1, 0].set_xlabel('X Coordinate')
        axes[1, 0].set_ylabel('Y Coordinate')
        axes[1, 0].set_title('Spatial Distribution of Nearest Distances')
        plt.colorbar(scatter, ax=axes[1, 0], label='Distance')
        
        # 4. è·ç¦»ç´¯ç§¯åˆ†å¸ƒå‡½æ•°
        sorted_distances = np.sort(distances_k1)
        cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances)
        
        axes[1, 1].plot(sorted_distances, cumulative)
        axes[1, 1].set_xlabel('Distance')
        axes[1, 1].set_ylabel('Cumulative Probability')
        axes[1, 1].set_title('Cumulative Distribution of Nearest Distances')
        axes[1, 1].grid(True)
        
        # æ·»åŠ ç™¾åˆ†ä½æ•°æ ‡è®°
        percentiles = [50, 90, 95, 99]
        for p in percentiles:
            dist_p = np.percentile(sorted_distances, p)
            axes[1, 1].axvline(dist_p, color='red', linestyle='--', alpha=0.7)
            axes[1, 1].text(dist_p, p/100, f'{p}%', rotation=90, 
                           verticalalignment='bottom')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/correspondence_distances.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_densification_patterns(self, save_dir):
        """Densificationæ¨¡å¼å¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        densification_analysis = self.correspondence_data['densification_analysis']
        
        # 1. ä¸åŒåŠå¾„çš„densificationåˆ†å¸ƒ
        radius_values = [0.05, 0.1, 0.2]
        for i, radius in enumerate(radius_values):
            neighbor_counts = densification_analysis['patterns'][radius]['neighbor_counts']
            
            axes[0, i].hist(neighbor_counts, bins=50, alpha=0.7, color='green')
            axes[0, i].axvline(neighbor_counts.mean(), color='red', linestyle='--', 
                              label=f'Mean: {neighbor_counts.mean():.1f}')
            axes[0, i].set_xlabel('Gaussian Count')
            axes[0, i].set_ylabel('Frequency')
            axes[0, i].set_title(f'Densification at r={radius}')
            axes[0, i].legend()
        
        # 2. å¯†åº¦åˆ†ç±»çš„ç©ºé—´åˆ†å¸ƒ
        density_categories = densification_analysis['density_categories']
        colors = {'low': 'blue', 'medium': 'green', 'high': 'red'}
        
        for i, (category, mask) in enumerate(density_categories.items()):
            category_points = self.colmap_points[mask]
            if len(category_points) > 0:
                axes[1, i].scatter(category_points[:, 0], category_points[:, 1], 
                                  c=colors[category], s=1, alpha=0.6)
                axes[1, i].set_xlabel('X Coordinate')
                axes[1, i].set_ylabel('Y Coordinate')
                axes[1, i].set_title(f'{category.capitalize()} Density Regions')
                axes[1, i].set_aspect('equal')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/densification_patterns.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_gradient_analysis(self, save_dir):
        """æ¢¯åº¦åˆ†æå¯è§†åŒ–"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        gradient_analysis = self.correspondence_data['gradient_analysis']
        gradient_magnitudes = gradient_analysis['gradient_magnitudes']
        
        # 1. æ¢¯åº¦å¼ºåº¦åˆ†å¸ƒ
        axes[0, 0].hist(gradient_magnitudes, bins=50, alpha=0.7, color='purple')
        axes[0, 0].axvline(gradient_magnitudes.mean(), color='red', linestyle='--',
                          label=f'Mean: {gradient_magnitudes.mean():.3f}')
        axes[0, 0].set_xlabel('Gradient Magnitude')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Gradient Magnitude Distribution')
        axes[0, 0].legend()
        
        # 2. æ¢¯åº¦vså¯†åº¦ç›¸å…³æ€§
        densification_analysis = self.correspondence_data['densification_analysis']
        neighbor_counts = densification_analysis['patterns'][0.1]['neighbor_counts']
        
        axes[0, 1].scatter(gradient_magnitudes, neighbor_counts, alpha=0.5, s=1)
        axes[0, 1].set_xlabel('Gradient Magnitude')
        axes[0, 1].set_ylabel('Local Gaussian Count')
        axes[0, 1].set_title('Gradient vs Densification')
        
        correlation = gradient_analysis['gradient_density_correlation']
        axes[0, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', 
                        transform=axes[0, 1].transAxes, verticalalignment='top')
        
        # 3. æ¢¯åº¦çš„ç©ºé—´åˆ†å¸ƒ
        x_coords = self.colmap_points[:, 0]
        y_coords = self.colmap_points[:, 1]
        
        scatter = axes[1, 0].scatter(x_coords, y_coords, c=gradient_magnitudes, 
                                    cmap='plasma', s=1, alpha=0.6)
        axes[1, 0].set_xlabel('X Coordinate')
        axes[1, 0].set_ylabel('Y Coordinate')
        axes[1, 0].set_title('Spatial Distribution of Gradients')
        plt.colorbar(scatter, ax=axes[1, 0], label='Gradient Magnitude')
        
        # 4. æ¢¯åº¦æ–¹å‘åˆ†æï¼ˆ2DæŠ•å½±ï¼‰
        colmap_gradients = gradient_analysis['colmap_gradients']
        gradient_angles = np.arctan2(colmap_gradients[:, 1], colmap_gradients[:, 0])
        
        axes[1, 1].hist(gradient_angles, bins=36, alpha=0.7, color='orange')
        axes[1, 1].set_xlabel('Gradient Direction (radians)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Gradient Direction Distribution')
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/gradient_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def create_interactive_3d_plot(self, save_dir):
        """åˆ›å»ºäº¤äº’å¼3Då¯è§†åŒ–"""
        print("ç”Ÿæˆäº¤äº’å¼3Då¯è§†åŒ–...")
        
        # é‡‡æ ·æ•°æ®ä»¥æé«˜æ€§èƒ½
        n_sample = 5000
        colmap_sample_idx = np.random.choice(len(self.colmap_points), 
                                           min(n_sample, len(self.colmap_points)), 
                                           replace=False)
        gaussian_sample_idx = np.random.choice(len(self.gaussian_positions), 
                                             min(n_sample, len(self.gaussian_positions)), 
                                             replace=False)
        
        colmap_sample = self.colmap_points[colmap_sample_idx]
        gaussian_sample = self.gaussian_positions[gaussian_sample_idx]
        
        # åˆ›å»ºäº¤äº’å¼plotlyå›¾
        fig = go.Figure()
        
        # æ·»åŠ COLMAPç‚¹äº‘
        fig.add_trace(go.Scatter3d(
            x=colmap_sample[:, 0],
            y=colmap_sample[:, 1], 
            z=colmap_sample[:, 2],
            mode='markers',
            marker=dict(
                size=3,
                color='blue',
                opacity=0.6
            ),
            name='COLMAP Points',
            text=[f'COLMAP Point {i}' for i in colmap_sample_idx]
        ))
        
        # æ·»åŠ é«˜æ–¯çƒä½ç½®
        fig.add_trace(go.Scatter3d(
            x=gaussian_sample[:, 0],
            y=gaussian_sample[:, 1],
            z=gaussian_sample[:, 2],
            mode='markers',
            marker=dict(
                size=2,
                color='red',
                opacity=0.4
            ),
            name='Gaussian Positions',
            text=[f'Gaussian {i}' for i in gaussian_sample_idx]
        ))
        
        fig.update_layout(
            title='Interactive 3D Correspondence Analysis',
            scene=dict(
                xaxis_title='X',
                yaxis_title='Y',
                zaxis_title='Z'
            ),
            width=1000,
            height=800
        )
        
        # ä¿å­˜äº¤äº’å¼å›¾
        fig.write_html(f'{save_dir}/interactive_3d_comparison.html')
        print(f"âœ… äº¤äº’å¼3Då¯è§†åŒ–å·²ä¿å­˜åˆ° {save_dir}/interactive_3d_comparison.html")
    
    def generate_correspondence_report(self, save_dir='correspondence_analysis'):
        """ç”Ÿæˆå¯¹åº”å…³ç³»åˆ†ææŠ¥å‘Š"""
        print("\n=== ç”Ÿæˆåˆ†ææŠ¥å‘Š ===")
        
        report_path = f'{save_dir}/correspondence_analysis_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("COLMAPç‚¹äº‘ vs 3DGSé«˜æ–¯çƒå¯¹åº”å…³ç³»åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            
            # 1. åŸºç¡€ç»Ÿè®¡
            f.write("1. åŸºç¡€ç»Ÿè®¡ä¿¡æ¯\n")
            f.write("-" * 30 + "\n")
            colmap_stats = self.correspondence_data['colmap_stats']
            gaussian_stats = self.correspondence_data['gaussian_stats']
            density_ratio = self.correspondence_data['density_ratio']
            
            f.write(f"COLMAPç‚¹æ•°: {colmap_stats['count']:,}\n")
            f.write(f"é«˜æ–¯çƒæ•°: {gaussian_stats['count']:,}\n")
            f.write(f"å¯†åº¦æ¯”ä¾‹: {density_ratio:.2f}x\n")
            f.write(f"å¹³å‡æ¯ä¸ªCOLMAPç‚¹å¯¹åº”: {density_ratio:.1f} ä¸ªé«˜æ–¯çƒ\n\n")
            
            # 2. ç©ºé—´åˆ†å¸ƒ
            f.write("2. ç©ºé—´åˆ†å¸ƒç‰¹å¾\n")
            f.write("-" * 30 + "\n")
            f.write(f"COLMAPç©ºé—´èŒƒå›´: {colmap_stats['bounds']['range']}\n")
            f.write(f"é«˜æ–¯çƒç©ºé—´èŒƒå›´: {gaussian_stats['bounds']['range']}\n")
            f.write(f"COLMAPä¸­å¿ƒ: {colmap_stats['center']}\n")
            f.write(f"é«˜æ–¯çƒä¸­å¿ƒ: {gaussian_stats['center']}\n\n")
            
            # 3. å¯¹åº”å…³ç³»åˆ†æ
            f.write("3. å¯¹åº”å…³ç³»åˆ†æ\n")
            f.write("-" * 30 + "\n")
            correspondence_results = self.correspondence_data['correspondence_results']
            
            for k in [1, 5, 10, 20, 30]:
                if k in correspondence_results:
                    result = correspondence_results[k]
                    f.write(f"k={k}: å¹³å‡è·ç¦» {result['mean_distance']:.4f} Â± {result['std_distance']:.4f}\n")
            
            f.write("\n")
            
            # 4. Densificationåˆ†æ
            f.write("4. Densificationæ¨¡å¼åˆ†æ\n")
            f.write("-" * 30 + "\n")
            densification_analysis = self.correspondence_data['densification_analysis']
            
            for radius in [0.05, 0.1, 0.2]:
                if radius in densification_analysis['patterns']:
                    pattern = densification_analysis['patterns'][radius]
                    f.write(f"åŠå¾„{radius}: å¹³å‡{pattern['mean_count']:.1f}ä¸ªé«˜æ–¯çƒ (Â±{pattern['std_count']:.1f})\n")
            
            # å¯†åº¦åˆ†ç±»ç»Ÿè®¡
            f.write("\nå¯†åº¦åˆ†ç±» (åŠå¾„0.1):\n")
            density_categories = densification_analysis['density_categories']
            total_points = len(self.colmap_points)
            
            for category, mask in density_categories.items():
                count = mask.sum()
                percentage = count / total_points * 100
                f.write(f"  {category.capitalize()}: {count} ç‚¹ ({percentage:.1f}%)\n")
            
            f.write("\n")
            
            # 5. æ¢¯åº¦åˆ†æ
            f.write("5. æ¢¯åº¦åˆ†æ\n")
            f.write("-" * 30 + "\n")
            gradient_analysis = self.correspondence_data['gradient_analysis']
            
            correlation = gradient_analysis['gradient_density_correlation']
            f.write(f"æ¢¯åº¦-å¯†åº¦ç›¸å…³æ€§: {correlation:.3f}\n")
            
            gradient_magnitudes = gradient_analysis['gradient_magnitudes']
            f.write(f"æ¢¯åº¦å¼ºåº¦ç»Ÿè®¡:\n")
            f.write(f"  å¹³å‡å€¼: {gradient_magnitudes.mean():.4f}\n")
            f.write(f"  æ ‡å‡†å·®: {gradient_magnitudes.std():.4f}\n")
            f.write(f"  æœ€å¤§å€¼: {gradient_magnitudes.max():.4f}\n")
            f.write(f"  æœ€å°å€¼: {gradient_magnitudes.min():.4f}\n\n")
            
            # 6. å…³é”®å‘ç°
            f.write("6. å…³é”®å‘ç°ä¸æ´å¯Ÿ\n")
            f.write("-" * 30 + "\n")
            
            # åˆ†æç»“æœå¹¶ç”Ÿæˆæ´å¯Ÿ
            insights = self.generate_insights()
            for insight in insights:
                f.write(f"â€¢ {insight}\n")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ° {report_path}")
        return report_path
    
    def generate_insights(self):
        """åŸºäºåˆ†æç»“æœç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # 1. å¯†åº¦æ¯”ä¾‹æ´å¯Ÿ
        density_ratio = self.correspondence_data['density_ratio']
        if density_ratio > 50:
            insights.append(f"å¯†åº¦æ¯”ä¾‹æé«˜({density_ratio:.1f}x)ï¼Œè¯´æ˜3DGSè¿›è¡Œäº†å¤§é‡densificationæ“ä½œ")
        elif density_ratio > 10:
            insights.append(f"å¯†åº¦æ¯”ä¾‹è¾ƒé«˜({density_ratio:.1f}x)ï¼Œå­˜åœ¨æ˜¾è‘—çš„densification")
        else:
            insights.append(f"å¯†åº¦æ¯”ä¾‹é€‚ä¸­({density_ratio:.1f}x)ï¼Œdensificationç¨‹åº¦æ¸©å’Œ")
        
        # 2. å¯¹åº”å…³ç³»è·ç¦»æ´å¯Ÿ
        correspondence_results = self.correspondence_data['correspondence_results']
        mean_distance = correspondence_results[1]['mean_distance']
        
        if mean_distance < 0.01:
            insights.append("COLMAPç‚¹ä¸æœ€è¿‘é«˜æ–¯çƒè·ç¦»å¾ˆå°ï¼Œç©ºé—´å¯¹åº”å…³ç³»è¾ƒå¥½")
        elif mean_distance < 0.1:
            insights.append("COLMAPç‚¹ä¸æœ€è¿‘é«˜æ–¯çƒè·ç¦»é€‚ä¸­ï¼Œå­˜åœ¨ä¸€å®šçš„ç©ºé—´åç§»")
        else:
            insights.append("COLMAPç‚¹ä¸æœ€è¿‘é«˜æ–¯çƒè·ç¦»è¾ƒå¤§ï¼Œç©ºé—´å¯¹åº”å…³ç³»è¾ƒå¼±")
        
        # 3. æ¢¯åº¦ç›¸å…³æ€§æ´å¯Ÿ
        gradient_analysis = self.correspondence_data['gradient_analysis']
        correlation = gradient_analysis['gradient_density_correlation']
        
        if abs(correlation) > 0.3:
            insights.append(f"æ¢¯åº¦ä¸densificationå­˜åœ¨{'æ­£' if correlation > 0 else 'è´Ÿ'}ç›¸å…³æ€§({correlation:.3f})ï¼Œå‡ ä½•å¤æ‚åº¦å½±å“é«˜æ–¯çƒç”Ÿæˆ")
        else:
            insights.append(f"æ¢¯åº¦ä¸densificationç›¸å…³æ€§è¾ƒå¼±({correlation:.3f})ï¼Œå‡ ä½•å¤æ‚åº¦å¯¹é«˜æ–¯çƒç”Ÿæˆå½±å“æœ‰é™")
        
        # 4. å¯†åº¦åˆ†å¸ƒæ´å¯Ÿ
        densification_analysis = self.correspondence_data['densification_analysis']
        density_categories = densification_analysis['density_categories']
        
        high_density_ratio = density_categories['high'].sum() / len(self.colmap_points)
        if high_density_ratio > 0.3:
            insights.append(f"é«˜å¯†åº¦åŒºåŸŸå æ¯”è¾ƒå¤§({high_density_ratio:.1%})ï¼Œåœºæ™¯å­˜åœ¨å¤§é‡ç»†èŠ‚éœ€è¦densification")
        else:
            insights.append(f"é«˜å¯†åº¦åŒºåŸŸå æ¯”é€‚ä¸­({high_density_ratio:.1%})ï¼Œdensificationåˆ†å¸ƒç›¸å¯¹å‡åŒ€")
        
        # 5. ç©ºé—´åˆ†å¸ƒæ´å¯Ÿ
        colmap_stats = self.correspondence_data['colmap_stats']
        gaussian_stats = self.correspondence_data['gaussian_stats']
        
        range_ratio = gaussian_stats['bounds']['range'] / colmap_stats['bounds']['range']
        range_ratio_mean = range_ratio.mean()
        
        if range_ratio_mean > 1.2:
            insights.append("é«˜æ–¯çƒåˆ†å¸ƒèŒƒå›´è¶…å‡ºCOLMAPç‚¹äº‘ï¼Œå­˜åœ¨åœºæ™¯å¤–æ¨ç”Ÿæˆ")
        elif range_ratio_mean < 0.8:
            insights.append("é«˜æ–¯çƒåˆ†å¸ƒèŒƒå›´å°äºCOLMAPç‚¹äº‘ï¼Œå¯èƒ½å­˜åœ¨è¾¹ç•Œæ”¶ç¼©")
        else:
            insights.append("é«˜æ–¯çƒä¸COLMAPç‚¹äº‘ç©ºé—´èŒƒå›´åŸºæœ¬ä¸€è‡´")
        
        return insights
    
    def extract_training_features(self, save_dir='correspondence_analysis'):
        """æå–å¯ç”¨äºè®­ç»ƒçš„ç‰¹å¾"""
        print("\n=== æå–è®­ç»ƒç‰¹å¾ ===")
        
        # 1. ä¸ºæ¯ä¸ªCOLMAPç‚¹æå–ç‰¹å¾
        features = {}
        
        # åŸºç¡€ä½ç½®ç‰¹å¾
        features['positions'] = self.colmap_points
        
        # å±€éƒ¨å¯†åº¦ç‰¹å¾
        tree = KDTree(self.gaussian_positions)
        for radius in [0.05, 0.1, 0.2]:
            neighbor_counts = tree.query_radius(self.colmap_points, r=radius, count_only=True)
            features[f'density_r{radius}'] = neighbor_counts
        
        # æ¢¯åº¦ç‰¹å¾
        gradient_analysis = self.correspondence_data['gradient_analysis']
        features['gradient_magnitude'] = gradient_analysis['gradient_magnitudes']
        features['gradient_vectors'] = gradient_analysis['colmap_gradients']
        
        # æœ€è¿‘é‚»è·ç¦»ç‰¹å¾
        correspondence_results = self.correspondence_data['correspondence_results']
        for k in [1, 5, 10]:
            if k in correspondence_results:
                distances = correspondence_results[k]['distances']
                if k == 1:
                    features[f'nearest_distance_k{k}'] = distances.flatten()
                else:
                    features[f'nearest_distance_k{k}'] = distances.mean(axis=1)
        
        # å±€éƒ¨å‡ ä½•å¤æ‚åº¦
        complexity_scores = self.compute_local_complexity()
        features['geometric_complexity'] = complexity_scores
        
        # ä¿å­˜ç‰¹å¾
        feature_path = f'{save_dir}/training_features.npz'
        np.savez(feature_path, **features)
        
        print(f"âœ… è®­ç»ƒç‰¹å¾å·²ä¿å­˜åˆ° {feature_path}")
        print(f"ğŸ“Š æå–çš„ç‰¹å¾:")
        for key, value in features.items():
            if isinstance(value, np.ndarray):
                print(f"  {key}: shape {value.shape}, dtype {value.dtype}")
        
        return features
    
    def compute_local_complexity(self, k=15):
        """è®¡ç®—å±€éƒ¨å‡ ä½•å¤æ‚åº¦"""
        tree = KDTree(self.colmap_points)
        complexity_scores = np.zeros(len(self.colmap_points))
        
        for i, point in enumerate(self.colmap_points):
            # æ‰¾åˆ°kä¸ªæœ€è¿‘é‚»
            distances, indices = tree.query([point], k=k+1)
            neighbors = self.colmap_points[indices[0][1:]]  # æ’é™¤è‡ªå·±
            
            if len(neighbors) > 3:
                # è®¡ç®—å±€éƒ¨è¡¨é¢çš„ä¸»æ›²ç‡
                centered_neighbors = neighbors - point
                
                # è®¡ç®—åæ–¹å·®çŸ©é˜µ
                cov_matrix = np.cov(centered_neighbors.T)
                eigenvals = np.linalg.eigvals(cov_matrix)
                eigenvals = np.sort(eigenvals)[::-1]  # é™åºæ’åˆ—
                
                # ä½¿ç”¨ç‰¹å¾å€¼æ¯”ä¾‹ä½œä¸ºå¤æ‚åº¦æŒ‡æ ‡
                if eigenvals[0] > 1e-8:
                    # çº¿æ€§åº¦ï¼šÎ»1 >> Î»2, Î»3
                    linearity = (eigenvals[0] - eigenvals[1]) / eigenvals[0]
                    # å¹³é¢åº¦ï¼šÎ»2 >> Î»3
                    planarity = (eigenvals[1] - eigenvals[2]) / eigenvals[0] if eigenvals[0] > 1e-8 else 0
                    # çƒå½¢åº¦ï¼šÎ»1 â‰ˆ Î»2 â‰ˆ Î»3
                    sphericity = eigenvals[2] / eigenvals[0] if eigenvals[0] > 1e-8 else 0
                    
                    # å¤æ‚åº¦ = 1 - æœ€å¤§çš„å½¢çŠ¶ç‰¹å¾ï¼ˆè¶Šä¸è§„åˆ™è¶Šå¤æ‚ï¼‰
                    complexity = 1.0 - max(linearity, planarity, sphericity)
                else:
                    complexity = 1.0  # å¦‚æœç‚¹é‡åˆï¼Œè®¤ä¸ºæ˜¯é«˜å¤æ‚åº¦
                
                complexity_scores[i] = complexity
        
        return complexity_scores
    
    def run_full_analysis(self, save_dir='correspondence_analysis'):
        """è¿è¡Œå®Œæ•´çš„å¯¹åº”å…³ç³»åˆ†æ"""
        print("ğŸ¯ å¼€å§‹COLMAPç‚¹äº‘ vs 3DGSé«˜æ–¯çƒå¯¹åº”å…³ç³»åˆ†æ")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œåˆ†æç»ˆæ­¢")
            return False
        
        # 2. ç©ºé—´åˆ†å¸ƒåˆ†æ
        self.analyze_spatial_distribution()
        
        # 3. å¯¹åº”å…³ç³»åˆ†æ
        self.analyze_correspondence_patterns()
        
        # 4. ç”Ÿæˆå¯è§†åŒ–
        self.visualize_correspondence_analysis(save_dir)
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        self.generate_correspondence_report(save_dir)
        
        # 6. æå–è®­ç»ƒç‰¹å¾
        self.extract_training_features(save_dir)
        
        print("\n" + "=" * 60)
        print("âœ… å¯¹åº”å…³ç³»åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {save_dir}/")
        print("ğŸ“‹ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
        print(f"  â€¢ correspondence_analysis_report.txt - åˆ†ææŠ¥å‘Š")
        print(f"  â€¢ training_features.npz - è®­ç»ƒç‰¹å¾")
        print(f"  â€¢ *.png - å¯è§†åŒ–å›¾è¡¨")
        print(f"  â€¢ interactive_3d_comparison.html - äº¤äº’å¼3Då›¾")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='COLMAPç‚¹äº‘ä¸3DGSé«˜æ–¯çƒå¯¹åº”å…³ç³»åˆ†æ')
    parser.add_argument('--colmap-points', required=True, 
                       help='COLMAPç‚¹äº‘æ–‡ä»¶è·¯å¾„ (points3D.ply)')
    parser.add_argument('--teacher-gaussians', required=True,
                       help='Teacheré«˜æ–¯çƒæ–‡ä»¶è·¯å¾„ (gaussian_ball.ply)')
    parser.add_argument('--output-dir', default='correspondence_analysis',
                       help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.colmap_points).exists():
        print(f"âŒ COLMAPç‚¹äº‘æ–‡ä»¶ä¸å­˜åœ¨: {args.colmap_points}")
        return
    
    if not Path(args.teacher_gaussians).exists():
        print(f"âŒ Teacheré«˜æ–¯çƒæ–‡ä»¶ä¸å­˜åœ¨: {args.teacher_gaussians}")
        return
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = CorrespondenceAnalyzer(
        colmap_points_path=args.colmap_points,
        teacher_gaussians_path=args.teacher_gaussians
    )
    
    success = analyzer.run_full_analysis(save_dir=args.output_dir)
    
    if success:
        print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼æŸ¥çœ‹ {args.output_dir}/ è·å–è¯¦ç»†ç»“æœã€‚")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ–‡ä»¶å’Œå‚æ•°ã€‚")

if __name__ == "__main__":
    main()