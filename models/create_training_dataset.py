#!/usr/bin/env python3
"""
Student Networkè®­ç»ƒæ•°æ®é›†åˆ›å»ºè„šæœ¬
=================================

åŸºäºé¢„è®¡ç®—çš„é‚»å±…å…³ç³»ï¼Œåˆ›å»ºCOLMAPâ†’3DGSçš„è®­ç»ƒæ•°æ®é›†
- è¾“å…¥ç‰¹å¾ï¼šCOLMAPç‚¹çš„å‡ ä½•å’Œç©ºé—´ç‰¹å¾
- è¾“å‡ºæ ‡ç­¾ï¼šå¯¹åº”é«˜æ–¯çƒçš„èšåˆç‰¹å¾
- æ”¯æŒå¤šå°ºåº¦é‚»åŸŸèšåˆ
- æ•°æ®å¢å¼ºå’Œè´¨é‡æ§åˆ¶
"""

import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import logging
import h5py
from torch.utils.data import Dataset, DataLoader
import argparse
from dataclasses import dataclass
import json

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    neighbor_data_dir: str = "batches"
    output_dir: str = "training_data"
    
    # æ•°æ®é€‰æ‹©
    use_radii: List[float] = None  # Noneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰åŠå¾„
    min_neighbors: int = 3  # æœ€å°‘é‚»å±…æ•°
    max_neighbors: int = 512  # æœ€å¤§é‚»å±…æ•°
    
    # ç‰¹å¾é…ç½®
    use_spatial_features: bool = True
    use_density_features: bool = True
    use_scale_features: bool = True
    
    # æ•°æ®åˆ†å‰²
    train_ratio: float = 0.8
    val_ratio: float = 0.1
    test_ratio: float = 0.1
    
    # å…¶ä»–
    random_seed: int = 42
    
    def __post_init__(self):
        if self.use_radii is None:
            self.use_radii = [0.012, 0.039, 0.107, 0.273]

class NeighborDataLoader:
    """é‚»å±…æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.neighbor_files = {}
        self.colmap_data = None
        self.gaussian_data = None
        
    def load_base_data(self):
        """åŠ è½½åŸºç¡€æ•°æ®"""
        logger.info("åŠ è½½åŸºç¡€COLMAPå’Œé«˜æ–¯æ•°æ®...")
        
        # åŠ è½½COLMAPæ•°æ®
        colmap_file = self.data_dir / "colmap_points.pt"
        if colmap_file.exists():
            self.colmap_data = torch.load(colmap_file, map_location='cpu')
            logger.info(f"COLMAPæ•°æ®: {self.colmap_data.shape}")
        else:
            raise FileNotFoundError(f"COLMAPæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {colmap_file}")
        
        # åŠ è½½é«˜æ–¯æ•°æ®
        gaussian_file = self.data_dir / "gaussian_data.pt"
        if gaussian_file.exists():
            gaussian_dict = torch.load(gaussian_file, map_location='cpu')
            self.gaussian_data = {
                'xyz': gaussian_dict['xyz'],
                'scale': gaussian_dict['scale']
            }
            logger.info(f"é«˜æ–¯æ•°æ®: xyz={self.gaussian_data['xyz'].shape}, scale={self.gaussian_data['scale'].shape}")
        else:
            raise FileNotFoundError(f"é«˜æ–¯æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {gaussian_file}")
    
    def load_neighbor_data(self, radii: List[float]):
        """åŠ è½½é‚»å±…æ•°æ®"""
        logger.info(f"åŠ è½½é‚»å±…æ•°æ®ï¼ŒåŠå¾„: {radii}")
        
        for radius in radii:
            # å°è¯•å¿«é€Ÿç‰ˆæœ¬å’Œå®Œæ•´ç‰ˆæœ¬
            fast_file = self.data_dir / f"neigh_r{radius:.6f}_fast.pt"
            full_file = self.data_dir / f"neigh_r{radius:.6f}.pt"
            
            if fast_file.exists():
                neighbor_file = fast_file
                logger.info(f"ä½¿ç”¨å¿«é€Ÿç‰ˆæœ¬: {neighbor_file}")
            elif full_file.exists():
                neighbor_file = full_file
                logger.info(f"ä½¿ç”¨å®Œæ•´ç‰ˆæœ¬: {neighbor_file}")
            else:
                logger.warning(f"æœªæ‰¾åˆ°åŠå¾„ {radius} çš„é‚»å±…æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            neighbor_data = torch.load(neighbor_file, map_location='cpu')
            self.neighbor_files[radius] = neighbor_data
            
            logger.info(f"  åŠå¾„ {radius}: {len(neighbor_data['indices'])} æ¡é‚»æ¥è¾¹")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(self.neighbor_files)} ä¸ªåŠå¾„çš„é‚»å±…æ•°æ®")

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨"""
    
    def __init__(self, colmap_data: torch.Tensor, gaussian_data: Dict[str, torch.Tensor]):
        self.colmap_xyz = colmap_data.float()
        self.gaussian_xyz = gaussian_data['xyz'].float()
        self.gaussian_scale = gaussian_data['scale'].float()
        
        # é¢„è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self._compute_global_stats()
    
    def _compute_global_stats(self):
        """è®¡ç®—å…¨å±€ç»Ÿè®¡ä¿¡æ¯ç”¨äºç‰¹å¾å½’ä¸€åŒ–"""
        # COLMAPç‚¹çš„ç»Ÿè®¡
        self.colmap_mean = self.colmap_xyz.mean(dim=0)
        self.colmap_std = self.colmap_xyz.std(dim=0)
        
        # é«˜æ–¯çƒçš„ç»Ÿè®¡  
        self.gaussian_mean = self.gaussian_xyz.mean(dim=0)
        self.gaussian_std = self.gaussian_xyz.std(dim=0)
        
        # å°ºåº¦ç»Ÿè®¡
        scale_magnitudes = torch.norm(self.gaussian_scale, dim=1)
        self.scale_mean = scale_magnitudes.mean()
        self.scale_std = scale_magnitudes.std()
        
        logger.info(f"å…¨å±€ç»Ÿè®¡ä¿¡æ¯å·²è®¡ç®—")
    
    def extract_colmap_features(self, point_indices: torch.Tensor) -> torch.Tensor:
        """æå–COLMAPç‚¹ç‰¹å¾"""
        points = self.colmap_xyz[point_indices]  # [N, 3]
        
        features = []
        
        # 1. å½’ä¸€åŒ–åæ ‡
        normalized_coords = (points - self.colmap_mean) / (self.colmap_std + 1e-8)
        features.append(normalized_coords)
        
        # 2. ç©ºé—´ç‰¹å¾ï¼šè·ç¦»åŸç‚¹
        distances = torch.norm(points, dim=1, keepdim=True)
        features.append(distances)
        
        # 3. å±€éƒ¨å¯†åº¦ä¼°è®¡ï¼ˆåŸºäºæœ€è¿‘é‚»ï¼‰
        if len(point_indices) > 1:
            # è®¡ç®—åˆ°æœ€è¿‘é‚»çš„è·ç¦»
            pairwise_dist = torch.cdist(points, points)
            # æ’é™¤è‡ªèº«ï¼ˆå¯¹è§’çº¿ï¼‰
            pairwise_dist.fill_diagonal_(float('inf'))
            nearest_dist = pairwise_dist.min(dim=1)[0].unsqueeze(1)
            density_features = 1.0 / (nearest_dist + 1e-6)  # å¯†åº¦å€’æ•°
        else:
            density_features = torch.zeros(len(points), 1)
        
        features.append(density_features)
        
        return torch.cat(features, dim=1)  # [N, feature_dim]
    
    def aggregate_gaussian_features(self, neighbor_indices: torch.Tensor, 
                                  row_splits: torch.Tensor) -> torch.Tensor:
        """èšåˆé«˜æ–¯çƒç‰¹å¾ä½œä¸ºæ ‡ç­¾"""
        N = len(row_splits) - 1
        features = []
        
        for i in range(N):
            start = row_splits[i].item()
            end = row_splits[i + 1].item()
            
            if start < end:
                # è·å–é‚»å±…ç´¢å¼•
                neighbor_idx = neighbor_indices[start:end]
                neighbor_xyz = self.gaussian_xyz[neighbor_idx]  # [K, 3]
                neighbor_scale = self.gaussian_scale[neighbor_idx]  # [K, 3]
                
                # èšåˆç‰¹å¾
                point_features = self._aggregate_single_point(neighbor_xyz, neighbor_scale)
            else:
                # æ²¡æœ‰é‚»å±…çš„ç‚¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                point_features = torch.zeros(self._get_output_feature_dim())
            
            features.append(point_features)
        
        return torch.stack(features, dim=0)  # [N, output_dim]
    
    def _aggregate_single_point(self, neighbor_xyz: torch.Tensor, 
                               neighbor_scale: torch.Tensor) -> torch.Tensor:
        """èšåˆå•ä¸ªç‚¹çš„é‚»å±…ç‰¹å¾"""
        features = []
        
        # 1. ä½ç½®ç‰¹å¾ï¼šè´¨å¿ƒ
        centroid = neighbor_xyz.mean(dim=0)
        features.append(centroid)
        
        # 2. ä½ç½®åˆ†å¸ƒï¼šæ ‡å‡†å·®
        position_std = neighbor_xyz.std(dim=0)
        features.append(position_std)
        
        # 3. å°ºåº¦ç‰¹å¾ï¼šå¹³å‡å°ºåº¦
        mean_scale = neighbor_scale.mean(dim=0)
        features.append(mean_scale)
        
        # 4. å°ºåº¦åˆ†å¸ƒï¼šæ ‡å‡†å·®
        scale_std = neighbor_scale.std(dim=0)
        features.append(scale_std)
        
        # 5. é‚»å±…æ•°é‡ï¼ˆå¯¹æ•°ï¼‰
        num_neighbors = torch.log(torch.tensor(len(neighbor_xyz)) + 1.0)
        features.append(num_neighbors.unsqueeze(0))
        
        # 6. å°ºåº¦å¤§å°ç»Ÿè®¡
        scale_magnitudes = torch.norm(neighbor_scale, dim=1)
        scale_mean = scale_magnitudes.mean().unsqueeze(0)
        scale_max = scale_magnitudes.max().unsqueeze(0)
        scale_min = scale_magnitudes.min().unsqueeze(0)
        features.extend([scale_mean, scale_max, scale_min])
        
        return torch.cat(features, dim=0)
    
    def _get_output_feature_dim(self) -> int:
        """è·å–è¾“å‡ºç‰¹å¾ç»´åº¦"""
        # centroid(3) + position_std(3) + mean_scale(3) + scale_std(3) + 
        # num_neighbors(1) + scale_mean(1) + scale_max(1) + scale_min(1)
        return 16

class TrainingDatasetCreator:
    """è®­ç»ƒæ•°æ®é›†åˆ›å»ºå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.data_loader = NeighborDataLoader(config.neighbor_data_dir)
        self.feature_extractor = None
        
    def create_dataset(self):
        """åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹åˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
        
        # 1. åŠ è½½æ•°æ®
        self.data_loader.load_base_data()
        self.data_loader.load_neighbor_data(self.config.use_radii)
        
        # 2. åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor(
            self.data_loader.colmap_data,
            self.data_loader.gaussian_data
        )
        
        # 3. å¤„ç†æ¯ä¸ªåŠå¾„
        all_samples = []
        
        for radius in self.config.use_radii:
            if radius not in self.data_loader.neighbor_files:
                logger.warning(f"è·³è¿‡åŠå¾„ {radius}ï¼Œæ•°æ®ä¸å­˜åœ¨")
                continue
            
            logger.info(f"å¤„ç†åŠå¾„ {radius}...")
            samples = self._process_radius_data(radius)
            all_samples.extend(samples)
            logger.info(f"  ç”Ÿæˆ {len(samples)} ä¸ªæ ·æœ¬")
        
        logger.info(f"æ€»å…±ç”Ÿæˆ {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # 4. æ•°æ®åˆ†å‰²å’Œä¿å­˜
        self._split_and_save_dataset(all_samples)
        
        logger.info("âœ… è®­ç»ƒæ•°æ®é›†åˆ›å»ºå®Œæˆï¼")
    
    def _process_radius_data(self, radius: float) -> List[Dict]:
        """å¤„ç†å•ä¸ªåŠå¾„çš„æ•°æ®"""
        neighbor_data = self.data_loader.neighbor_files[radius]
        
        indices = neighbor_data['indices']
        row_splits = neighbor_data['row_splits'] 
        count = neighbor_data['count']
        
        # æ ·æœ¬ç´¢å¼•åˆ—è¡¨
        samples = []
        
        N = len(count)
        valid_indices = []
        
        # ç­›é€‰æœ‰æ•ˆæ ·æœ¬
        for i in range(N):
            neighbor_count = count[i].item()
            if self.config.min_neighbors <= neighbor_count <= self.config.max_neighbors:
                valid_indices.append(i)
        
        logger.info(f"  æœ‰æ•ˆæ ·æœ¬: {len(valid_indices)}/{N}")
        
        if len(valid_indices) == 0:
            return []
        
        # æ‰¹é‡æå–ç‰¹å¾
        valid_indices = torch.tensor(valid_indices)
        
        # è¾“å…¥ç‰¹å¾ï¼šCOLMAPç‚¹ç‰¹å¾
        input_features = self.feature_extractor.extract_colmap_features(valid_indices)
        
        # è¾“å‡ºæ ‡ç­¾ï¼šé«˜æ–¯çƒèšåˆç‰¹å¾
        output_features = self.feature_extractor.aggregate_gaussian_features(indices, row_splits)
        output_features = output_features[valid_indices]
        
        # åˆ›å»ºæ ·æœ¬
        for i, idx in enumerate(valid_indices):
            sample = {
                'input_features': input_features[i].numpy(),
                'output_features': output_features[i].numpy(),
                'radius': radius,
                'colmap_index': idx.item(),
                'neighbor_count': count[idx].item(),
            }
            samples.append(sample)
        
        return samples
    
    def _split_and_save_dataset(self, all_samples: List[Dict]):
        """åˆ†å‰²å¹¶ä¿å­˜æ•°æ®é›†"""
        logger.info("åˆ†å‰²å¹¶ä¿å­˜æ•°æ®é›†...")
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(self.config.random_seed)
        
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(len(all_samples))
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        n_total = len(all_samples)
        n_train = int(n_total * self.config.train_ratio)
        n_val = int(n_total * self.config.val_ratio)
        
        train_indices = indices[:n_train]
        val_indices = indices[n_train:n_train + n_val]
        test_indices = indices[n_train + n_val:]
        
        # åˆ†å‰²æ•°æ®
        train_samples = [all_samples[i] for i in train_indices]
        val_samples = [all_samples[i] for i in val_indices]
        test_samples = [all_samples[i] for i in test_indices]
        
        logger.info(f"æ•°æ®åˆ†å‰²: è®­ç»ƒ={len(train_samples)}, éªŒè¯={len(val_samples)}, æµ‹è¯•={len(test_samples)}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°æ®é›†
        self._save_split('train', train_samples, output_dir)
        self._save_split('val', val_samples, output_dir)
        self._save_split('test', test_samples, output_dir)
        
        # ä¿å­˜é…ç½®å’Œç»Ÿè®¡ä¿¡æ¯
        self._save_metadata(output_dir, len(train_samples), len(val_samples), len(test_samples))
    
    def _save_split(self, split_name: str, samples: List[Dict], output_dir: Path):
        """ä¿å­˜æ•°æ®åˆ†å‰²"""
        if len(samples) == 0:
            logger.warning(f"è·³è¿‡ç©ºçš„ {split_name} åˆ†å‰²")
            return
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        input_features = np.stack([s['input_features'] for s in samples])
        output_features = np.stack([s['output_features'] for s in samples])
        
        # å…ƒæ•°æ®
        metadata = {
            'radius': [s['radius'] for s in samples],
            'colmap_index': [s['colmap_index'] for s in samples],
            'neighbor_count': [s['neighbor_count'] for s in samples],
        }
        
        # ä¿å­˜ä¸ºHDF5æ ¼å¼
        h5_file = output_dir / f"{split_name}.h5"
        with h5py.File(h5_file, 'w') as f:
            f.create_dataset('input_features', data=input_features, compression='gzip')
            f.create_dataset('output_features', data=output_features, compression='gzip')
            
            # ä¿å­˜å…ƒæ•°æ®
            for key, values in metadata.items():
                f.create_dataset(f'meta_{key}', data=values)
        
        logger.info(f"ä¿å­˜ {split_name}: {h5_file} ({len(samples)} æ ·æœ¬)")
    
    def _save_metadata(self, output_dir: Path, n_train: int, n_val: int, n_test: int):
        """ä¿å­˜å…ƒæ•°æ®"""
        metadata = {
            'config': self.config.__dict__,
            'dataset_stats': {
                'total_samples': n_train + n_val + n_test,
                'train_samples': n_train,
                'val_samples': n_val,
                'test_samples': n_test,
            },
            'feature_dims': {
                'input_dim': len(self.feature_extractor.extract_colmap_features(torch.tensor([0]))[0]),
                'output_dim': self.feature_extractor._get_output_feature_dim(),
            },
            'radii_used': list(self.data_loader.neighbor_files.keys()),
        }
        
        metadata_file = output_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)
        
        logger.info(f"å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")

class StudentNetworkDataset(Dataset):
    """PyTorchæ•°æ®é›†ç±»"""
    
    def __init__(self, h5_file: str):
        self.h5_file = h5_file
        
        with h5py.File(h5_file, 'r') as f:
            self.n_samples = len(f['input_features'])
            self.input_dim = f['input_features'].shape[1]
            self.output_dim = f['output_features'].shape[1]
    
    def __len__(self):
        return self.n_samples
    
    def __getitem__(self, idx):
        with h5py.File(self.h5_file, 'r') as f:
            input_features = torch.from_numpy(f['input_features'][idx]).float()
            output_features = torch.from_numpy(f['output_features'][idx]).float()
            
            return {
                'input': input_features,
                'target': output_features,
                'radius': f['meta_radius'][idx],
                'colmap_index': f['meta_colmap_index'][idx],
                'neighbor_count': f['meta_neighbor_count'][idx],
            }

def main():
    parser = argparse.ArgumentParser(description='åˆ›å»ºStudent Networkè®­ç»ƒæ•°æ®é›†')
    parser.add_argument('--neighbor_data_dir', type=str, default='batches',
                       help='é‚»å±…æ•°æ®ç›®å½•')
    parser.add_argument('--output_dir', type=str, default='training_data',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--min_neighbors', type=int, default=3,
                       help='æœ€å°‘é‚»å±…æ•°')
    parser.add_argument('--max_neighbors', type=int, default=512,
                       help='æœ€å¤§é‚»å±…æ•°')
    parser.add_argument('--train_ratio', type=float, default=0.8,
                       help='è®­ç»ƒé›†æ¯”ä¾‹')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = TrainingConfig(
        neighbor_data_dir=args.neighbor_data_dir,
        output_dir=args.output_dir,
        min_neighbors=args.min_neighbors,
        max_neighbors=args.max_neighbors,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=1.0 - args.train_ratio - args.val_ratio,
    )
    
    # åˆ›å»ºæ•°æ®é›†
    creator = TrainingDatasetCreator(config)
    creator.create_dataset()
    
    # æµ‹è¯•æ•°æ®åŠ è½½
    logger.info("æµ‹è¯•æ•°æ®åŠ è½½...")
    train_file = Path(config.output_dir) / "train.h5"
    if train_file.exists():
        dataset = StudentNetworkDataset(str(train_file))
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
        
        # æµ‹è¯•ä¸€ä¸ªæ‰¹æ¬¡
        for batch in dataloader:
            logger.info(f"æ‰¹æ¬¡æµ‹è¯•æˆåŠŸ:")
            logger.info(f"  è¾“å…¥å½¢çŠ¶: {batch['input'].shape}")
            logger.info(f"  è¾“å‡ºå½¢çŠ¶: {batch['target'].shape}")
            logger.info(f"  åŠå¾„èŒƒå›´: {batch['radius'].min():.6f} - {batch['radius'].max():.6f}")
            break
    
    logger.info("ğŸ‰ è®­ç»ƒæ•°æ®é›†åˆ›å»ºå¹¶éªŒè¯å®Œæˆï¼")

if __name__ == "__main__":
    main() 