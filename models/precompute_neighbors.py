#!/usr/bin/env python3
"""
ç¦»çº¿é¢„è®¡ç®— COLMAP ç‚¹åˆ° 3DGS é«˜æ–¯ä¸­å¿ƒçš„é‚»æ¥å…³ç³»
===================================================

åŠŸèƒ½ï¼š
- æ”¯æŒå¤šåŠå¾„çƒæŸ¥è¯¢ï¼ˆball_queryï¼‰
- ç”Ÿæˆ CSR æ ¼å¼çš„å˜é•¿é‚»æ¥è¡¨
- åˆ†å—å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼ˆ2M+ é«˜æ–¯çƒï¼‰
- ç»Ÿè®¡é‚»åŸŸå†…å°ºåº¦ä¿¡æ¯
- GPU åŠ é€Ÿè®¡ç®—

ä¾èµ–ï¼š
    pip install torch torchvision
    pip install pytorch3d  # æ¨è
    # æˆ–è€… pip install pointnet2_ops  # å¤‡é€‰

ç”¨æ³•ï¼š
    python precompute_neighbors.py \
      --colmap data/colmap_xyz.pt \
      --gauss  data/gaussians.pt \
      --radii  0.012 0.039 0.107 0.273 \
      --kmax   128 512 1024 2048 \
      --chunk  200000 \
      --out    cache/neighbors/
"""

import argparse
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Union
import numpy as np
import torch
import torch.nn.functional as F

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================= ä¾èµ–æ£€æŸ¥ =============================

def check_and_import_ball_query():
    """æ£€æŸ¥å¹¶å¯¼å…¥çƒæŸ¥è¯¢å‡½æ•°ï¼Œä¼˜å…ˆçº§ï¼špytorch3d > pointnet2_ops > torch_fallback"""
    ball_query_func = None
    backend_name = ""
    
    # å°è¯• pytorch3d
    try:
        from pytorch3d.ops import ball_query
        # æµ‹è¯•å¯¼å…¥æ˜¯å¦çœŸçš„å¯ç”¨
        torch.manual_seed(42)
        test_p1 = torch.randn(1, 10, 3, device='cuda' if torch.cuda.is_available() else 'cpu')
        test_p2 = torch.randn(1, 20, 3, device='cuda' if torch.cuda.is_available() else 'cpu')
        _ = ball_query(test_p1, test_p2, radius=0.1, K=5)
        ball_query_func = ball_query
        backend_name = "pytorch3d"
        logger.info("âœ… ä½¿ç”¨ pytorch3d.ops.ball_query")
    except (ImportError, RuntimeError, Exception) as e:
        logger.warning(f"pytorch3d ä¸å¯ç”¨ ({e})ï¼Œå°è¯• pointnet2_ops...")
        
        # å°è¯• pointnet2_ops
        try:
            from pointnet2_ops import pointnet2_utils
            def ball_query_wrapper(p1, p2, radius, K):
                # pointnet2_ops æ¥å£é€‚é…
                idx = pointnet2_utils.ball_query(radius, K, p2, p1)
                return idx
            ball_query_func = ball_query_wrapper
            backend_name = "pointnet2_ops"
            logger.info("âœ… ä½¿ç”¨ pointnet2_ops.ball_query")
        except (ImportError, RuntimeError, Exception) as e:
            logger.warning(f"pointnet2_ops ä¸å¯ç”¨ ({e})ï¼Œä½¿ç”¨çº¯PyTorchå®ç°...")
            
            # çº¯PyTorchå®ç°çš„çƒæŸ¥è¯¢
            def torch_ball_query(p1, p2, radius, K):
                """
                å†…å­˜ä¼˜åŒ–çš„çº¯PyTorchçƒæŸ¥è¯¢å®ç°
                Args:
                    p1: [B, N, 3] æŸ¥è¯¢ç‚¹
                    p2: [B, M, 3] å‚è€ƒç‚¹
                    radius: æŸ¥è¯¢åŠå¾„
                    K: æœ€å¤§é‚»å±…æ•°
                Returns:
                    [B, N, K] é‚»å±…ç´¢å¼•ï¼Œ-1è¡¨ç¤ºæ— æ•ˆ
                """
                B, N, _ = p1.shape
                _, M, _ = p2.shape
                device = p1.device
                
                # åˆå§‹åŒ–ç»“æœ
                neighbor_indices = torch.full((B, N, K), -1, dtype=torch.long, device=device)
                
                # åˆ†æ‰¹å¤„ç†æŸ¥è¯¢ç‚¹ä»¥èŠ‚çœå†…å­˜
                query_batch_size = min(1000, N)  # æ¯æ¬¡å¤„ç†1000ä¸ªæŸ¥è¯¢ç‚¹
                
                for b in range(B):
                    for start_n in range(0, N, query_batch_size):
                        end_n = min(start_n + query_batch_size, N)
                        batch_p1 = p1[b, start_n:end_n]  # [batch_size, 3]
                        batch_p2 = p2[b]  # [M, 3]
                        
                        # è®¡ç®—è·ç¦»çŸ©é˜µ [batch_size, M]
                        batch_p1_expanded = batch_p1.unsqueeze(1)  # [batch_size, 1, 3]
                        batch_p2_expanded = batch_p2.unsqueeze(0)  # [1, M, 3]
                        distances = torch.norm(batch_p1_expanded - batch_p2_expanded, dim=2)  # [batch_size, M]
                        
                        # æ‰¾åˆ°åŠå¾„å†…çš„ç‚¹
                        valid_mask = distances <= radius  # [batch_size, M]
                        
                        # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç‚¹æ‰¾åˆ°æœ€è¿‘çš„Kä¸ªé‚»å±…
                        for local_n in range(end_n - start_n):
                            global_n = start_n + local_n
                            valid_neighbors = torch.where(valid_mask[local_n])[0]
                            
                            if len(valid_neighbors) > 0:
                                neighbor_distances = distances[local_n, valid_neighbors]
                                # æ’åºå¹¶é€‰æ‹©æœ€è¿‘çš„Kä¸ª
                                sorted_indices = torch.argsort(neighbor_distances)
                                num_neighbors = min(K, len(valid_neighbors))
                                selected_neighbors = valid_neighbors[sorted_indices[:num_neighbors]]
                                neighbor_indices[b, global_n, :num_neighbors] = selected_neighbors
                
                return neighbor_indices
            
            ball_query_func = torch_ball_query
            backend_name = "torch_fallback"
            logger.info("âœ… ä½¿ç”¨çº¯PyTorchçƒæŸ¥è¯¢å®ç°")
    
    return ball_query_func, backend_name

# ============================= æ•°æ®åŠ è½½ =============================

def load_arrays(path: Union[str, Path]) -> torch.Tensor:
    """åŠ è½½ .pt æˆ– .npz æ ¼å¼çš„æ•°ç»„æ•°æ®"""
    path = Path(path)
    
    if not path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    if path.suffix == '.pt':
        data = torch.load(path, map_location='cpu')
        if isinstance(data, dict):
            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æ‰¾åˆ°åæ ‡æ•°æ®
            if 'xyz' in data:
                return data['xyz']
            elif len(data) == 1:
                return list(data.values())[0]
            else:
                raise ValueError(f"å­—å…¸æ ¼å¼çš„ .pt æ–‡ä»¶éœ€è¦åŒ…å« 'xyz' é”®: {list(data.keys())}")
        else:
            return data
    elif path.suffix == '.npz':
        data = np.load(path)
        if 'xyz' in data:
            return torch.from_numpy(data['xyz'])
        elif len(data.files) == 1:
            return torch.from_numpy(data[data.files[0]])
        else:
            raise ValueError(f"npz æ–‡ä»¶éœ€è¦åŒ…å« 'xyz' é”®: {list(data.keys())}")
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")

def load_gaussian_data(path: Union[str, Path]) -> Tuple[torch.Tensor, torch.Tensor]:
    """åŠ è½½é«˜æ–¯æ•°æ®ï¼Œè¿”å› (xyz, scale)"""
    path = Path(path)
    
    if path.suffix == '.pt':
        data = torch.load(path, map_location='cpu')
        if isinstance(data, dict):
            xyz = data['xyz']
            scale = data['scale']
        else:
            raise ValueError("é«˜æ–¯æ•°æ®éœ€è¦å­—å…¸æ ¼å¼ï¼ŒåŒ…å« 'xyz' å’Œ 'scale' é”®")
    elif path.suffix == '.npz':
        data = np.load(path)
        xyz = torch.from_numpy(data['xyz'])
        scale = torch.from_numpy(data['scale'])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}")
    
    return xyz, scale

# ============================= CSR å¤„ç† =============================

def to_csr(neighbor_indices: torch.Tensor, num_query_points: int) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    å°†é‚»å±…ç´¢å¼•è½¬æ¢ä¸º CSR æ ¼å¼
    
    Args:
        neighbor_indices: [N, K] é‚»å±…ç´¢å¼•ï¼Œ-1 è¡¨ç¤ºæ— æ•ˆ
        num_query_points: æŸ¥è¯¢ç‚¹æ•°é‡ N
    
    Returns:
        indices: [nnz] æ‰å¹³åŒ–çš„æœ‰æ•ˆé‚»å±…ç´¢å¼•
        row_splits: [N+1] æ¯è¡Œçš„èµ·å§‹ä½ç½®
    """
    # æ‰¾åˆ°æœ‰æ•ˆé‚»å±…ï¼ˆé -1ï¼‰
    valid_mask = neighbor_indices != -1
    
    # è®¡ç®—æ¯ä¸ªæŸ¥è¯¢ç‚¹çš„é‚»å±…æ•°é‡
    neighbor_counts = valid_mask.sum(dim=1).int()
    
    # ç”Ÿæˆ row_splits
    row_splits = torch.zeros(num_query_points + 1, dtype=torch.int32)
    row_splits[1:] = torch.cumsum(neighbor_counts, dim=0)
    
    # æå–æœ‰æ•ˆçš„é‚»å±…ç´¢å¼•
    indices = neighbor_indices[valid_mask].int()
    
    return indices, row_splits

def compute_neighbor_stats(indices: torch.Tensor, row_splits: torch.Tensor, 
                          gauss_scale: torch.Tensor) -> Dict[str, torch.Tensor]:
    """è®¡ç®—é‚»åŸŸç»Ÿè®¡ä¿¡æ¯"""
    num_points = len(row_splits) - 1
    
    # è®¡ç®— log scale magnitudes
    scale_magnitudes = torch.norm(gauss_scale, dim=1)  # [M]
    # è£å‰ªæç«¯å€¼ï¼ˆ99.5åˆ†ä½æ•°ï¼‰
    scale_p995 = torch.quantile(scale_magnitudes, 0.995)
    scale_magnitudes = torch.clamp(scale_magnitudes, max=scale_p995)
    log_scale_magnitudes = torch.log(scale_magnitudes + 1e-8)  # [M]
    
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢ç‚¹è®¡ç®—é‚»åŸŸç»Ÿè®¡
    mean_log_scale = torch.zeros(num_points, dtype=torch.float32)
    scale_sum = torch.zeros(num_points, dtype=torch.float32)
    
    for i in range(num_points):
        start_idx = row_splits[i].item()
        end_idx = row_splits[i + 1].item()
        
        if start_idx < end_idx:
            neighbor_idx = indices[start_idx:end_idx]
            neighbor_log_scales = log_scale_magnitudes[neighbor_idx]
            neighbor_scales = scale_magnitudes[neighbor_idx]
            
            mean_log_scale[i] = neighbor_log_scales.mean()
            scale_sum[i] = neighbor_scales.sum()
        else:
            # æ— é‚»å±…çš„æƒ…å†µ
            mean_log_scale[i] = 0.0
            scale_sum[i] = 0.0
    
    return {
        'mean_log_scale': mean_log_scale,
        'scale_sum': scale_sum
    }

# ============================= çƒæŸ¥è¯¢ =============================

def ball_query_multi_radius(colmap_xyz: torch.Tensor, gauss_xyz: torch.Tensor,
                           radii: List[float], kmax_list: List[int], 
                           chunk_size: int, ball_query_func, backend_name: str) -> Dict[str, Dict]:
    """
    å¤šåŠå¾„çƒæŸ¥è¯¢ï¼Œåˆ†å—å¤„ç†å¤§æ•°æ®
    
    Returns:
        Dict[radius_str, CSRDict] where CSRDict contains:
            - indices: torch.Tensor
            - row_splits: torch.Tensor  
            - count: torch.Tensor
            - mean_log_scale: torch.Tensor
            - scale_sum: torch.Tensor
    """
    device = colmap_xyz.device
    N = len(colmap_xyz)
    M = len(gauss_xyz)
    
    results = {}
    
    # è®¡ç®—é«˜æ–¯çƒçš„å°ºåº¦ä¿¡æ¯ï¼ˆç”¨äºåç»­ç»Ÿè®¡ï¼‰
    logger.info("é¢„è®¡ç®—é«˜æ–¯çƒå°ºåº¦ä¿¡æ¯...")
    if gauss_xyz.shape[1] > 3:
        # å‡è®¾å3åˆ—æ˜¯å°ºåº¦
        gauss_scale = gauss_xyz[:, 3:6] if gauss_xyz.shape[1] >= 6 else gauss_xyz[:, 3:4].repeat(1, 3)
    else:
        # å¦‚æœæ²¡æœ‰å°ºåº¦ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
        gauss_scale = torch.ones(M, 3, device=device) * 0.01
        logger.warning("æœªæ‰¾åˆ°å°ºåº¦ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼ 0.01")
    
    for radius, kmax in zip(radii, kmax_list):
        logger.info(f"å¤„ç†åŠå¾„ {radius:.6f}, æœ€å¤§é‚»å±…æ•° {kmax}...")
        
        # å­˜å‚¨æ‰€æœ‰é‚»å±…ç´¢å¼•
        all_neighbor_indices = []
        
        # åˆ†å—å¤„ç†
        num_chunks = (M + chunk_size - 1) // chunk_size
        
        for chunk_i in range(num_chunks):
            start_idx = chunk_i * chunk_size
            end_idx = min((chunk_i + 1) * chunk_size, M)
            
            gauss_chunk = gauss_xyz[start_idx:end_idx, :3]  # åªå–åæ ‡
            
            logger.info(f"  å— {chunk_i+1}/{num_chunks}: é«˜æ–¯çƒ {start_idx}-{end_idx}")
            
            # çƒæŸ¥è¯¢
            if backend_name == "pytorch3d":
                # pytorch3d: ball_query(p1, p2, radius, K)
                neighbor_idx = ball_query_func(
                    colmap_xyz.unsqueeze(0),  # [1, N, 3]
                    gauss_chunk.unsqueeze(0),  # [1, M_chunk, 3]
                    radius=radius,
                    K=kmax
                )[0].squeeze(0)  # [N, K]
                
            elif backend_name == "pointnet2_ops":
                # pointnet2_ops éœ€è¦ [1, N, 3] å’Œ [1, M_chunk, 3]
                neighbor_idx = ball_query_func(
                    colmap_xyz.unsqueeze(0),  # [1, N, 3]
                    gauss_chunk.unsqueeze(0),  # [1, M_chunk, 3]
                    radius, kmax
                ).squeeze(0)  # [N, K]
                
            elif backend_name == "torch_fallback":
                # çº¯PyTorchå®ç°
                neighbor_idx = ball_query_func(
                    colmap_xyz.unsqueeze(0),  # [1, N, 3]
                    gauss_chunk.unsqueeze(0),  # [1, M_chunk, 3]
                    radius, kmax
                ).squeeze(0)  # [N, K]
                
            # è°ƒæ•´ç´¢å¼•åˆ°å…¨å±€ï¼ˆå¯¹æ‰€æœ‰åç«¯éƒ½é€‚ç”¨ï¼‰
            valid_mask = neighbor_idx != -1
            neighbor_idx[valid_mask] += start_idx
            
            all_neighbor_indices.append(neighbor_idx)
        
        # åˆå¹¶æ‰€æœ‰å—çš„ç»“æœ
        logger.info("åˆå¹¶åˆ†å—ç»“æœ...")
        combined_neighbors = torch.cat(all_neighbor_indices, dim=1)  # [N, K*num_chunks]
        
        # æ¯ä¸ªç‚¹åªä¿ç•™æœ€è¿‘çš„ kmax ä¸ªé‚»å±…
        if combined_neighbors.shape[1] > kmax:
            # è®¡ç®—è·ç¦»å¹¶æ’åº
            distances = []
            for i in range(N):
                query_point = colmap_xyz[i:i+1]  # [1, 3]
                neighbor_mask = combined_neighbors[i] != -1
                if neighbor_mask.any():
                    neighbor_points = gauss_xyz[combined_neighbors[i][neighbor_mask], :3]  # [K_valid, 3]
                    dist = torch.norm(neighbor_points - query_point, dim=1)
                    distances.append(dist)
                else:
                    distances.append(torch.tensor([], device=device))
            
            # é‡æ–°é€‰æ‹©æœ€è¿‘çš„é‚»å±…
            final_neighbors = torch.full((N, kmax), -1, dtype=torch.long, device=device)
            for i in range(N):
                neighbor_mask = combined_neighbors[i] != -1
                if neighbor_mask.any():
                    valid_neighbors = combined_neighbors[i][neighbor_mask]
                    valid_distances = distances[i]
                    
                    if len(valid_distances) > kmax:
                        _, sorted_idx = torch.sort(valid_distances)
                        selected_neighbors = valid_neighbors[sorted_idx[:kmax]]
                    else:
                        selected_neighbors = valid_neighbors
                    
                    final_neighbors[i, :len(selected_neighbors)] = selected_neighbors
            
            neighbor_indices = final_neighbors
        else:
            neighbor_indices = combined_neighbors
        
        # è½¬æ¢ä¸º CSR æ ¼å¼
        logger.info("è½¬æ¢ä¸º CSR æ ¼å¼...")
        indices, row_splits = to_csr(neighbor_indices, N)
        
        # è®¡ç®—é‚»å±…æ•°é‡
        count = row_splits[1:] - row_splits[:-1]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        logger.info("è®¡ç®—é‚»åŸŸç»Ÿè®¡...")
        stats = compute_neighbor_stats(indices, row_splits, gauss_scale)
        
        # ç»Ÿè®¡æ—¥å¿—
        mean_neighbors = count.float().mean().item()
        median_neighbors = count.float().median().item()
        max_neighbors = count.max().item()
        
        logger.info(f"åŠå¾„ {radius:.6f} ç»Ÿè®¡:")
        logger.info(f"  å¹³å‡é‚»å±…æ•°: {mean_neighbors:.2f}")
        logger.info(f"  ä¸­ä½é‚»å±…æ•°: {median_neighbors:.2f}")
        logger.info(f"  æœ€å¤§é‚»å±…æ•°: {max_neighbors}")
        logger.info(f"  æ€»è¾¹æ•°: {len(indices)}")
        
        # ä¿å­˜ç»“æœ
        results[f"{radius:.6f}"] = {
            'indices': indices.cpu(),
            'row_splits': row_splits.cpu(),
            'count': count.cpu(),
            'mean_log_scale': stats['mean_log_scale'].cpu(),
            'scale_sum': stats['scale_sum'].cpu(),
            'radius': radius,
            'kmax': kmax,
            'stats': {
                'mean_neighbors': mean_neighbors,
                'median_neighbors': median_neighbors,
                'max_neighbors': max_neighbors,
                'total_edges': len(indices)
            }
        }
    
    return results

# ============================= ä¸»å‡½æ•° =============================

def main():
    parser = argparse.ArgumentParser(description='é¢„è®¡ç®— COLMAP-3DGS é‚»æ¥å…³ç³»')
    parser.add_argument('--colmap', type=str, required=True, 
                       help='COLMAP ç‚¹åæ ‡æ–‡ä»¶ (.pt/.npz)')
    parser.add_argument('--gauss', type=str, required=True,
                       help='é«˜æ–¯æ•°æ®æ–‡ä»¶ (.pt/.npzï¼ŒåŒ…å« xyz å’Œ scale)')
    parser.add_argument('--radii', type=float, nargs='+', 
                       default=[0.012, 0.039, 0.107, 0.273],
                       help='æŸ¥è¯¢åŠå¾„åˆ—è¡¨')
    parser.add_argument('--kmax', type=int, nargs='+',
                       default=[128, 512, 1024, 2048],
                       help='å„åŠå¾„çš„æœ€å¤§é‚»å±…æ•°')
    parser.add_argument('--chunk', type=int, default=200000,
                       help='é«˜æ–¯çƒåˆ†å—å¤§å°')
    parser.add_argument('--out', type=str, default='cache/neighbors',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¡ç®—è®¾å¤‡')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å‚æ•°
    if len(args.radii) != len(args.kmax):
        raise ValueError("radii å’Œ kmax çš„é•¿åº¦å¿…é¡»ç›¸åŒ")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥çƒæŸ¥è¯¢åç«¯
    ball_query_func, backend_name = check_and_import_ball_query()
    
    # åŠ è½½æ•°æ®
    logger.info("åŠ è½½æ•°æ®...")
    colmap_xyz = load_arrays(args.colmap).float()
    
    try:
        gauss_xyz, gauss_scale = load_gaussian_data(args.gauss)
        # åˆå¹¶åæ ‡å’Œå°ºåº¦
        gauss_data = torch.cat([gauss_xyz.float(), gauss_scale.float()], dim=1)
    except ValueError:
        # å¦‚æœé«˜æ–¯æ–‡ä»¶åªåŒ…å«åæ ‡
        gauss_data = load_arrays(args.gauss).float()
        logger.warning("é«˜æ–¯æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å°ºåº¦ä¿¡æ¯")
    
    logger.info(f"COLMAP ç‚¹æ•°: {len(colmap_xyz):,}")
    logger.info(f"é«˜æ–¯çƒæ•°: {len(gauss_data):,}")
    
    # ç§»åŠ¨åˆ°GPU
    colmap_xyz = colmap_xyz.to(device)
    gauss_data = gauss_data.to(device)
    
    # æ‰§è¡Œå¤šåŠå¾„çƒæŸ¥è¯¢
    logger.info("å¼€å§‹é¢„è®¡ç®—é‚»æ¥å…³ç³»...")
    start_time = time.time()
    
    results = ball_query_multi_radius(
        colmap_xyz, gauss_data, args.radii, args.kmax, 
        args.chunk, ball_query_func, backend_name
    )
    
    total_time = time.time() - start_time
    logger.info(f"æ€»è€—æ—¶: {total_time:.2f} ç§’")
    
    # ä¿å­˜ç»“æœ
    for radius_str, data in results.items():
        output_path = out_dir / f"neigh_r{radius_str}.pt"
        torch.save(data, output_path)
        logger.info(f"ä¿å­˜: {output_path}")
    
    logger.info("âœ… é¢„è®¡ç®—å®Œæˆï¼")

# ============================= è‡ªæ£€æµ‹è¯• =============================

def quick_test():
    """å¿«é€Ÿè‡ªæ£€æµ‹è¯•"""
    logger.info("ğŸ§ª è¿è¡Œå¿«é€Ÿè‡ªæ£€...")
    
    # æ£€æŸ¥çƒæŸ¥è¯¢åç«¯
    try:
        ball_query_func, backend_name = check_and_import_ball_query()
    except ImportError:
        logger.error("è‡ªæ£€å¤±è´¥ï¼šç¼ºå°‘çƒæŸ¥è¯¢åç«¯")
        return False
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    N_colmap = 1000  # 1k COLMAP ç‚¹
    M_gauss = 10000  # 10k é«˜æ–¯çƒ
    
    torch.manual_seed(42)
    colmap_xyz = torch.randn(N_colmap, 3, device=device) * 2.0
    gauss_xyz = torch.randn(M_gauss, 3, device=device) * 2.0
    gauss_scale = torch.rand(M_gauss, 3, device=device) * 0.1 + 0.01
    gauss_data = torch.cat([gauss_xyz, gauss_scale], dim=1)
    
    # æµ‹è¯•å‚æ•°
    radii = [0.1, 0.2]
    kmax_list = [32, 64]
    chunk_size = 5000
    
    try:
        results = ball_query_multi_radius(
            colmap_xyz, gauss_data, radii, kmax_list, 
            chunk_size, ball_query_func, backend_name
        )
        
        # æ£€æŸ¥ç»“æœ
        for radius_str, data in results.items():
            indices = data['indices']
            row_splits = data['row_splits']
            count = data['count']
            
            logger.info(f"åŠå¾„ {radius_str}:")
            logger.info(f"  CSR indices å½¢çŠ¶: {indices.shape}")
            logger.info(f"  CSR row_splits å½¢çŠ¶: {row_splits.shape}")
            logger.info(f"  count å½¢çŠ¶: {count.shape}")
            logger.info(f"  å¹³å‡é‚»å±…æ•°: {count.float().mean():.2f}")
        
        logger.info("âœ… è‡ªæ£€é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ è‡ªæ£€å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # å…ˆè¿è¡Œè‡ªæ£€
    if not quick_test():
        logger.error("è‡ªæ£€å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        exit(1)
    
    # è¿è¡Œä¸»ç¨‹åº
    main() 