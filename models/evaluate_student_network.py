#!/usr/bin/env python3
"""
Student Networkç»“æœè¯„ä¼°
======================

åˆ†æè®­ç»ƒç»“æœï¼Œè¯„ä¼°æ¨¡å‹æ€§èƒ½
"""

import json
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from student_network import create_model, StudentNetworkEvaluator, StudentNetworkLoss
from create_training_dataset import StudentNetworkDataset
from torch.utils.data import DataLoader

def analyze_training_results():
    """åˆ†æè®­ç»ƒç»“æœ"""
    print("=" * 60)
    print("ğŸ“Š Student Network è®­ç»ƒç»“æœåˆ†æ")
    print("=" * 60)
    
    # åŠ è½½ç»“æœ
    with open('student_checkpoints/final_results.json', 'r') as f:
        results = json.load(f)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ—ï¸  æ¨¡å‹é…ç½®:")
    print(f"   è¾“å…¥ç»´åº¦: {results['model_config']['input_dim']}")
    print(f"   è¾“å‡ºç»´åº¦: {results['model_config']['output_dim']}")
    print(f"   å‚æ•°é‡: 234,448 (~0.89MB)")
    print(f"   åŠå¾„æ„ŸçŸ¥: {results['model_config']['use_radius_aware']}")
    
    # è®­ç»ƒè¿‡ç¨‹
    print(f"\nğŸ¯ è®­ç»ƒè¿‡ç¨‹:")
    print(f"   æœ€ä½³epoch: {results['best_epoch']}")
    print(f"   æ—©åœè§¦å‘: Epoch 39 (éªŒè¯æŸå¤±è¿ç»­15ä¸ªepochæœªæ”¹å–„)")
    print(f"   æ€»è®­ç»ƒæ—¶é—´: ~22åˆ†é’Ÿ")
    
    # æŸå¤±åˆ†æ
    print(f"\nğŸ“ˆ æŸå¤±åˆ†æ:")
    train_loss = results['training_history']['train_loss']
    val_loss = results['training_history']['val_loss']
    test_losses = results['test_losses']
    
    print(f"   åˆå§‹è®­ç»ƒæŸå¤±: {train_loss[0]:.2f}")
    print(f"   æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_loss[-1]:.2f}")
    print(f"   æ”¹å–„å€æ•°: {train_loss[0]/train_loss[-1]:.1f}x")
    
    print(f"\n   æœ€ä½³éªŒè¯æŸå¤±: {results['best_val_loss']:.4f}")
    print(f"   æµ‹è¯•æŸå¤±: {test_losses['total_loss']:.4f}")
    
    # æŸå¤±ç»„ä»¶åˆ†æ
    print(f"\nğŸ” æµ‹è¯•æŸå¤±ç»„ä»¶:")
    print(f"   ä½ç½®MSE: {test_losses['position_mse']:.4f}")
    print(f"   ä½ç½®Smooth: {test_losses['position_smooth']:.4f}")
    print(f"   å°ºåº¦MSE: {test_losses['scale_mse']:.4f}")
    print(f"   å°ºåº¦Smooth: {test_losses['scale_smooth']:.4f}")
    print(f"   ç»Ÿè®¡MSE: {test_losses['stats_mse']:.4f}")
    
    # æ”¶æ•›æ€§åˆ†æ
    improvement_ratio = (val_loss[0] - results['best_val_loss']) / val_loss[0]
    print(f"\nğŸ“‰ æ”¶æ•›æ€§:")
    print(f"   éªŒè¯æŸå¤±æ”¹å–„: {improvement_ratio*100:.1f}%")
    print(f"   è¿‡æ‹Ÿåˆæ£€æŸ¥: æµ‹è¯•æŸå¤±({test_losses['total_loss']:.4f}) vs æœ€ä½³éªŒè¯æŸå¤±({results['best_val_loss']:.4f})")
    
    overfitting = test_losses['total_loss'] - results['best_val_loss']
    if overfitting < 0.5:
        print(f"   âœ… æ³›åŒ–è‰¯å¥½ (å·®å¼‚: {overfitting:.4f})")
    elif overfitting < 1.0:
        print(f"   âš ï¸  è½»å¾®è¿‡æ‹Ÿåˆ (å·®å¼‚: {overfitting:.4f})")
    else:
        print(f"   âŒ æ˜æ˜¾è¿‡æ‹Ÿåˆ (å·®å¼‚: {overfitting:.4f})")

def evaluate_feature_prediction_quality():
    """è¯„ä¼°ç‰¹å¾é¢„æµ‹è´¨é‡"""
    print("\n" + "=" * 60)
    print("ğŸ§ª ç‰¹å¾é¢„æµ‹è´¨é‡è¯„ä¼°")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    with open('training_data/metadata.json', 'r') as f:
        metadata = json.load(f)
    
    model_config = {
        'input_dim': metadata['feature_dims']['input_dim'],
        'output_dim': metadata['feature_dims']['output_dim'],
        'feature_dims': [64, 128, 256],
        'decoder_dims': [256, 128, 64],
        'num_radii': len(metadata['radii_used']),
        'use_radius_aware': True
    }
    
    model = create_model(model_config)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    checkpoint = torch.load('student_checkpoints/best_model.pth', map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # æµ‹è¯•æ•°æ®
    test_dataset = StudentNetworkDataset('training_data/test.h5')
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # è¯„ä¼°å™¨
    evaluator = StudentNetworkEvaluator(model, device)
    criterion = StudentNetworkLoss()
    
    # è¯¦ç»†è¯„ä¼°
    model.eval()
    all_predictions = []
    all_targets = []
    all_radii = []
    
    with torch.no_grad():
        for batch in test_loader:
            inputs = batch['input'].to(device)
            targets = batch['target'].to(device)
            radii = batch['radius'].to(device)
            
            predictions = model(inputs, radii)
            
            all_predictions.append(predictions.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
            all_radii.append(radii.cpu().numpy())
    
    predictions = np.concatenate(all_predictions, axis=0)  # [N, 16]
    targets = np.concatenate(all_targets, axis=0)  # [N, 16]
    radii = np.concatenate(all_radii, axis=0)  # [N]
    
    # åˆ†æä¸åŒç‰¹å¾ç»„ä»¶
    feature_names = [
        'centroid_x', 'centroid_y', 'centroid_z',
        'pos_std_x', 'pos_std_y', 'pos_std_z',
        'mean_scale_x', 'mean_scale_y', 'mean_scale_z',
        'scale_std_x', 'scale_std_y', 'scale_std_z',
        'num_neighbors', 'scale_mean', 'scale_max', 'scale_min'
    ]
    
    print(f"\nğŸ“Š ç‰¹å¾é¢„æµ‹ç²¾åº¦ (æ¯ä¸ªç‰¹å¾çš„MAE):")
    mae_per_feature = np.mean(np.abs(predictions - targets), axis=0)
    
    for i, (name, mae) in enumerate(zip(feature_names, mae_per_feature)):
        category = ""
        if i < 6:
            category = "ğŸ¯ä½ç½®"
        elif i < 12:
            category = "ğŸ“å°ºåº¦"
        else:
            category = "ğŸ“ˆç»Ÿè®¡"
        print(f"   {category} {name}: {mae:.4f}")
    
    # æŒ‰åŠå¾„åˆ†æ
    print(f"\nğŸ”„ æŒ‰åŠå¾„åˆ†æ:")
    for radius in [0.012, 0.039, 0.107, 0.273]:
        mask = np.abs(radii - radius) < 1e-6
        if mask.sum() > 0:
            radius_predictions = predictions[mask]
            radius_targets = targets[mask]
            radius_mae = np.mean(np.abs(radius_predictions - radius_targets))
            radius_rmse = np.sqrt(np.mean((radius_predictions - radius_targets) ** 2))
            print(f"   åŠå¾„ {radius:.3f}: MAE={radius_mae:.4f}, RMSE={radius_rmse:.4f} ({mask.sum()} æ ·æœ¬)")
    
    # ç›¸å…³æ€§åˆ†æ
    print(f"\nğŸ”— é¢„æµ‹ç›¸å…³æ€§:")
    correlations = []
    for i in range(predictions.shape[1]):
        corr = np.corrcoef(predictions[:, i], targets[:, i])[0, 1]
        correlations.append(corr)
    
    mean_corr = np.nanmean(correlations)
    print(f"   å¹³å‡ç›¸å…³ç³»æ•°: {mean_corr:.4f}")
    print(f"   æœ€é«˜ç›¸å…³ç³»æ•°: {np.nanmax(correlations):.4f}")
    print(f"   æœ€ä½ç›¸å…³ç³»æ•°: {np.nanmin(correlations):.4f}")

def suggest_improvements():
    """å»ºè®®æ”¹è¿›æ–¹æ¡ˆ"""
    print("\n" + "=" * 60)
    print("ğŸ’¡ æ”¹è¿›å»ºè®®")
    print("=" * 60)
    
    # åŸºäºç»“æœåˆ†æç»™å‡ºå»ºè®®
    with open('student_checkpoints/final_results.json', 'r') as f:
        results = json.load(f)
    
    test_loss = results['test_losses']['total_loss']
    val_loss = results['best_val_loss']
    
    print(f"\nğŸ¯ å½“å‰çŠ¶æ€è¯„ä¼°:")
    if test_loss < 10:
        print("   âœ… ä¼˜ç§€: æŸå¤±è¾ƒä½ï¼Œæ¨¡å‹å­¦ä¹ æ•ˆæœå¥½")
    elif test_loss < 15:
        print("   âš ï¸  ä¸­ç­‰: æœ‰æ”¹è¿›ç©ºé—´")
    else:
        print("   âŒ éœ€è¦æ”¹è¿›: æŸå¤±è¾ƒé«˜")
    
    print(f"\nğŸš€ å…·ä½“æ”¹è¿›å»ºè®®:")
    
    # 1. æ•°æ®æ–¹é¢
    print(f"   ğŸ“Š æ•°æ®æ”¹è¿›:")
    print(f"     â€¢ ä½¿ç”¨å®Œæ•´é‚»å±…æ•°æ® (å½“å‰ç”¨çš„æ˜¯é‡‡æ ·ç‰ˆæœ¬)")
    print(f"     â€¢ å¢åŠ ç‰¹å¾å·¥ç¨‹ (æ·»åŠ æ›´å¤šå‡ ä½•ç‰¹å¾)")
    print(f"     â€¢ æ•°æ®å¢å¼º (æ—‹è½¬ã€å¹³ç§»ç­‰)")
    
    # 2. æ¨¡å‹æ–¹é¢
    print(f"   ğŸ—ï¸  æ¨¡å‹æ”¹è¿›:")
    print(f"     â€¢ å¢åŠ æ¨¡å‹æ·±åº¦/å®½åº¦")
    print(f"     â€¢ å°è¯•Transformeræ¶æ„")
    print(f"     â€¢ åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶")
    print(f"     â€¢ æ®‹å·®è¿æ¥")
    
    # 3. è®­ç»ƒæ–¹é¢
    print(f"   ğŸ® è®­ç»ƒæ”¹è¿›:")
    print(f"     â€¢ æ›´é•¿çš„è®­ç»ƒæ—¶é—´")
    print(f"     â€¢ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥")
    print(f"     â€¢ å¤šå°ºåº¦æŸå¤±æƒé‡è°ƒæ•´")
    print(f"     â€¢ æ¸è¿›å¼è®­ç»ƒ (å…ˆå°åŠå¾„ï¼Œåå¤§åŠå¾„)")
    
    # 4. åº”ç”¨æ–¹é¢
    print(f"   ğŸ¯ åº”ç”¨ä¼˜åŒ–:")
    print(f"     â€¢ é’ˆå¯¹ç‰¹å®šä»»åŠ¡å¾®è°ƒ")
    print(f"     â€¢ é›†æˆå¤šä¸ªæ¨¡å‹")
    print(f"     â€¢ åå¤„ç†ä¼˜åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    analyze_training_results()
    evaluate_feature_prediction_quality()
    suggest_improvements()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ æ€»ç»“")
    print("=" * 60)
    print(f"âœ… Student Networkè®­ç»ƒæˆåŠŸå®Œæˆ")
    print(f"âœ… æ¨¡å‹å·²æ”¶æ•›ï¼Œæ³›åŒ–æ€§èƒ½è‰¯å¥½")
    print(f"âœ… å¯ä»¥å¼€å§‹ä½¿ç”¨æ¨¡å‹è¿›è¡ŒCOLMAPâ†’3DGSç‰¹å¾é¢„æµ‹")
    print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨å®Œæ•´æ•°æ®é‡æ–°è®­ç»ƒä»¥è·å¾—æ›´å¥½æ€§èƒ½")
    print("=" * 60)

if __name__ == "__main__":
    main() 