import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import json

# æ·»åŠ 3dgsæ ¹ç›®å½•åˆ°path
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene import GaussianModel

def analyze_fine_scale_distribution():
    """ç²¾ç»†åˆ†æé«˜æ–¯çƒå°ºå¯¸åˆ†å¸ƒï¼Œç‰¹åˆ«å…³æ³¨è¶…å°çƒçš„å†…éƒ¨åˆ†å±‚"""
    print("ğŸ” ç²¾ç»†é«˜æ–¯çƒå°ºå¯¸åˆ†å¸ƒåˆ†æ")
    print("=" * 50)
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    ply_path = "./output/truck-150w/gaussian_ball/iteration_994230_best_psnr/gaussian_ball.ply"
    gaussians = GaussianModel(3)
    gaussians.load_ply(ply_path, use_train_test_exp=False)
    
    # æå–å‚æ•°
    xyz = gaussians._xyz.detach().cpu().numpy()
    scaling = gaussians._scaling.detach().cpu().numpy()
    opacity = gaussians._opacity.detach().cpu().numpy()
    
    # å¤„ç†NaNå€¼
    nan_mask = np.isnan(xyz)
    nan_positions = np.any(nan_mask, axis=1)
    nan_count = np.sum(nan_positions)
    
    if nan_count > 0:
        print(f"âš ï¸ å‘ç° {nan_count} ä¸ªNaNä½ç½®ï¼Œå°†è¢«æ’é™¤")
        valid_mask = ~nan_positions
        xyz = xyz[valid_mask]
        scaling = scaling[valid_mask]
        opacity = opacity[valid_mask]
    
    # è®¡ç®—å¹³å‡ç¼©æ”¾
    avg_scale = np.mean(scaling, axis=1)
    
    print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»é«˜æ–¯çƒæ•°: {len(avg_scale):,}")
    print(f"  å¹³å‡ç¼©æ”¾èŒƒå›´: {avg_scale.min():.6f} ~ {avg_scale.max():.6f}")
    print(f"  å¹³å‡å€¼: {avg_scale.mean():.6f}")
    print(f"  ä¸­ä½æ•°: {np.median(avg_scale):.6f}")
    print(f"  æ ‡å‡†å·®: {avg_scale.std():.6f}")
    
    # è¯¦ç»†ç™¾åˆ†ä½æ•°åˆ†æ
    percentiles = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 98, 99, 99.5, 99.9]
    scale_percentiles = np.percentile(avg_scale, percentiles)
    
    print(f"\nğŸ“ˆ è¯¦ç»†ç™¾åˆ†ä½æ•°åˆ†æ:")
    for p, val in zip(percentiles, scale_percentiles):
        count = np.sum(avg_scale <= val)
        print(f"  {p:4.1f}%: {val:.6f} ({count:,}çƒ)")
    
    # é‡æ–°å®šä¹‰ä¹‹å‰çš„"è¶…å°çƒ"è¾¹ç•Œ
    previous_threshold = 0.0032229693606495857
    ultra_small_count = np.sum(avg_scale <= previous_threshold)
    ultra_small_ratio = ultra_small_count / len(avg_scale) * 100
    
    print(f"\nğŸ” ä¹‹å‰çš„'è¶…å°çƒ'åˆ†æ:")
    print(f"  é˜ˆå€¼: {previous_threshold:.6f}")
    print(f"  æ•°é‡: {ultra_small_count:,} ({ultra_small_ratio:.2f}%)")
    print(f"  è¿™ç¡®å®å äº†ç»å¤§å¤šæ•°ï¼éœ€è¦è¿›ä¸€æ­¥ç»†åˆ†")
    
    # å¯¹"è¶…å°çƒ"è¿›è¡Œæ›´ç»†è‡´çš„åˆ†å±‚
    ultra_small_mask = avg_scale <= previous_threshold
    ultra_small_scales = avg_scale[ultra_small_mask]
    
    print(f"\nğŸ¯ è¶…å°çƒå†…éƒ¨åˆ†æ:")
    print(f"  æ•°é‡: {len(ultra_small_scales):,}")
    print(f"  èŒƒå›´: {ultra_small_scales.min():.6f} ~ {ultra_small_scales.max():.6f}")
    print(f"  å‡å€¼: {ultra_small_scales.mean():.6f}")
    print(f"  ä¸­ä½æ•°: {np.median(ultra_small_scales):.6f}")
    print(f"  æ ‡å‡†å·®: {ultra_small_scales.std():.6f}")
    
    # è¶…å°çƒå†…éƒ¨ç»†åˆ†ï¼ˆ10ä¸ªå±‚çº§ï¼‰
    ultra_small_percentiles = [10, 20, 30, 40, 50, 60, 70, 80, 90]
    ultra_small_thresholds = np.percentile(ultra_small_scales, ultra_small_percentiles)
    
    print(f"\nğŸ”„ è¶…å°çƒå†…éƒ¨10å±‚ç»†åˆ†:")
    for i in range(10):
        if i == 0:
            mask = ultra_small_scales <= ultra_small_thresholds[0]
            layer_desc = "å¾®è§‚çƒ"
        elif i == 9:
            mask = ultra_small_scales > ultra_small_thresholds[8]
            layer_desc = "å‡†å°çƒ"
        else:
            mask = (ultra_small_scales > ultra_small_thresholds[i-1]) & (ultra_small_scales <= ultra_small_thresholds[i])
            layer_desc = f"è¶…å°çƒ{i}"
        
        count = np.sum(mask)
        if count > 0:
            layer_range = f"{ultra_small_scales[mask].min():.6f}~{ultra_small_scales[mask].max():.6f}"
            ratio = count / len(ultra_small_scales) * 100
            print(f"  å±‚{i} ({layer_desc}): {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {layer_range}")
    
    # å…¨å±€æ™ºèƒ½åˆ†å±‚ç­–ç•¥
    print(f"\nğŸ§  æ™ºèƒ½å…¨å±€åˆ†å±‚ç­–ç•¥:")
    
    # æ–¹æ¡ˆ1: å¯¹æ•°åˆ†å±‚ (å¤„ç†å¹‚å¾‹åˆ†å¸ƒ)
    log_scales = np.log10(avg_scale + 1e-10)  # é¿å…log(0)
    log_percentiles = [10, 25, 50, 75, 90, 95, 99]
    log_thresholds = np.percentile(log_scales, log_percentiles)
    actual_thresholds = 10**log_thresholds - 1e-10
    
    print(f"\n  æ–¹æ¡ˆ1: å¯¹æ•°åˆ†å±‚ (é€‚åˆå¹‚å¾‹åˆ†å¸ƒ)")
    for i in range(len(log_percentiles) + 1):
        if i == 0:
            mask = avg_scale <= actual_thresholds[0]
            layer_name = "nano"
        elif i == len(log_percentiles):
            mask = avg_scale > actual_thresholds[-1]
            layer_name = "giant"
        else:
            mask = (avg_scale > actual_thresholds[i-1]) & (avg_scale <= actual_thresholds[i])
            layer_name = f"scale{i}"
        
        count = np.sum(mask)
        if count > 0:
            ratio = count / len(avg_scale) * 100
            range_str = f"{avg_scale[mask].min():.6f}~{avg_scale[mask].max():.6f}"
            print(f"    å±‚{i} ({layer_name}): {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {range_str}")
    
    # æ–¹æ¡ˆ2: ç­‰æ•°é‡åˆ†å±‚
    equal_count_layers = 20  # 20å±‚
    equal_count_thresholds = np.percentile(avg_scale, np.linspace(5, 95, equal_count_layers-1))
    
    print(f"\n  æ–¹æ¡ˆ2: ç­‰æ•°é‡åˆ†å±‚ ({equal_count_layers}å±‚)")
    for i in range(equal_count_layers):
        if i == 0:
            mask = avg_scale <= equal_count_thresholds[0]
        elif i == equal_count_layers - 1:
            mask = avg_scale > equal_count_thresholds[-1]
        else:
            mask = (avg_scale > equal_count_thresholds[i-1]) & (avg_scale <= equal_count_thresholds[i])
        
        count = np.sum(mask)
        if count > 0:
            ratio = count / len(avg_scale) * 100
            range_str = f"{avg_scale[mask].min():.6f}~{avg_scale[mask].max():.6f}"
            print(f"    å±‚{i:2d}: {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {range_str}")
    
    # æ–¹æ¡ˆ3: æ··åˆç­–ç•¥ - ç²¾ç»†+ç²—ç³™
    print(f"\n  æ–¹æ¡ˆ3: æ··åˆç­–ç•¥ (å‰95%ç²¾ç»†åˆ†å±‚ + å5%ç²—ç³™åˆ†å±‚)")
    
    # å‰95%ç»†åˆ†ä¸º15å±‚
    p95_threshold = np.percentile(avg_scale, 95)
    small_scales = avg_scale[avg_scale <= p95_threshold]
    fine_thresholds = np.percentile(small_scales, np.linspace(6.67, 93.33, 14))  # 15å±‚çš„14ä¸ªåˆ†ç•Œç‚¹
    
    # å5%åˆ†ä¸º5å±‚
    large_scales = avg_scale[avg_scale > p95_threshold]
    coarse_thresholds = np.percentile(large_scales, [20, 40, 60, 80])
    
    mixed_thresholds = np.concatenate([fine_thresholds, [p95_threshold], coarse_thresholds])
    
    for i in range(20):  # æ€»å…±20å±‚
        if i == 0:
            mask = avg_scale <= mixed_thresholds[0]
            layer_type = "ç²¾ç»†"
        elif i < 15:
            mask = (avg_scale > mixed_thresholds[i-1]) & (avg_scale <= mixed_thresholds[i])
            layer_type = "ç²¾ç»†"
        elif i == 15:
            mask = (avg_scale > mixed_thresholds[i-1]) & (avg_scale <= mixed_thresholds[i])
            layer_type = "è¿‡æ¸¡"
        else:
            if i == 19:
                mask = avg_scale > mixed_thresholds[i-1]
            else:
                mask = (avg_scale > mixed_thresholds[i-1]) & (avg_scale <= mixed_thresholds[i])
            layer_type = "ç²—ç³™"
        
        count = np.sum(mask)
        if count > 0:
            ratio = count / len(avg_scale) * 100
            range_str = f"{avg_scale[mask].min():.6f}~{avg_scale[mask].max():.6f}"
            print(f"    å±‚{i:2d} ({layer_type}): {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {range_str}")
    
    # åˆ›å»ºå¯è§†åŒ–
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "fine_scale_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. å°ºå¯¸åˆ†å¸ƒç›´æ–¹å›¾
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('Fine-Grained Gaussian Scale Distribution Analysis', fontsize=16, fontweight='bold')
    
    # å…¨å±€åˆ†å¸ƒ
    ax = axes[0, 0]
    ax.hist(avg_scale, bins=100, alpha=0.7, color='blue', density=True)
    ax.set_xlabel('Average Scale')
    ax.set_ylabel('Density')
    ax.set_title('Overall Scale Distribution')
    ax.set_yscale('log')
    ax.grid(True, alpha=0.3)
    
    # è¶…å°çƒå†…éƒ¨åˆ†å¸ƒ
    ax = axes[0, 1]
    ax.hist(ultra_small_scales, bins=50, alpha=0.7, color='red', density=True)
    ax.set_xlabel('Average Scale')
    ax.set_ylabel('Density')
    ax.set_title(f'Ultra-Small Gaussians Distribution\n({ultra_small_ratio:.1f}% of total)')
    ax.grid(True, alpha=0.3)
    
    # å¯¹æ•°å°ºåº¦åˆ†å¸ƒ
    ax = axes[1, 0]
    ax.hist(log_scales, bins=50, alpha=0.7, color='green', density=True)
    ax.set_xlabel('Log10(Average Scale)')
    ax.set_ylabel('Density')
    ax.set_title('Log-Scale Distribution')
    ax.grid(True, alpha=0.3)
    
    # ç´¯ç§¯åˆ†å¸ƒ
    ax = axes[1, 1]
    sorted_scales = np.sort(avg_scale)
    cumulative = np.arange(1, len(sorted_scales) + 1) / len(sorted_scales) * 100
    ax.plot(sorted_scales, cumulative, 'b-', linewidth=2)
    ax.set_xlabel('Average Scale')
    ax.set_ylabel('Cumulative Percentage (%)')
    ax.set_title('Cumulative Distribution')
    ax.set_xscale('log')
    ax.grid(True, alpha=0.3)
    
    # æ ‡æ³¨å…³é”®ç‚¹
    key_percentiles = [50, 90, 95, 99]
    for p in key_percentiles:
        val = np.percentile(avg_scale, p)
        ax.axvline(val, color='red', linestyle='--', alpha=0.7)
        ax.text(val, p, f'{p}%', rotation=90, ha='right', va='bottom')
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    distribution_file = os.path.join(output_dir, 'fine_scale_distribution.png')
    plt.savefig(distribution_file, dpi=200, bbox_inches='tight')
    plt.close()
    
    print(f"âœ… åˆ†å¸ƒå›¾ä¿å­˜: {distribution_file}")
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_results = {
        'basic_stats': {
            'total_gaussians': len(avg_scale),
            'scale_range': [float(avg_scale.min()), float(avg_scale.max())],
            'mean': float(avg_scale.mean()),
            'median': float(np.median(avg_scale)),
            'std': float(avg_scale.std())
        },
        'percentile_analysis': {
            'percentiles': percentiles,
            'values': [float(v) for v in scale_percentiles]
        },
        'ultra_small_analysis': {
            'threshold': previous_threshold,
            'count': int(ultra_small_count),
            'ratio': float(ultra_small_ratio),
            'internal_stats': {
                'range': [float(ultra_small_scales.min()), float(ultra_small_scales.max())],
                'mean': float(ultra_small_scales.mean()),
                'median': float(np.median(ultra_small_scales)),
                'std': float(ultra_small_scales.std())
            }
        },
        'layering_strategies': {
            'logarithmic': {
                'description': 'å¯¹æ•°åˆ†å±‚ï¼Œé€‚åˆå¹‚å¾‹åˆ†å¸ƒ',
                'layers': len(log_percentiles) + 1,
                'thresholds': [float(t) for t in actual_thresholds]
            },
            'equal_count': {
                'description': f'ç­‰æ•°é‡åˆ†å±‚ï¼Œ{equal_count_layers}å±‚',
                'layers': equal_count_layers,
                'thresholds': [float(t) for t in equal_count_thresholds]
            },
            'mixed': {
                'description': 'æ··åˆç­–ç•¥ï¼šå‰95%ç²¾ç»† + å5%ç²—ç³™',
                'layers': 20,
                'thresholds': [float(t) for t in mixed_thresholds],
                'fine_layers': 15,
                'coarse_layers': 5
            }
        },
        'recommendations': [
            "è¶…å°çƒå 99%ï¼Œéœ€è¦è¿›ä¸€æ­¥ç»†åˆ†",
            "å»ºè®®ä½¿ç”¨æ··åˆç­–ç•¥ï¼šå¯¹å°çƒç²¾ç»†åˆ†å±‚ï¼Œå¯¹å¤§çƒç²—ç³™åˆ†å±‚", 
            "å¯¹æ•°åˆ†å±‚å¯èƒ½æ›´é€‚åˆè¿™ç§å¹‚å¾‹åˆ†å¸ƒ",
            "æ¯å±‚çƒæ•°å·®å¼‚å·¨å¤§ï¼Œæ¸è¿›å¼è¯„ä¼°éœ€è¦è€ƒè™‘å±‚çº§é‡è¦æ€§"
        ]
    }
    
    results_file = os.path.join(output_dir, 'fine_scale_analysis_results.json')
    with open(results_file, 'w') as f:
        json.dump(analysis_results, f, indent=2)
    
    print(f"âœ… åˆ†æç»“æœä¿å­˜: {results_file}")
    
    print(f"\nğŸ‰ ç²¾ç»†å°ºå¯¸åˆ†æå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"ğŸ’¡ å…³é”®å‘ç°:")
    print(f"   1. è¶…å°çƒç¡®å®å {ultra_small_ratio:.1f}%ï¼Œéœ€è¦ç»†åˆ†")
    print(f"   2. å°ºå¯¸åˆ†å¸ƒå‘ˆç°æ˜æ˜¾çš„å¹‚å¾‹ç‰¹å¾")
    print(f"   3. å»ºè®®ä½¿ç”¨æ··åˆåˆ†å±‚ç­–ç•¥å¹³è¡¡ç²¾ç»†åº¦å’Œå®ç”¨æ€§")
    
    return analysis_results

def main():
    analyze_fine_scale_distribution()

if __name__ == "__main__":
    main() 