import os
import sys
import torch
import numpy as np
import argparse
import json
import glob
from PIL import Image

# æ·»åŠ 3dgsæ ¹ç›®å½•åˆ°path
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene import GaussianModel
from scene.cameras import Camera
from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary, qvec2rotmat
from arguments import ModelParams, PipelineParams
from gaussian_renderer import render
from utils.general_utils import PILtoTorch
from utils.graphics_utils import focal2fov
from utils.loss_utils import l1_loss

def psnr(img1, img2):
    """æŒ‰ç…§train.pyçš„PSNRè®¡ç®—"""
    mse = (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)
    return 20 * torch.log10(1.0 / torch.sqrt(mse))

def load_test_camera(colmap_path, images_path, camera_name="000001.jpg", resolution_scale=2.0):
    """åŠ è½½æµ‹è¯•ç›¸æœº"""
    cameras_bin = os.path.join(colmap_path, 'cameras.bin')
    images_bin = os.path.join(colmap_path, 'images.bin')
    
    cam_intrinsics = read_intrinsics_binary(cameras_bin)
    cam_extrinsics = read_extrinsics_binary(images_bin)
    
    # æ‰¾åˆ°æŒ‡å®šç›¸æœº
    target_img_id = None
    for img_id, img_info in cam_extrinsics.items():
        if img_info.name == camera_name:
            target_img_id = img_id
            break
    
    if target_img_id is None:
        print(f"âŒ æœªæ‰¾åˆ°ç›¸æœº: {camera_name}")
        return None
    
    img_info = cam_extrinsics[target_img_id]
    intrinsic = cam_intrinsics[img_info.camera_id]
    
    # è§£æå‚æ•°
    fx, fy, cx, cy = intrinsic.params
    width = int(intrinsic.width / resolution_scale)
    height = int(intrinsic.height / resolution_scale)
    fx_scaled = fx / resolution_scale
    fy_scaled = fy / resolution_scale
    
    FoVx = focal2fov(fx_scaled, width)
    FoVy = focal2fov(fy_scaled, height)
    
    R = np.transpose(qvec2rotmat(img_info.qvec))
    T = np.array(img_info.tvec)
    
    # åŠ è½½å›¾åƒ
    image_path = os.path.join(images_path, img_info.name)
    image = Image.open(image_path)
    if resolution_scale != 1.0:
        image = image.resize((width, height), Image.LANCZOS)
    
    camera = Camera(
        resolution=(width, height),
        colmap_id=target_img_id,
        R=R,
        T=T,
        FoVx=FoVx,
        FoVy=FoVy,
        depth_params=None,
        image=image,
        invdepthmap=None,
        image_name=img_info.name,
        uid=0,
        data_device="cuda",
        train_test_exp=False,
        is_test_dataset=False,
        is_test_view=False
    )
    
    return camera

def evaluate_ply_file(ply_path, camera, pipe, background):
    """è¯„ä¼°å•ä¸ªPLYæ–‡ä»¶çš„PSNR"""
    if not os.path.exists(ply_path):
        return None
    
    try:
        # åŠ è½½é«˜æ–¯çƒ
        gaussians = GaussianModel(3)
        gaussians.load_ply(ply_path, use_train_test_exp=False)
        
        # æ£€æŸ¥SPARSE_ADAM_AVAILABLE
        try:
            from diff_gaussian_rasterization import SparseGaussianAdam
            SPARSE_ADAM_AVAILABLE = True
        except:
            SPARSE_ADAM_AVAILABLE = False
        
        # æ¸²æŸ“
        render_result = render(camera, gaussians, pipe, background, 1., SPARSE_ADAM_AVAILABLE, None, False)
        rendered_image = torch.clamp(render_result["render"], 0.0, 1.0)
        
        # GTå›¾åƒ
        gt_image = torch.clamp(camera.original_image.to("cuda"), 0.0, 1.0)
        
        # è®¡ç®—æŒ‡æ ‡
        psnr_val = psnr(rendered_image, gt_image).mean().item()
        l1_val = l1_loss(rendered_image, gt_image).mean().item()
        
        return {
            "psnr": psnr_val,
            "l1_loss": l1_val,
            "gaussian_count": gaussians.get_xyz.shape[0]
        }
        
    except Exception as e:
        print(f"  âŒ è¯„ä¼°å¤±è´¥: {str(e)}")
        return None

def evaluate_size_layers(layers_dir, output_file='size_layers_evaluation.json'):
    """è¯„ä¼°å°ºå¯¸åˆ†å±‚çš„æ¸è¿›å¼PSNR"""
    print("ğŸ“Š è¯„ä¼°å°ºå¯¸åˆ†å±‚çš„PSNRè´¡çŒ®")
    print("=" * 50)
    
    # è®¾ç½®æ¸²æŸ“ç¯å¢ƒ
    pipeline_parser = argparse.ArgumentParser()
    pipe_parser = PipelineParams(pipeline_parser)
    pipe_args = pipeline_parser.parse_args([])
    pipe = pipe_parser.extract(pipe_args)
    
    background = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
    
    # åŠ è½½ç›¸æœº
    colmap_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/sparse/0"
    images_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/images"
    camera = load_test_camera(colmap_path, images_path, "000001.jpg", 2.0)
    
    if camera is None:
        return
    
    print(f"âœ… åŠ è½½æµ‹è¯•ç›¸æœº: 000001.jpg")
    
    # æŸ¥æ‰¾æ‰€æœ‰PLYæ–‡ä»¶
    single_layer_files = sorted(glob.glob(os.path.join(layers_dir, "size_layer_*.ply")))
    progressive_files = sorted(glob.glob(os.path.join(layers_dir, "size_progressive_*.ply")))
    
    print(f"ğŸ“ æ‰¾åˆ°å•å±‚æ–‡ä»¶: {len(single_layer_files)}ä¸ª")
    print(f"ğŸ“ˆ æ‰¾åˆ°æ¸è¿›æ–‡ä»¶: {len(progressive_files)}ä¸ª")
    
    # è¯„ä¼°å•å±‚æ–‡ä»¶
    print(f"\nğŸ¯ è¯„ä¼°å•å±‚æ–‡ä»¶...")
    single_results = []
    
    for i, ply_file in enumerate(single_layer_files):
        filename = os.path.basename(ply_file)
        print(f"  è¯„ä¼° {filename}...")
        
        result = evaluate_ply_file(ply_file, camera, pipe, background)
        if result is not None:
            result['file'] = filename
            result['layer_id'] = i
            single_results.append(result)
            print(f"    âœ… PSNR: {result['psnr']:.2f}dB, L1: {result['l1_loss']:.6f}, çƒæ•°: {result['gaussian_count']:,}")
        else:
            print(f"    âŒ è¯„ä¼°å¤±è´¥")
    
    # è¯„ä¼°æ¸è¿›æ–‡ä»¶
    print(f"\nğŸ“ˆ è¯„ä¼°æ¸è¿›å¼ç´¯ç§¯...")
    progressive_results = []
    
    for i, ply_file in enumerate(progressive_files):
        filename = os.path.basename(ply_file)
        print(f"  è¯„ä¼° {filename}...")
        
        result = evaluate_ply_file(ply_file, camera, pipe, background)
        if result is not None:
            result['file'] = filename
            result['cumulative_layers'] = i + 1
            progressive_results.append(result)
            print(f"    âœ… PSNR: {result['psnr']:.2f}dB, L1: {result['l1_loss']:.6f}, çƒæ•°: {result['gaussian_count']:,}")
        else:
            print(f"    âŒ è¯„ä¼°å¤±è´¥")
    
    # åˆ†æè´¡çŒ®
    print(f"\nğŸ” åˆ†æPSNRè´¡çŒ®...")
    contribution_analysis = []
    
    layer_names = ['è¶…å°çƒ', 'å°çƒ', 'ä¸­çƒ', 'å¤§çƒ', 'è¶…å¤§çƒ']
    
    for i, result in enumerate(progressive_results):
        if i == 0:
            contribution = result['psnr']
            layers_desc = layer_names[0]
        else:
            contribution = result['psnr'] - progressive_results[i-1]['psnr']
            layers_desc = f"{layer_names[0]}-{layer_names[i]}"
        
        contribution_analysis.append({
            'stage': i,
            'layers_description': layers_desc,
            'cumulative_psnr': result['psnr'],
            'psnr_contribution': contribution,
            'gaussian_count': result['gaussian_count'],
            'efficiency': contribution / (result['gaussian_count'] / 1000000)  # PSNR/M balls
        })
        
        print(f"  é˜¶æ®µ{i} ({layers_desc}): {result['psnr']:.2f}dB (+{contribution:.2f}), {result['gaussian_count']:,}çƒ")
    
    # ä¿å­˜ç»“æœ
    evaluation_results = {
        'test_camera': '000001.jpg',
        'evaluation_timestamp': str(torch.cuda.Event()),
        'single_layer_results': single_results,
        'progressive_results': progressive_results,
        'contribution_analysis': contribution_analysis,
        'summary': {
            'total_layers': len(single_results),
            'final_psnr': progressive_results[-1]['psnr'] if progressive_results else 0,
            'total_gaussians': progressive_results[-1]['gaussian_count'] if progressive_results else 0,
            'best_efficiency_stage': max(contribution_analysis, key=lambda x: x['efficiency'])['stage'] if contribution_analysis else -1
        }
    }
    
    output_path = os.path.join(layers_dir, output_file)
    with open(output_path, 'w') as f:
        json.dump(evaluation_results, f, indent=2)
    
    print(f"\nâœ… è¯„ä¼°ç»“æœä¿å­˜: {output_path}")
    
    # æ‰“å°æ€»ç»“
    print(f"\nğŸ“Š è¯„ä¼°æ€»ç»“:")
    print(f"  æœ€ç»ˆPSNR: {progressive_results[-1]['psnr']:.2f}dB")
    print(f"  æ€»é«˜æ–¯çƒ: {progressive_results[-1]['gaussian_count']:,}")
    print(f"  æœ€é«˜æ•ˆé˜¶æ®µ: é˜¶æ®µ{evaluation_results['summary']['best_efficiency_stage']}")
    
    # æ‰¾å‡ºè´¡çŒ®æœ€å¤§çš„å±‚
    max_contrib = max(contribution_analysis, key=lambda x: x['psnr_contribution'])
    print(f"  æœ€å¤§è´¡çŒ®å±‚: {max_contrib['layers_description']} (+{max_contrib['psnr_contribution']:.2f}dB)")
    
    return evaluation_results

def main():
    print("ğŸ“Š å°ºå¯¸åˆ†å±‚PSNRè¯„ä¼°")
    print("=" * 40)
    
    layers_dir = "size_based_layers"
    
    if not os.path.exists(layers_dir):
        print(f"âŒ åˆ†å±‚ç›®å½•ä¸å­˜åœ¨: {layers_dir}")
        print("è¯·å…ˆè¿è¡Œ create_size_based_layers.py")
        return
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluate_size_layers(layers_dir)
    
    if results:
        print(f"\nğŸ‰ å°ºå¯¸åˆ†å±‚è¯„ä¼°å®Œæˆ!")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {layers_dir}/size_layers_evaluation.json")

if __name__ == "__main__":
    main() 