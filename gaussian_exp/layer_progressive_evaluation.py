import os
import sys
import torch
import numpy as np
import argparse
import json
from pathlib import Path

# æ·»åŠ 3dgsæ ¹ç›®å½•åˆ°path
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene import GaussianModel
from scene.cameras import Camera
from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary, qvec2rotmat
from arguments import ModelParams, PipelineParams
from gaussian_renderer import render
from utils.general_utils import PILtoTorch
from utils.graphics_utils import focal2fov
from utils.loss_utils import l1_loss
from PIL import Image

def psnr(img1, img2):
    """æŒ‰ç…§train.pyçš„PSNRè®¡ç®—"""
    mse = (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)
    return 20 * torch.log10(1.0 / torch.sqrt(mse))

def load_gaussians(ply_path):
    """åŠ è½½é«˜æ–¯çƒæ¨¡å‹"""
    print(f"ğŸ¯ åŠ è½½é«˜æ–¯çƒ: {ply_path}")
    gaussians = GaussianModel(3)
    gaussians.load_ply(ply_path, use_train_test_exp=False)
    return gaussians

def analyze_position_layers(gaussians, num_layers=5):
    """æŒ‰ä½ç½®åˆ†å±‚åˆ†æå¹¶è¿”å›å±‚çº§æ©ç """
    print(f"ğŸ“ æŒ‰ä½ç½®åˆ†å±‚åˆ†æ ({num_layers}å±‚)")
    
    xyz = gaussians.get_xyz.detach().cpu().numpy()
    opacity = gaussians.get_opacity.detach().cpu().numpy().squeeze()
    
    # è¿‡æ»¤æ‰NaNå€¼
    valid_mask = ~np.isnan(xyz).any(axis=1) & ~np.isnan(opacity)
    total_count = len(xyz)
    valid_indices = np.where(valid_mask)[0]
    
    xyz_valid = xyz[valid_mask]
    print(f"  æœ‰æ•ˆé«˜æ–¯çƒæ•°é‡: {len(xyz_valid)} / {total_count}")
    
    # æŒ‰Zè½´åˆ†å±‚ï¼ˆæ·±åº¦ï¼‰
    z_min, z_max = xyz_valid[:, 2].min(), xyz_valid[:, 2].max()
    z_step = (z_max - z_min) / num_layers
    
    print(f"  Zè½´èŒƒå›´: [{z_min:.3f}, {z_max:.3f}]")
    print(f"  æ¯å±‚åšåº¦: {z_step:.3f}")
    
    layer_masks = []
    layer_info = []
    
    for i in range(num_layers):
        z_start = z_min + i * z_step
        z_end = z_min + (i + 1) * z_step
        
        # æœ€åä¸€å±‚åŒ…å«è¾¹ç•Œ
        if i == num_layers - 1:
            layer_valid_mask = (xyz_valid[:, 2] >= z_start) & (xyz_valid[:, 2] <= z_end)
        else:
            layer_valid_mask = (xyz_valid[:, 2] >= z_start) & (xyz_valid[:, 2] < z_end)
        
        # å°†æœ‰æ•ˆé«˜æ–¯çƒçš„æ©ç æ˜ å°„å›å…¨å±€æ©ç 
        global_mask = np.zeros(total_count, dtype=bool)
        global_mask[valid_indices[layer_valid_mask]] = True
        
        layer_count = global_mask.sum()
        layer_masks.append(global_mask)
        
        info = {
            'layer_id': i,
            'z_range': [z_start, z_end],
            'count': layer_count,
            'percentage': layer_count / total_count * 100
        }
        layer_info.append(info)
        
        print(f"  å±‚ {i:2d} [{z_start:7.2f}, {z_end:7.2f}]: {layer_count:7d}ä¸ªé«˜æ–¯çƒ ({info['percentage']:5.1f}%)")
    
    return layer_masks, layer_info

def create_layer_ply_files(gaussians, layer_masks, layer_info, output_dir):
    """ä¸ºæ¯ä¸€å±‚åˆ›å»ºPLYæ–‡ä»¶"""
    print(f"\nğŸ’¾ åˆ›å»ºåˆ†å±‚PLYæ–‡ä»¶...")
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰é«˜æ–¯çƒå±æ€§
    xyz = gaussians.get_xyz
    features_dc = gaussians._features_dc
    features_rest = gaussians._features_rest
    scaling = gaussians._scaling
    rotation = gaussians._rotation
    opacity = gaussians._opacity
    
    layer_files = []
    
    for i, (mask, info) in enumerate(zip(layer_masks, layer_info)):
        if mask.sum() == 0:
            print(f"  âš ï¸ å±‚ {i} ä¸ºç©ºï¼Œè·³è¿‡")
            continue
            
        # åˆ›å»ºæ–°çš„é«˜æ–¯æ¨¡å‹
        layer_gaussians = GaussianModel(3)
        
        # å¤åˆ¶å±æ€§åˆ°æ–°æ¨¡å‹
        layer_gaussians._xyz = xyz[mask].clone()
        layer_gaussians._features_dc = features_dc[mask].clone()
        layer_gaussians._features_rest = features_rest[mask].clone()
        layer_gaussians._scaling = scaling[mask].clone()
        layer_gaussians._rotation = rotation[mask].clone()
        layer_gaussians._opacity = opacity[mask].clone()
        
        # è®¾ç½®å…¶ä»–å¿…è¦å±æ€§
        layer_gaussians.active_sh_degree = gaussians.active_sh_degree
        layer_gaussians.max_sh_degree = gaussians.max_sh_degree
        
        # ä¿å­˜PLYæ–‡ä»¶
        layer_file = os.path.join(output_dir, f"layer_{i}_z{info['z_range'][0]:.1f}to{info['z_range'][1]:.1f}_{info['count']}balls.ply")
        
        # æ‰‹åŠ¨ä¿å­˜PLYï¼ˆå› ä¸ºsave_plyå¯èƒ½éœ€è¦ä¼˜åŒ–å™¨ï¼‰
        save_layer_ply(layer_gaussians, layer_file)
        
        layer_files.append(layer_file)
        print(f"  âœ… å±‚ {i}: {layer_file} ({info['count']}ä¸ªé«˜æ–¯çƒ)")
    
    return layer_files

def save_layer_ply(gaussians, path):
    """æ‰‹åŠ¨ä¿å­˜PLYæ–‡ä»¶"""
    import plyfile
    
    xyz = gaussians._xyz.detach().cpu().numpy()
    normals = np.zeros_like(xyz)
    f_dc = gaussians._features_dc.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
    f_rest = gaussians._features_rest.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
    opacities = gaussians._opacity.detach().cpu().numpy()
    scale = gaussians._scaling.detach().cpu().numpy()
    rotation = gaussians._rotation.detach().cpu().numpy()

    dtype_full = [(attribute, 'f4') for attribute in ['x', 'y', 'z', 'nx', 'ny', 'nz']]
    dtype_full += [(attribute, 'f4') for attribute in ['f_dc_0', 'f_dc_1', 'f_dc_2']]
    dtype_full += [(f'f_rest_{i}', 'f4') for i in range(f_rest.shape[1])]
    dtype_full += [('opacity', 'f4')]
    dtype_full += [(f'scale_{i}', 'f4') for i in range(scale.shape[1])]
    dtype_full += [(f'rot_{i}', 'f4') for i in range(rotation.shape[1])]

    attributes = np.concatenate((xyz, normals, f_dc, f_rest, opacities, scale, rotation), axis=1)
    elements = np.empty(xyz.shape[0], dtype=dtype_full)
    
    for i, attr in enumerate(dtype_full):
        elements[attr[0]] = attributes[:, i]

    vertex_element = plyfile.PlyElement.describe(elements, 'vertex')
    plyfile.PlyData([vertex_element]).write(path)

def create_progressive_ply_files(layer_files, layer_info, output_dir):
    """åˆ›å»ºæ¸è¿›å¼ç´¯ç§¯çš„PLYæ–‡ä»¶ (1, 1+2, 1+2+3, ...)"""
    print(f"\nğŸ“ˆ åˆ›å»ºæ¸è¿›å¼ç´¯ç§¯PLYæ–‡ä»¶...")
    
    progressive_files = []
    
    for i in range(len(layer_files)):
        # ç´¯ç§¯å‰i+1å±‚
        combined_gaussians = None
        total_count = 0
        layer_names = []
        
        for j in range(i + 1):
            layer_gaussians = GaussianModel(3)
            layer_gaussians.load_ply(layer_files[j], use_train_test_exp=False)
            
            if combined_gaussians is None:
                combined_gaussians = layer_gaussians
            else:
                # åˆå¹¶é«˜æ–¯çƒ
                combined_gaussians = combine_gaussians(combined_gaussians, layer_gaussians)
            
            total_count += layer_info[j]['count']
            layer_names.append(f"L{j}")
        
        # ä¿å­˜ç´¯ç§¯æ–‡ä»¶
        progressive_file = os.path.join(output_dir, f"progressive_{'_'.join(layer_names)}_{total_count}balls.ply")
        save_layer_ply(combined_gaussians, progressive_file)
        progressive_files.append(progressive_file)
        
        print(f"  âœ… ç´¯ç§¯ {'->'.join(layer_names)}: {progressive_file} ({total_count}ä¸ªé«˜æ–¯çƒ)")
    
    return progressive_files

def combine_gaussians(gaussians1, gaussians2):
    """åˆå¹¶ä¸¤ä¸ªé«˜æ–¯æ¨¡å‹"""
    combined = GaussianModel(3)
    
    # åˆå¹¶æ‰€æœ‰å±æ€§
    combined._xyz = torch.cat([gaussians1._xyz, gaussians2._xyz], dim=0)
    combined._features_dc = torch.cat([gaussians1._features_dc, gaussians2._features_dc], dim=0)
    combined._features_rest = torch.cat([gaussians1._features_rest, gaussians2._features_rest], dim=0)
    combined._scaling = torch.cat([gaussians1._scaling, gaussians2._scaling], dim=0)
    combined._rotation = torch.cat([gaussians1._rotation, gaussians2._rotation], dim=0)
    combined._opacity = torch.cat([gaussians1._opacity, gaussians2._opacity], dim=0)
    
    # è®¾ç½®å…¶ä»–å±æ€§
    combined.active_sh_degree = gaussians1.active_sh_degree
    combined.max_sh_degree = gaussians1.max_sh_degree
    
    return combined

def load_single_camera(colmap_path, images_path, resolution_scale=2.0):
    """åŠ è½½å•ä¸ªç›¸æœºç”¨äºå¿«é€Ÿæµ‹è¯•"""
    cameras_bin = os.path.join(colmap_path, 'cameras.bin')
    images_bin = os.path.join(colmap_path, 'images.bin')
    
    cam_intrinsics = read_intrinsics_binary(cameras_bin)
    cam_extrinsics = read_extrinsics_binary(images_bin)
    
    # å–ç¬¬ä¸€ä¸ªç›¸æœº
    first_img_id = list(cam_extrinsics.keys())[0]
    img_info = cam_extrinsics[first_img_id]
    intrinsic = cam_intrinsics[img_info.camera_id]
    
    # è§£æå‚æ•°
    fx, fy, cx, cy = intrinsic.params
    width = int(intrinsic.width / resolution_scale)
    height = int(intrinsic.height / resolution_scale)
    fx_scaled = fx / resolution_scale
    fy_scaled = fy / resolution_scale
    
    FoVx = focal2fov(fx_scaled, width)
    FoVy = focal2fov(fy_scaled, height)
    
    R = np.transpose(qvec2rotmat(img_info.qvec))
    T = np.array(img_info.tvec)
    
    # åŠ è½½å›¾åƒ
    image_path = os.path.join(images_path, img_info.name)
    image = Image.open(image_path)
    if resolution_scale != 1.0:
        image = image.resize((width, height), Image.LANCZOS)
    
    camera = Camera(
        resolution=(width, height),
        colmap_id=first_img_id,
        R=R,
        T=T,
        FoVx=FoVx,
        FoVy=FoVy,
        depth_params=None,
        image=image,
        invdepthmap=None,
        image_name=img_info.name,
        uid=0,
        data_device="cuda",
        train_test_exp=False,
        is_test_dataset=False,
        is_test_view=False
    )
    
    return camera

def evaluate_ply_file(ply_path, camera, pipe, background):
    """è¯„ä¼°å•ä¸ªPLYæ–‡ä»¶"""
    if not os.path.exists(ply_path):
        return {"psnr": 0.0, "l1_loss": 0.0, "error": "File not found"}
    
    try:
        # åŠ è½½é«˜æ–¯çƒ
        gaussians = GaussianModel(3)
        gaussians.load_ply(ply_path, use_train_test_exp=False)
        
        # æ£€æŸ¥SPARSE_ADAM_AVAILABLE
        try:
            from diff_gaussian_rasterization import SparseGaussianAdam
            SPARSE_ADAM_AVAILABLE = True
        except:
            SPARSE_ADAM_AVAILABLE = False
        
        # æ¸²æŸ“
        render_result = render(camera, gaussians, pipe, background, 1., SPARSE_ADAM_AVAILABLE, None, False)
        rendered_image = torch.clamp(render_result["render"], 0.0, 1.0)
        
        # GTå›¾åƒ
        gt_image = torch.clamp(camera.original_image.to("cuda"), 0.0, 1.0)
        
        # è®¡ç®—æŒ‡æ ‡
        psnr_val = psnr(rendered_image, gt_image).mean().item()
        l1_val = l1_loss(rendered_image, gt_image).mean().item()
        
        return {
            "psnr": psnr_val,
            "l1_loss": l1_val,
            "gaussian_count": gaussians.get_xyz.shape[0],
            "error": None
        }
        
    except Exception as e:
        return {"psnr": 0.0, "l1_loss": 0.0, "error": str(e)}

def main():
    parser = argparse.ArgumentParser(description='åˆ†å±‚æ¸è¿›å¼é«˜æ–¯çƒè¯„ä¼°')
    parser.add_argument('--ply-path', type=str, required=True, help='åŸå§‹PLYæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model-path', type=str, required=True, help='æ¨¡å‹è·¯å¾„')
    parser.add_argument('--num-layers', type=int, default=5, help='åˆ†å±‚æ•°é‡')
    parser.add_argument('--output-dir', type=str, default='layer_progressive_analysis', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resolution-scale', type=float, default=2.0, help='åˆ†è¾¨ç‡ç¼©æ”¾')
    
    args = parser.parse_args()
    
    print("ğŸ” åˆ†å±‚æ¸è¿›å¼é«˜æ–¯çƒè¯„ä¼°")
    print("=" * 60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # 1. åŠ è½½åŸå§‹é«˜æ–¯çƒ
    gaussians = load_gaussians(args.ply_path)
    
    # 2. åˆ†å±‚åˆ†æ
    layer_masks, layer_info = analyze_position_layers(gaussians, args.num_layers)
    
    # 3. åˆ›å»ºåˆ†å±‚PLYæ–‡ä»¶
    layer_files = create_layer_ply_files(gaussians, layer_masks, layer_info, args.output_dir)
    
    # 4. åˆ›å»ºæ¸è¿›å¼PLYæ–‡ä»¶
    progressive_files = create_progressive_ply_files(layer_files, layer_info, args.output_dir)
    
    # 5. è®¾ç½®è¯„ä¼°ç¯å¢ƒ
    print(f"\nğŸ¨ è®¾ç½®è¯„ä¼°ç¯å¢ƒ...")
    
    # Pipelineå‚æ•°
    pipeline_parser = argparse.ArgumentParser()
    pipe_parser = PipelineParams(pipeline_parser)
    pipe_args = pipeline_parser.parse_args([])
    pipe = pipe_parser.extract(pipe_args)
    
    # èƒŒæ™¯
    background = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
    
    # åŠ è½½æµ‹è¯•ç›¸æœº
    colmap_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/sparse/0"
    images_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/images"
    camera = load_single_camera(colmap_path, images_path, args.resolution_scale)
    
    print(f"âœ… ä½¿ç”¨ç›¸æœº: {camera.image_name}")
    
    # 6. è¯„ä¼°æ‰€æœ‰æ–‡ä»¶
    print(f"\nğŸ“Š è¯„ä¼°åˆ†å±‚æ–‡ä»¶...")
    
    all_results = []
    
    # è¯„ä¼°å•å±‚æ–‡ä»¶
    for i, (layer_file, info) in enumerate(zip(layer_files, layer_info)):
        print(f"  è¯„ä¼°å±‚ {i}...")
        result = evaluate_ply_file(layer_file, camera, pipe, background)
        result.update({
            "type": "single_layer",
            "layer_id": i,
            "layer_range": info['z_range'],
            "file_path": layer_file
        })
        all_results.append(result)
        
        if result["error"]:
            print(f"    âŒ é”™è¯¯: {result['error']}")
        else:
            print(f"    âœ… PSNR: {result['psnr']:.3f} dB, L1: {result['l1_loss']:.6f}, é«˜æ–¯çƒ: {result['gaussian_count']}")
    
    # è¯„ä¼°æ¸è¿›å¼æ–‡ä»¶
    print(f"\nğŸ“ˆ è¯„ä¼°æ¸è¿›å¼ç´¯ç§¯æ–‡ä»¶...")
    for i, prog_file in enumerate(progressive_files):
        print(f"  è¯„ä¼°ç´¯ç§¯ L0-L{i}...")
        result = evaluate_ply_file(prog_file, camera, pipe, background)
        result.update({
            "type": "progressive",
            "layers_included": list(range(i + 1)),
            "file_path": prog_file
        })
        all_results.append(result)
        
        if result["error"]:
            print(f"    âŒ é”™è¯¯: {result['error']}")
        else:
            print(f"    âœ… PSNR: {result['psnr']:.3f} dB, L1: {result['l1_loss']:.6f}, é«˜æ–¯çƒ: {result['gaussian_count']}")
    
    # 7. ä¿å­˜ç»“æœ
    results_file = os.path.join(args.output_dir, 'layer_progressive_results.json')
    with open(results_file, 'w') as f:
        json.dump({
            'layer_info': layer_info,
            'evaluation_results': all_results,
            'camera_info': {
                'name': camera.image_name,
                'resolution': [camera.image_width, camera.image_height],
                'resolution_scale': args.resolution_scale
            },
            'original_file': args.ply_path,
            'total_gaussians': gaussians.get_xyz.shape[0]
        }, f, indent=2)
    
    print(f"\nğŸ‰ åˆ†å±‚è¯„ä¼°å®Œæˆ!")
    print(f"ğŸ“Š ç»“æœä¿å­˜åœ¨: {args.output_dir}/")
    print(f"ğŸ“ åˆ†å±‚æ–‡ä»¶: {len(layer_files)}ä¸ª")
    print(f"ğŸ“ˆ æ¸è¿›æ–‡ä»¶: {len(progressive_files)}ä¸ª")
    print(f"ğŸ“‹ è¯„ä¼°ç»“æœ: {results_file}")

if __name__ == "__main__":
    main() 