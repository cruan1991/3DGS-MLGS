import os
import sys
import torch
import numpy as np
import argparse
import json
import glob
from PIL import Image
import matplotlib.pyplot as plt
from plyfile import PlyData, PlyElement

# æ·»åŠ 3dgsæ ¹ç›®å½•åˆ°path
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene import GaussianModel
from scene.cameras import Camera
from scene.colmap_loader import read_intrinsics_binary, read_extrinsics_binary, qvec2rotmat
from arguments import ModelParams, PipelineParams
from gaussian_renderer import render
from utils.graphics_utils import focal2fov
from utils.loss_utils import l1_loss

def psnr(img1, img2):
    """æŒ‰ç…§train.pyçš„PSNRè®¡ç®—"""
    mse = (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)
    return 20 * torch.log10(1.0 / torch.sqrt(mse))

def load_test_camera(colmap_path, images_path, camera_name="000001.jpg", resolution_scale=4.0):
    """åŠ è½½æµ‹è¯•ç›¸æœº"""
    cameras_bin = os.path.join(colmap_path, 'cameras.bin')
    images_bin = os.path.join(colmap_path, 'images.bin')
    
    cam_intrinsics = read_intrinsics_binary(cameras_bin)
    cam_extrinsics = read_extrinsics_binary(images_bin)
    
    # æ‰¾åˆ°æŒ‡å®šç›¸æœº
    target_img_id = None
    for img_id, img_info in cam_extrinsics.items():
        if img_info.name == camera_name:
            target_img_id = img_id
            break
    
    if target_img_id is None:
        print(f"âŒ æœªæ‰¾åˆ°ç›¸æœº: {camera_name}")
        return None
    
    img_info = cam_extrinsics[target_img_id]
    intrinsic = cam_intrinsics[img_info.camera_id]
    
    # è§£æå‚æ•°
    fx, fy, cx, cy = intrinsic.params
    width = int(intrinsic.width / resolution_scale)
    height = int(intrinsic.height / resolution_scale)
    fx_scaled = fx / resolution_scale
    fy_scaled = fy / resolution_scale
    
    FoVx = focal2fov(fx_scaled, width)
    FoVy = focal2fov(fy_scaled, height)
    
    R = np.transpose(qvec2rotmat(img_info.qvec))
    T = np.array(img_info.tvec)
    
    # åŠ è½½å›¾åƒ
    image_path = os.path.join(images_path, img_info.name)
    image = Image.open(image_path)
    if resolution_scale != 1.0:
        image = image.resize((width, height), Image.LANCZOS)
    
    camera = Camera(
        resolution=(width, height),
        colmap_id=target_img_id,
        R=R,
        T=T,
        FoVx=FoVx,
        FoVy=FoVy,
        depth_params=None,
        image=image,
        invdepthmap=None,
        image_name=img_info.name,
        uid=0,
        data_device="cuda",
        train_test_exp=False,
        is_test_dataset=False,
        is_test_view=False
    )
    
    return camera

def save_gaussians_like_original(xyz_data, normals_data, f_dc_data, f_rest_data, 
                               opacity_data, scale_data, rotation_data, output_path):
    """ä½¿ç”¨ä¸åŸå§‹save_plyå®Œå…¨ç›¸åŒçš„é€»è¾‘ä¿å­˜"""
    
    # æ„é€ å±æ€§åˆ—è¡¨ (ä¸åŸå§‹é€»è¾‘ç›¸åŒ)
    def construct_list_of_attributes():
        l = ['x', 'y', 'z', 'nx', 'ny', 'nz']
        # DCç‰¹å¾
        for i in range(f_dc_data.shape[1]):
            l.append('f_dc_{}'.format(i))
        # Restç‰¹å¾  
        for i in range(f_rest_data.shape[1]):
            l.append('f_rest_{}'.format(i))
        l.append('opacity')
        for i in range(scale_data.shape[1]):
            l.append('scale_{}'.format(i))
        for i in range(rotation_data.shape[1]):
            l.append('rot_{}'.format(i))
        return l
    
    # ç»„åˆæ‰€æœ‰å±æ€§ (ä¸åŸå§‹å®Œå…¨ç›¸åŒ)
    dtype_full = [(attribute, 'f4') for attribute in construct_list_of_attributes()]
    elements = np.empty(xyz_data.shape[0], dtype=dtype_full)
    attributes = np.concatenate((xyz_data, normals_data, f_dc_data, f_rest_data, 
                               opacity_data, scale_data, rotation_data), axis=1)
    elements[:] = list(map(tuple, attributes))
    
    # ä¿å­˜æ–‡ä»¶
    el = PlyElement.describe(elements, 'vertex')
    PlyData([el]).write(output_path)

def create_fine_scale_layers():
    """åˆ›å»ºåŸºäºçœŸå®å°ºå¯¸çš„ç²¾ç»†åˆ†å±‚"""
    print("ğŸ”„ åˆ›å»ºåŸºäºçœŸå®å°ºå¯¸çš„ç²¾ç»†åˆ†å±‚")
    print("=" * 60)
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    ply_path = "./output/truck-150w/gaussian_ball/iteration_994230_best_psnr/gaussian_ball.ply"
    gaussians = GaussianModel(3)
    gaussians.load_ply(ply_path, use_train_test_exp=False)
    
    # ä½¿ç”¨ä¸åŸå§‹save_plyå®Œå…¨ç›¸åŒçš„é€»è¾‘æå–æ•°æ®
    xyz = gaussians._xyz.detach().cpu().numpy()
    normals = np.zeros_like(xyz)
    f_dc = gaussians._features_dc.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
    f_rest = gaussians._features_rest.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
    opacity = gaussians._opacity.detach().cpu().numpy()
    scaling = gaussians._scaling.detach().cpu().numpy()  # log space
    rotation = gaussians._rotation.detach().cpu().numpy()
    
    # å¤„ç†NaNå€¼
    nan_mask = np.isnan(xyz)
    nan_positions = np.any(nan_mask, axis=1)
    nan_count = np.sum(nan_positions)
    
    if nan_count > 0:
        print(f"âš ï¸ å‘ç° {nan_count} ä¸ªNaNä½ç½®ï¼Œå°†è¢«æ’é™¤")
        valid_mask = ~nan_positions
        xyz = xyz[valid_mask]
        normals = normals[valid_mask]
        f_dc = f_dc[valid_mask]
        f_rest = f_rest[valid_mask]
        opacity = opacity[valid_mask]
        scaling = scaling[valid_mask]
        rotation = rotation[valid_mask]
    
    # è½¬æ¢åˆ°å®é™…å°ºå¯¸
    actual_scales = np.exp(scaling)  # ä»log spaceè½¬æ¢åˆ°real space
    avg_actual_scale = np.mean(actual_scales, axis=1)
    max_actual_scale = np.max(actual_scales, axis=1)
    
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æœ‰æ•ˆé«˜æ–¯çƒæ•°: {len(avg_actual_scale):,}")
    print(f"  å¹³å‡å®é™…å°ºå¯¸èŒƒå›´: {avg_actual_scale.min():.6f} ~ {avg_actual_scale.max():.6f}")
    print(f"  æœ€å¤§å®é™…å°ºå¯¸èŒƒå›´: {max_actual_scale.min():.6f} ~ {max_actual_scale.max():.6f}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "fine_scale_layers"
    os.makedirs(output_dir, exist_ok=True)
    
    # ç²¾ç»†åˆ†å±‚æ–¹æ¡ˆï¼š15å±‚ (è‡³å°‘3ç»„ç»†åˆ†)
    print(f"\nğŸ¯ ç²¾ç»†åˆ†å±‚æ–¹æ¡ˆ: 15å±‚")
    
    # ä½¿ç”¨å¹³å‡å®é™…å°ºå¯¸è¿›è¡Œåˆ†å±‚
    layer_percentiles = np.linspace(6.67, 93.33, 14)  # 15å±‚çš„14ä¸ªåˆ†ç•Œç‚¹
    thresholds = np.percentile(avg_actual_scale, layer_percentiles)
    
    layer_info = []
    single_layer_files = []
    
    print(f"ğŸ“‹ å•å±‚æ–‡ä»¶ç”Ÿæˆ:")
    for i in range(15):
        if i == 0:
            mask = avg_actual_scale <= thresholds[0]
            layer_name = f"nano_{i:02d}"
            layer_desc = "çº³ç±³çº§"
        elif i == 14:
            mask = avg_actual_scale > thresholds[13]
            layer_name = f"giant_{i:02d}"
            layer_desc = "å·¨å‹"
        else:
            mask = (avg_actual_scale > thresholds[i-1]) & (avg_actual_scale <= thresholds[i])
            if i < 5:
                layer_name = f"micro_{i:02d}"
                layer_desc = f"å¾®å‹{i}"
            elif i < 10:
                layer_name = f"small_{i:02d}"
                layer_desc = f"å°å‹{i-4}"
            else:
                layer_name = f"medium_{i:02d}"
                layer_desc = f"ä¸­å‹{i-9}"
        
        count = np.sum(mask)
        if count > 0:
            ratio = count / len(avg_actual_scale) * 100
            range_str = f"{avg_actual_scale[mask].min():.6f}~{avg_actual_scale[mask].max():.6f}"
            
            # ä¿å­˜å•å±‚æ–‡ä»¶
            filename = f"layer_{i:02d}_{layer_name}_{count}balls.ply"
            layer_path = os.path.join(output_dir, filename)
            
            save_gaussians_like_original(
                xyz[mask], normals[mask], f_dc[mask], f_rest[mask],
                opacity[mask], scaling[mask], rotation[mask], layer_path
            )
            
            layer_info.append({
                'id': i,
                'name': layer_name,
                'description': layer_desc,
                'count': int(count),
                'ratio': float(ratio),
                'scale_range': [float(avg_actual_scale[mask].min()), float(avg_actual_scale[mask].max())],
                'file': filename
            })
            
            single_layer_files.append(layer_path)
            print(f"  å±‚{i:2d} ({layer_desc}): {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {range_str} -> {filename}")
    
    print(f"\nğŸ”„ æ¸è¿›å¼ç´¯ç§¯æ–‡ä»¶ç”Ÿæˆ:")
    progressive_files = []
    
    for end_layer in range(len(layer_info)):
        # åˆå¹¶mask
        combined_mask = np.zeros(len(avg_actual_scale), dtype=bool)
        
        for layer_id in range(end_layer + 1):
            layer = layer_info[layer_id]
            if layer_id == 0:
                mask = avg_actual_scale <= thresholds[0]
            elif layer_id == len(layer_info) - 1:
                mask = avg_actual_scale > thresholds[13]
            else:
                mask = (avg_actual_scale > thresholds[layer_id-1]) & (avg_actual_scale <= thresholds[layer_id])
            
            combined_mask |= mask
        
        cumulative_count = np.sum(combined_mask)
        print(f"  ç´¯ç§¯0-{end_layer}: {cumulative_count:,}çƒ", end="")
        
        if cumulative_count == 0:
            print(f" (ç©ºå±‚ï¼Œè·³è¿‡)")
            continue
        
        # ä¿å­˜æ¸è¿›å¼æ–‡ä»¶
        if end_layer < 5:
            group_name = "micro"
        elif end_layer < 10:
            group_name = "small"
        else:
            group_name = "medium_large"
        
        prog_filename = f"progressive_{group_name}_L0_L{end_layer:02d}_{cumulative_count}balls.ply"
        prog_file_path = os.path.join(output_dir, prog_filename)
        
        save_gaussians_like_original(
            xyz[combined_mask], normals[combined_mask], f_dc[combined_mask], f_rest[combined_mask],
            opacity[combined_mask], scaling[combined_mask], rotation[combined_mask], prog_file_path
        )
        
        progressive_files.append(prog_file_path)
        print(f" -> {prog_filename}")
    
    # ä¿å­˜åˆ†å±‚ä¿¡æ¯
    layering_manifest = {
        'method': 'fine_actual_scale_based',
        'total_layers': len(layer_info),
        'total_gaussians': len(avg_actual_scale),
        'thresholds': [float(t) for t in thresholds],
        'layer_info': layer_info,
        'single_layer_files': [os.path.basename(f) for f in single_layer_files],
        'progressive_files': [os.path.basename(f) for f in progressive_files],
        'groups': {
            'micro': {'layers': list(range(5)), 'description': 'å¾®å‹çƒ (å±‚0-4)'},
            'small': {'layers': list(range(5, 10)), 'description': 'å°å‹çƒ (å±‚5-9)'},
            'medium_large': {'layers': list(range(10, 15)), 'description': 'ä¸­å¤§å‹çƒ (å±‚10-14)'}
        }
    }
    
    manifest_path = os.path.join(output_dir, 'fine_layers_manifest.json')
    with open(manifest_path, 'w') as f:
        json.dump(layering_manifest, f, indent=2)
    
    print(f"\nâœ… ç²¾ç»†åˆ†å±‚å®Œæˆ:")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"  ğŸ“‹ åˆ†å±‚æ¸…å•: {manifest_path}")
    print(f"  ğŸ¯ å•å±‚æ–‡ä»¶: {len(single_layer_files)} ä¸ª")
    print(f"  ğŸ“ˆ æ¸è¿›æ–‡ä»¶: {len(progressive_files)} ä¸ª")
    print(f"  ğŸ“Š åˆ†ç»„: å¾®å‹(0-4), å°å‹(5-9), ä¸­å¤§å‹(10-14)")
    
    return layering_manifest

def evaluate_fine_layers():
    """è¯„ä¼°ç²¾ç»†åˆ†å±‚çš„æ¸²æŸ“æ•ˆæœå’ŒPSNR"""
    print("\nğŸ“ˆ è¯„ä¼°ç²¾ç»†åˆ†å±‚æ¸²æŸ“æ•ˆæœå’ŒPSNR")
    print("=" * 60)
    
    # è®¾ç½®æ¸²æŸ“ç¯å¢ƒ
    pipeline_parser = argparse.ArgumentParser()
    pipe_parser = PipelineParams(pipeline_parser)
    pipe_args = pipeline_parser.parse_args([])
    pipe = pipe_parser.extract(pipe_args)
    
    background = torch.tensor([0, 0, 0], dtype=torch.float32, device="cuda")
    
    # åŠ è½½ç›¸æœº
    colmap_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/sparse/0"
    images_path = "/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs/data/mipnerf360/360/tandt_db/tandt/truck/images"
    camera = load_test_camera(colmap_path, images_path, "000001.jpg", 5.0)  # 5xç¼©æ”¾å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
    
    if camera is None:
        return
    
    print(f"âœ… åŠ è½½æµ‹è¯•ç›¸æœº: 000001.jpg (åˆ†è¾¨ç‡: {camera.image_width}x{camera.image_height})")
    
    # åŠ è½½åˆ†å±‚æ¸…å•
    manifest_path = "fine_scale_layers/fine_layers_manifest.json"
    if not os.path.exists(manifest_path):
        print(f"âŒ åˆ†å±‚æ¸…å•ä¸å­˜åœ¨: {manifest_path}")
        return
    
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = 'fine_layers_evaluation'
    os.makedirs(output_dir, exist_ok=True)
    
    # æ£€æŸ¥SPARSE_ADAM_AVAILABLE
    try:
        from diff_gaussian_rasterization import SparseGaussianAdam
        SPARSE_ADAM_AVAILABLE = True
    except:
        SPARSE_ADAM_AVAILABLE = False
    
    def render_ply_file(ply_path):
        """æ¸²æŸ“PLYæ–‡ä»¶å¹¶è®¡ç®—PSNR"""
        if not os.path.exists(ply_path):
            return None, {"error": "File not found"}
        
        try:
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
            
            # åŠ è½½é«˜æ–¯çƒ
            gaussians = GaussianModel(3)
            gaussians.load_ply(ply_path, use_train_test_exp=False)
            
            gaussian_count = gaussians.get_xyz.shape[0]
            
            # æ¸²æŸ“
            render_result = render(camera, gaussians, pipe, background, 1., 
                                 SPARSE_ADAM_AVAILABLE, None, False)
            rendered_image = torch.clamp(render_result["render"], 0.0, 1.0)
            
            # GTå›¾åƒ
            gt_image = torch.clamp(camera.original_image.to("cuda"), 0.0, 1.0)
            
            # è®¡ç®—æŒ‡æ ‡
            psnr_val = psnr(rendered_image, gt_image).mean().item()
            l1_val = l1_loss(rendered_image, gt_image).mean().item()
            
            # è½¬æ¢ä¸ºnumpyç”¨äºå¯è§†åŒ–
            rendered_np = rendered_image.detach().cpu().numpy().transpose(1, 2, 0)
            gt_np = gt_image.detach().cpu().numpy().transpose(1, 2, 0)
            
            # æ¸…ç†å†…å­˜
            del gaussians, render_result, rendered_image, gt_image
            torch.cuda.empty_cache()
            
            return (rendered_np, gt_np), {
                "psnr": psnr_val,
                "l1_loss": l1_val,
                "gaussian_count": gaussian_count
            }
            
        except Exception as e:
            print(f"    âŒ æ¸²æŸ“å¤±è´¥: {str(e)[:50]}...")
            torch.cuda.empty_cache()
            return None, {"error": str(e)}
    
    # 1. è¯„ä¼°æ¸è¿›å¼æ–‡ä»¶
    print(f"\nğŸ¯ è¯„ä¼°æ¸è¿›å¼ç´¯ç§¯æ•ˆæœ:")
    
    progressive_files = [f"fine_scale_layers/{f}" for f in manifest['progressive_files']]
    progressive_results = []
    
    for i, prog_file in enumerate(progressive_files):
        filename = os.path.basename(prog_file)
        print(f"\nğŸ¨ æ¸²æŸ“æ¸è¿›æ–‡ä»¶ {i+1}/{len(progressive_files)}: {filename}")
        
        file_size_mb = os.path.getsize(prog_file) / (1024 * 1024)
        print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {file_size_mb:.1f}MB")
        
        images, metrics = render_ply_file(prog_file)
        
        if images is not None:
            print(f"   âœ… PSNR: {metrics['psnr']:.3f}dB, é«˜æ–¯çƒæ•°: {metrics['gaussian_count']:,}")
            
            progressive_results.append({
                'stage': i,
                'file': filename,
                'file_size_mb': file_size_mb,
                'psnr': metrics['psnr'],
                'l1_loss': metrics['l1_loss'],
                'gaussian_count': metrics['gaussian_count'],
                'images': images
            })
        else:
            print(f"   âŒ æ¸²æŸ“å¤±è´¥: {metrics.get('error', 'Unknown')}")
    
    # 2. è¯„ä¼°å…³é”®å•å±‚æ–‡ä»¶ï¼ˆæ¯5å±‚é‡‡æ ·ä¸€æ¬¡ï¼‰
    print(f"\nğŸ¯ è¯„ä¼°å…³é”®å•å±‚æ•ˆæœ:")
    
    single_layer_results = []
    key_layers = [0, 4, 9, 14]  # æ¯ç»„çš„ä»£è¡¨å±‚
    
    for layer_id in key_layers:
        if layer_id < len(manifest['layer_info']):
            layer = manifest['layer_info'][layer_id]
            layer_file = f"fine_scale_layers/{layer['file']}"
            
            print(f"\nğŸ¨ æ¸²æŸ“å•å±‚ {layer_id}: {layer['description']}")
            
            images, metrics = render_ply_file(layer_file)
            
            if images is not None:
                print(f"   âœ… PSNR: {metrics['psnr']:.3f}dB, é«˜æ–¯çƒæ•°: {metrics['gaussian_count']:,}")
                
                single_layer_results.append({
                    'layer_id': layer_id,
                    'layer_name': layer['name'],
                    'layer_description': layer['description'],
                    'file': layer['file'],
                    'psnr': metrics['psnr'],
                    'l1_loss': metrics['l1_loss'],
                    'gaussian_count': metrics['gaussian_count'],
                    'images': images
                })
            else:
                print(f"   âŒ æ¸²æŸ“å¤±è´¥: {metrics.get('error', 'Unknown')}")
    
    # 3. ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–
    print(f"\nğŸ¨ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")
    
    # æ¸è¿›å¼å¯¹æ¯”å›¾
    if progressive_results:
        fig, axes = plt.subplots(2, 4, figsize=(20, 10))
        fig.suptitle('Fine-Scale Progressive Layering Evaluation', fontsize=16, fontweight='bold')
        
        # æ˜¾ç¤ºå‰7ä¸ªæ¸è¿›é˜¶æ®µ
        for i in range(min(7, len(progressive_results))):
            row = i // 4
            col = i % 4
            
            if row < 2 and col < 4:
                ax = axes[row, col]
                result = progressive_results[i]
                
                ax.imshow(result['images'][0])  # æ˜¾ç¤ºæ¸²æŸ“å›¾åƒ
                
                title = f"L0-L{i:02d}\n{result['gaussian_count']:,} balls"
                title += f"\nPSNR: {result['psnr']:.2f}dB"
                
                ax.set_title(title, fontsize=10)
                ax.axis('off')
        
        # æœ€åä¸€ä¸ªå­å›¾æ˜¾ç¤ºPSNRæ›²çº¿
        ax = axes[1, 3]
        stages = [r['stage'] for r in progressive_results]
        psnr_values = [r['psnr'] for r in progressive_results]
        
        ax.plot(stages, psnr_values, 'bo-', linewidth=2, markersize=6)
        ax.set_xlabel('Progressive Stage')
        ax.set_ylabel('PSNR (dB)')
        ax.set_title('PSNR Evolution', fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # æ ‡æ³¨æ•°å€¼
        for stage, psnr_val in zip(stages[::2], psnr_values[::2]):  # æ¯éš”ä¸€ä¸ªæ ‡æ³¨
            ax.annotate(f'{psnr_val:.1f}', (stage, psnr_val),
                       textcoords="offset points", xytext=(0,5), 
                       ha='center', fontsize=8)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        progressive_comparison_file = os.path.join(output_dir, 'fine_progressive_comparison.png')
        plt.savefig(progressive_comparison_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ¸è¿›å¯¹æ¯”å›¾ä¿å­˜: {progressive_comparison_file}")
    
    # å•å±‚å¯¹æ¯”å›¾
    if single_layer_results:
        fig, axes = plt.subplots(1, len(single_layer_results), figsize=(5*len(single_layer_results), 5))
        if len(single_layer_results) == 1:
            axes = [axes]
        
        fig.suptitle('Key Single Layers Comparison', fontsize=16, fontweight='bold')
        
        for i, result in enumerate(single_layer_results):
            ax = axes[i]
            ax.imshow(result['images'][0])
            
            title = f"Layer {result['layer_id']}\n{result['layer_description']}"
            title += f"\n{result['gaussian_count']:,} balls"
            title += f"\nPSNR: {result['psnr']:.2f}dB"
            
            ax.set_title(title, fontsize=10)
            ax.axis('off')
        
        plt.tight_layout()
        
        single_comparison_file = os.path.join(output_dir, 'fine_single_layers_comparison.png')
        plt.savefig(single_comparison_file, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å•å±‚å¯¹æ¯”å›¾ä¿å­˜: {single_comparison_file}")
    
    # 4. ä¿å­˜è¯¦ç»†ç»“æœ
    evaluation_results = {
        'test_camera': '000001.jpg',
        'resolution_scale': 5.0,
        'total_layers': manifest['total_layers'],
        'total_gaussians': manifest['total_gaussians'],
        'progressive_evaluation': [
            {
                'stage': r['stage'],
                'file': r['file'],
                'psnr': r['psnr'],
                'l1_loss': r['l1_loss'],
                'gaussian_count': r['gaussian_count'],
                'file_size_mb': r['file_size_mb']
            }
            for r in progressive_results
        ],
        'single_layer_evaluation': [
            {
                'layer_id': r['layer_id'],
                'layer_name': r['layer_name'],
                'layer_description': r['layer_description'],
                'psnr': r['psnr'],
                'l1_loss': r['l1_loss'],
                'gaussian_count': r['gaussian_count']
            }
            for r in single_layer_results
        ],
        'analysis': {
            'psnr_range': [min(r['psnr'] for r in progressive_results), max(r['psnr'] for r in progressive_results)] if progressive_results else None,
            'best_single_layer': max(single_layer_results, key=lambda x: x['psnr']) if single_layer_results else None,
            'progressive_improvement': progressive_results[-1]['psnr'] - progressive_results[0]['psnr'] if len(progressive_results) > 1 else 0
        }
    }
    
    results_file = os.path.join(output_dir, 'fine_layers_evaluation_results.json')
    with open(results_file, 'w') as f:
        json.dump(evaluation_results, f, indent=2)
    
    print(f"âœ… è¯¦ç»†ç»“æœä¿å­˜: {results_file}")
    
    # 5. æ‰“å°æ€»ç»“
    print(f"\nğŸ“Š ç²¾ç»†åˆ†å±‚è¯„ä¼°æ€»ç»“:")
    print(f"  æ€»å±‚æ•°: {manifest['total_layers']}")
    print(f"  æ¸è¿›å¼è¯„ä¼°: {len(progressive_results)}/{len(progressive_files)} æˆåŠŸ")
    print(f"  å•å±‚è¯„ä¼°: {len(single_layer_results)}/{len(key_layers)} æˆåŠŸ")
    
    if progressive_results:
        final_psnr = progressive_results[-1]['psnr']
        initial_psnr = progressive_results[0]['psnr']
        improvement = final_psnr - initial_psnr
        print(f"  æœ€ç»ˆPSNR: {final_psnr:.3f}dB")
        print(f"  æ¸è¿›æå‡: {improvement:.3f}dB")
        
        if single_layer_results:
            best_single = max(single_layer_results, key=lambda x: x['psnr'])
            print(f"  æœ€ä½³å•å±‚: Layer {best_single['layer_id']} ({best_single['layer_description']}) - {best_single['psnr']:.3f}dB")
    
    return evaluation_results

def main():
    print("ğŸ‰ ç²¾ç»†å°ºå¯¸åˆ†å±‚å®Œæ•´æµç¨‹")
    print("=" * 60)
    
    # 1. åˆ›å»ºç²¾ç»†åˆ†å±‚æ–‡ä»¶
    print("æ­¥éª¤1: åˆ›å»ºç²¾ç»†åˆ†å±‚æ–‡ä»¶ (15å±‚)")
    manifest = create_fine_scale_layers()
    
    # 2. è¯„ä¼°åˆ†å±‚æ•ˆæœ
    print("\næ­¥éª¤2: è¯„ä¼°åˆ†å±‚æ¸²æŸ“æ•ˆæœå’ŒPSNR")
    results = evaluate_fine_layers()
    
    if results:
        print(f"\nğŸ‰ ç²¾ç»†åˆ†å±‚å®Œæ•´æµç¨‹å®Œæˆ!")
        print(f"ğŸ“ åˆ†å±‚æ–‡ä»¶ç›®å½•: fine_scale_layers/")
        print(f"ğŸ“ è¯„ä¼°ç»“æœç›®å½•: fine_layers_evaluation/")
        print(f"ğŸ¯ åˆ†ç»„ç­–ç•¥: å¾®å‹(0-4), å°å‹(5-9), ä¸­å¤§å‹(10-14)")

if __name__ == "__main__":
    main() 