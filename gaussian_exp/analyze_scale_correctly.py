import os
import sys
import torch
import numpy as np
import matplotlib.pyplot as plt
import json

# æ·»åŠ 3dgsæ ¹ç›®å½•åˆ°path
sys.path.append('/shared/user59/workspace/cihan/3dgs_Vincent/my_3dgs')

from scene import GaussianModel

def analyze_scale_correctly():
    """æ­£ç¡®åˆ†æé«˜æ–¯çƒå°ºå¯¸ - ç†è§£negative scalingçš„å«ä¹‰"""
    print("ğŸ” æ­£ç¡®çš„é«˜æ–¯çƒå°ºå¯¸åˆ†æ")
    print("=" * 50)
    
    # åŠ è½½åŸå§‹æ¨¡å‹
    ply_path = "./output/truck-150w/gaussian_ball/iteration_994230_best_psnr/gaussian_ball.ply"
    gaussians = GaussianModel(3)
    gaussians.load_ply(ply_path, use_train_test_exp=False)
    
    # æå–å‚æ•°
    xyz = gaussians._xyz.detach().cpu().numpy()
    scaling = gaussians._scaling.detach().cpu().numpy()  # è¿™æ˜¯log spaceçš„scaling
    opacity = gaussians._opacity.detach().cpu().numpy()
    
    # å¤„ç†NaNå€¼
    nan_mask = np.isnan(xyz)
    nan_positions = np.any(nan_mask, axis=1)
    nan_count = np.sum(nan_positions)
    
    if nan_count > 0:
        print(f"âš ï¸ å‘ç° {nan_count} ä¸ªNaNä½ç½®ï¼Œå°†è¢«æ’é™¤")
        valid_mask = ~nan_positions
        xyz = xyz[valid_mask]
        scaling = scaling[valid_mask]
        opacity = opacity[valid_mask]
    
    print(f"ğŸ“Š åŸå§‹scalingå‚æ•°åˆ†æ:")
    print(f"  æ€»é«˜æ–¯çƒæ•°: {len(scaling):,}")
    print(f"  Scaling shape: {scaling.shape}")
    print(f"  ScalingèŒƒå›´: {scaling.min():.6f} ~ {scaling.max():.6f}")
    print(f"  è¿™äº›æ˜¯**å¯¹æ•°ç©ºé—´**çš„scalingå€¼ï¼")
    
    # è½¬æ¢åˆ°å®é™…å°ºå¯¸ (expæ“ä½œ)
    actual_scales = np.exp(scaling)  # ä»log spaceè½¬æ¢åˆ°real space
    
    print(f"\nğŸ“ å®é™…å°ºå¯¸å‚æ•°åˆ†æ (exp(log_scaling)):")
    print(f"  Xå°ºå¯¸èŒƒå›´: {actual_scales[:, 0].min():.6f} ~ {actual_scales[:, 0].max():.6f}")
    print(f"  Yå°ºå¯¸èŒƒå›´: {actual_scales[:, 1].min():.6f} ~ {actual_scales[:, 1].max():.6f}")
    print(f"  Zå°ºå¯¸èŒƒå›´: {actual_scales[:, 2].min():.6f} ~ {actual_scales[:, 2].max():.6f}")
    
    # è®¡ç®—å¹³å‡å®é™…å°ºå¯¸
    avg_actual_scale = np.mean(actual_scales, axis=1)
    max_actual_scale = np.max(actual_scales, axis=1)
    min_actual_scale = np.min(actual_scales, axis=1)
    
    print(f"\nğŸ“ˆ å®é™…å°ºå¯¸ç»Ÿè®¡:")
    print(f"  å¹³å‡å°ºå¯¸èŒƒå›´: {avg_actual_scale.min():.6f} ~ {avg_actual_scale.max():.6f}")
    print(f"  å¹³å‡å°ºå¯¸å‡å€¼: {avg_actual_scale.mean():.6f}")
    print(f"  å¹³å‡å°ºå¯¸ä¸­ä½æ•°: {np.median(avg_actual_scale):.6f}")
    print(f"  å¹³å‡å°ºå¯¸æ ‡å‡†å·®: {avg_actual_scale.std():.6f}")
    
    # åˆ†æå°ºå¯¸åˆ†å¸ƒ
    percentiles = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 99]
    avg_scale_percentiles = np.percentile(avg_actual_scale, percentiles)
    
    print(f"\nğŸ“Š å¹³å‡å®é™…å°ºå¯¸ç™¾åˆ†ä½æ•°åˆ†æ:")
    for p, val in zip(percentiles, avg_scale_percentiles):
        count = np.sum(avg_actual_scale <= val)
        print(f"  {p:2d}%: {val:.6f} ({count:,}çƒ)")
    
    # é‡æ–°è¯„ä¼°ä¹‹å‰çš„"è¶…å°çƒ"
    previous_threshold = 0.003223  # ä¹‹å‰ç”¨çš„é˜ˆå€¼
    ultra_small_count = np.sum(avg_actual_scale <= previous_threshold)
    ultra_small_ratio = ultra_small_count / len(avg_actual_scale) * 100
    
    print(f"\nğŸ”„ é‡æ–°è¯„ä¼°'è¶…å°çƒ'æ¦‚å¿µ:")
    print(f"  ä¹‹å‰é˜ˆå€¼: {previous_threshold:.6f} (å®é™…å°ºå¯¸)")
    print(f"  ç¬¦åˆæ¡ä»¶æ•°é‡: {ultra_small_count:,} ({ultra_small_ratio:.2f}%)")
    
    # ç°åœ¨çœ‹èµ·æ¥æ›´åˆç†äº†ï¼Œè®©æˆ‘ä»¬é‡æ–°åˆ†å±‚
    print(f"\nğŸ¯ åŸºäºå®é™…å°ºå¯¸çš„æ™ºèƒ½åˆ†å±‚:")
    
    # æ–¹æ¡ˆ1: åŸºäºå®é™…å°ºå¯¸çš„ç­‰ç™¾åˆ†æ¯”åˆ†å±‚
    print(f"\n  æ–¹æ¡ˆ1: åŸºäºå¹³å‡å®é™…å°ºå¯¸çš„åˆ†å±‚")
    size_thresholds = np.percentile(avg_actual_scale, [20, 40, 60, 80])
    
    layer_names = ["è¶…å¾®", "å¾®å°", "å°å‹", "ä¸­å‹", "å¤§å‹"]
    layer_descriptions = ["Ultra-micro", "Micro", "Small", "Medium", "Large"]
    
    for i in range(5):
        if i == 0:
            mask = avg_actual_scale <= size_thresholds[0]
        elif i == 4:
            mask = avg_actual_scale > size_thresholds[3]
        else:
            mask = (avg_actual_scale > size_thresholds[i-1]) & (avg_actual_scale <= size_thresholds[i])
        
        count = np.sum(mask)
        ratio = count / len(avg_actual_scale) * 100
        if count > 0:
            range_str = f"{avg_actual_scale[mask].min():.6f}~{avg_actual_scale[mask].max():.6f}"
            print(f"    å±‚{i} ({layer_names[i]}): {count:,}çƒ ({ratio:.1f}%) èŒƒå›´: {range_str}")
    
    # æ–¹æ¡ˆ2: åŸºäºæœ€å¤§å°ºå¯¸åˆ†å±‚ (å¯èƒ½æ›´æœ‰æ„ä¹‰)
    print(f"\n  æ–¹æ¡ˆ2: åŸºäºæœ€å¤§å®é™…å°ºå¯¸çš„åˆ†å±‚")
    max_size_thresholds = np.percentile(max_actual_scale, [20, 40, 60, 80])
    
    for i in range(5):
        if i == 0:
            mask = max_actual_scale <= max_size_thresholds[0]
        elif i == 4:
            mask = max_actual_scale > max_size_thresholds[3]
        else:
            mask = (max_actual_scale > max_size_thresholds[i-1]) & (max_actual_scale <= max_size_thresholds[i])
        
        count = np.sum(mask)
        ratio = count / len(max_actual_scale) * 100
        if count > 0:
            range_str = f"{max_actual_scale[mask].min():.6f}~{max_actual_scale[mask].max():.6f}"
            print(f"    å±‚{i} ({layer_names[i]}): {count:,}çƒ ({ratio:.1f}%) æœ€å¤§å°ºå¯¸èŒƒå›´: {range_str}")
    
    # æ–¹æ¡ˆ3: è€ƒè™‘å½¢çŠ¶å·®å¼‚çš„åˆ†å±‚
    print(f"\n  æ–¹æ¡ˆ3: åŸºäºå½¢çŠ¶ç‰¹å¾çš„åˆ†å±‚")
    
    # è®¡ç®—æ¤­çƒå½¢çŠ¶ç‰¹å¾
    aspect_ratios = max_actual_scale / min_actual_scale  # æœ€å¤§è½´ä¸æœ€å°è½´çš„æ¯”ä¾‹
    volume_approx = np.prod(actual_scales, axis=1)  # è¿‘ä¼¼ä½“ç§¯
    
    print(f"    å½¢çŠ¶æ¯”åˆ†æ:")
    print(f"      å½¢çŠ¶æ¯”èŒƒå›´: {aspect_ratios.min():.2f} ~ {aspect_ratios.max():.2f}")
    print(f"      å½¢çŠ¶æ¯”å‡å€¼: {aspect_ratios.mean():.2f}")
    
    # æŒ‰å½¢çŠ¶æ¯”åˆ†ç±»
    sphere_like = aspect_ratios <= 2.0    # æ¥è¿‘çƒå½¢
    ellipsoid_like = (aspect_ratios > 2.0) & (aspect_ratios <= 5.0)  # æ¤­çƒå½¢
    needle_like = aspect_ratios > 5.0     # é’ˆçŠ¶/ç‰‡çŠ¶
    
    print(f"      çƒå½¢: {np.sum(sphere_like):,}çƒ ({np.sum(sphere_like)/len(aspect_ratios)*100:.1f}%)")
    print(f"      æ¤­çƒå½¢: {np.sum(ellipsoid_like):,}çƒ ({np.sum(ellipsoid_like)/len(aspect_ratios)*100:.1f}%)")
    print(f"      é’ˆçŠ¶: {np.sum(needle_like):,}çƒ ({np.sum(needle_like)/len(aspect_ratios)*100:.1f}%)")
    
    # åˆ†æä½“ç§¯åˆ†å¸ƒ
    print(f"    ä½“ç§¯åˆ†æ:")
    print(f"      ä½“ç§¯èŒƒå›´: {volume_approx.min():.9f} ~ {volume_approx.max():.6f}")
    print(f"      ä½“ç§¯å‡å€¼: {volume_approx.mean():.9f}")
    print(f"      ä½“ç§¯ä¸­ä½æ•°: {np.median(volume_approx):.9f}")
    
    # ä½“ç§¯åˆ†å±‚
    volume_thresholds = np.percentile(volume_approx, [20, 40, 60, 80])
    print(f"    æŒ‰ä½“ç§¯åˆ†å±‚:")
    for i in range(5):
        if i == 0:
            mask = volume_approx <= volume_thresholds[0]
        elif i == 4:
            mask = volume_approx > volume_thresholds[3]
        else:
            mask = (volume_approx > volume_thresholds[i-1]) & (volume_approx <= volume_thresholds[i])
        
        count = np.sum(mask)
        ratio = count / len(volume_approx) * 100
        if count > 0:
            range_str = f"{volume_approx[mask].min():.9f}~{volume_approx[mask].max():.6f}"
            print(f"      å±‚{i}: {count:,}çƒ ({ratio:.1f}%) ä½“ç§¯èŒƒå›´: {range_str}")
    
    # åˆ›å»ºå¯è§†åŒ–ï¼ˆä¿®å¤å°ºå¯¸é—®é¢˜ï¼‰
    print(f"\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "correct_scale_analysis"
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿®å¤å›¾åƒå°ºå¯¸é—®é¢˜
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Correct Gaussian Scale Analysis (Actual Sizes)', fontsize=16, fontweight='bold')
    
    # 1. Log scalingåˆ†å¸ƒ
    ax = axes[0, 0]
    ax.hist(scaling.flatten(), bins=50, alpha=0.7, color='blue', density=True)
    ax.set_xlabel('Log Scaling Values')
    ax.set_ylabel('Density')
    ax.set_title('Original Log Scaling Distribution')
    ax.grid(True, alpha=0.3)
    
    # 2. å®é™…å°ºå¯¸åˆ†å¸ƒ
    ax = axes[0, 1]
    ax.hist(avg_actual_scale, bins=50, alpha=0.7, color='red', density=True)
    ax.set_xlabel('Average Actual Scale')
    ax.set_ylabel('Density')
    ax.set_title('Average Actual Scale Distribution')
    ax.set_xscale('log')
    ax.grid(True, alpha=0.3)
    
    # 3. æœ€å¤§å°ºå¯¸åˆ†å¸ƒ
    ax = axes[0, 2]
    ax.hist(max_actual_scale, bins=50, alpha=0.7, color='green', density=True)
    ax.set_xlabel('Maximum Actual Scale')
    ax.set_ylabel('Density')
    ax.set_title('Maximum Actual Scale Distribution')
    ax.set_xscale('log')
    ax.grid(True, alpha=0.3)
    
    # 4. å½¢çŠ¶æ¯”åˆ†å¸ƒ
    ax = axes[1, 0]
    ax.hist(aspect_ratios, bins=50, alpha=0.7, color='purple', density=True)
    ax.set_xlabel('Aspect Ratio (Max/Min)')
    ax.set_ylabel('Density')
    ax.set_title('Shape Aspect Ratio Distribution')
    ax.set_xscale('log')
    ax.grid(True, alpha=0.3)
    
    # 5. ä½“ç§¯åˆ†å¸ƒ
    ax = axes[1, 1]
    ax.hist(volume_approx, bins=50, alpha=0.7, color='orange', density=True)
    ax.set_xlabel('Approximate Volume')
    ax.set_ylabel('Density')
    ax.set_title('Volume Distribution')
    ax.set_xscale('log')
    ax.grid(True, alpha=0.3)
    
    # 6. ç´¯ç§¯åˆ†å¸ƒå¯¹æ¯”
    ax = axes[1, 2]
    
    # å¹³å‡å°ºå¯¸ç´¯ç§¯åˆ†å¸ƒ
    sorted_avg = np.sort(avg_actual_scale)
    cumulative_avg = np.arange(1, len(sorted_avg) + 1) / len(sorted_avg) * 100
    ax.plot(sorted_avg, cumulative_avg, 'r-', linewidth=2, label='Average Scale')
    
    # æœ€å¤§å°ºå¯¸ç´¯ç§¯åˆ†å¸ƒ
    sorted_max = np.sort(max_actual_scale)
    cumulative_max = np.arange(1, len(sorted_max) + 1) / len(sorted_max) * 100
    ax.plot(sorted_max, cumulative_max, 'g-', linewidth=2, label='Maximum Scale')
    
    ax.set_xlabel('Actual Scale')
    ax.set_ylabel('Cumulative Percentage (%)')
    ax.set_title('Cumulative Distribution Comparison')
    ax.set_xscale('log')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾åƒ
    distribution_file = os.path.join(output_dir, 'correct_scale_distribution.png')
    plt.savefig(distribution_file, dpi=150, bbox_inches='tight')  # é™ä½DPIé¿å…è¿‡å¤§
    plt.close()
    
    print(f"âœ… åˆ†å¸ƒå›¾ä¿å­˜: {distribution_file}")
    
    # ä¿å­˜åˆ†æç»“æœ
    analysis_results = {
        'understanding_correction': {
            'original_scaling_is_log_space': True,
            'actual_scales_computed_by': 'exp(log_scaling)',
            'previous_error': 'Used log values as actual sizes'
        },
        'actual_scale_stats': {
            'avg_scale_range': [float(avg_actual_scale.min()), float(avg_actual_scale.max())],
            'avg_scale_mean': float(avg_actual_scale.mean()),
            'avg_scale_median': float(np.median(avg_actual_scale)),
            'avg_scale_std': float(avg_actual_scale.std())
        },
        'shape_analysis': {
            'aspect_ratio_range': [float(aspect_ratios.min()), float(aspect_ratios.max())],
            'aspect_ratio_mean': float(aspect_ratios.mean()),
            'shape_categories': {
                'sphere_like': int(np.sum(sphere_like)),
                'ellipsoid_like': int(np.sum(ellipsoid_like)),
                'needle_like': int(np.sum(needle_like))
            }
        },
        'volume_analysis': {
            'volume_range': [float(volume_approx.min()), float(volume_approx.max())],
            'volume_mean': float(volume_approx.mean()),
            'volume_median': float(np.median(volume_approx))
        },
        'recommended_layering': {
            'avg_scale_thresholds': [float(t) for t in size_thresholds],
            'max_scale_thresholds': [float(t) for t in max_size_thresholds],
            'volume_thresholds': [float(t) for t in volume_thresholds]
        },
        'key_insights': [
            "åŸå§‹scalingæ˜¯å¯¹æ•°ç©ºé—´å€¼ï¼Œéœ€è¦exp()è½¬æ¢ä¸ºå®é™…å°ºå¯¸",
            "å®é™…å°ºå¯¸åˆ†å¸ƒæ›´åŠ åˆç†ï¼Œæ²¡æœ‰99%éƒ½æ˜¯è¶…å°çƒçš„é—®é¢˜",
            "åº”è¯¥åŸºäºå®é™…å°ºå¯¸é‡æ–°è®¾è®¡åˆ†å±‚ç­–ç•¥",
            "å¯ä»¥è€ƒè™‘å½¢çŠ¶å’Œä½“ç§¯ä½œä¸ºåˆ†å±‚ä¾æ®",
            "å¤§éƒ¨åˆ†é«˜æ–¯çƒæ˜¯è¿‘çƒå½¢çš„ï¼Œå°‘æ•°æ˜¯æ¤­çƒæˆ–é’ˆçŠ¶"
        ]
    }
    
    results_file = os.path.join(output_dir, 'correct_scale_analysis_results.json')
    with open(results_file, 'w') as f:
        json.dump(analysis_results, f, indent=2)
    
    print(f"âœ… åˆ†æç»“æœä¿å­˜: {results_file}")
    
    print(f"\nğŸ‰ æ­£ç¡®çš„å°ºå¯¸åˆ†æå®Œæˆ!")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}/")
    print(f"ğŸ’¡ å…³é”®å‘ç°:")
    print(f"   1. **é‡å¤§å‘ç°**: åŸå§‹scalingæ˜¯å¯¹æ•°ç©ºé—´å€¼ï¼")
    print(f"   2. å®é™…å°ºå¯¸åˆ†å¸ƒåˆç†: {avg_actual_scale.min():.6f} ~ {avg_actual_scale.max():.6f}")
    print(f"   3. ç°åœ¨å¯ä»¥è¿›è¡Œæœ‰æ„ä¹‰çš„å°ºå¯¸åˆ†å±‚äº†")
    print(f"   4. å¤§éƒ¨åˆ†æ˜¯çƒå½¢({np.sum(sphere_like)/len(aspect_ratios)*100:.1f}%)ï¼Œå°‘æ•°æ¤­çƒå½¢å’Œé’ˆçŠ¶")
    
    return analysis_results, {
        'avg_actual_scale': avg_actual_scale,
        'max_actual_scale': max_actual_scale,
        'aspect_ratios': aspect_ratios,
        'volume_approx': volume_approx
    }

def main():
    analyze_scale_correctly()

if __name__ == "__main__":
    main() 